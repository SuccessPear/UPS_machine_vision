{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 8\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 5000\n",
    "ADA_TARGET = 0.6\n",
    "ADA_INTERVAL = 4\n",
    "ADA_SPEED = 0.01\n",
    "ADA_MAX_P = 0.85  # Clamp augmentation probability\n",
    "FID_INTERVAL = 100  # Check FID every 100 batches\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, root_dir, class_name = \"\", transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        \n",
    "        if class_name == \"\":\n",
    "            for class_dir in os.listdir(root_dir):\n",
    "                class_path = os.path.join(root_dir, class_dir)\n",
    "                if os.path.isdir(class_path):\n",
    "                    for img_name in os.listdir(class_path):\n",
    "                        self.image_paths.append(os.path.join(class_path, img_name))\n",
    "        else:\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if os.path.isdir(class_path):\n",
    "                for img_name in os.listdir(class_path):\n",
    "                    self.image_paths.append(os.path.join(class_path, img_name))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Basic transforms (ADA will handle additional augmentations)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = AnimalDataset(root_dir='Newdata/Train', class_name=\"\", transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptiveAugment:\n",
    "    def __init__(self, ada_target=0.6, ada_speed=0.01, max_p=0.8, warmup=500):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ada_target: Target discriminator accuracy (default: 0.6)\n",
    "            ada_speed: How quickly p adjusts (smaller = more stable)\n",
    "            max_p: Maximum augmentation probability (recommended ≤0.85)\n",
    "            warmup: Batches before allowing p to increase (prevents early over-augmentation)\n",
    "        \"\"\"\n",
    "        self.ada_target = ada_target\n",
    "        self.ada_speed = ada_speed\n",
    "        self.max_p = max_p\n",
    "        self.warmup = warmup\n",
    "        self.augment_p = 0\n",
    "        self.r_t = 0  # Running accuracy estimate\n",
    "        self.steps = 0\n",
    "        self.sign_history = []\n",
    "\n",
    "    def update(self, real_pred):\n",
    "        \"\"\"Update augmentation probability based on discriminator accuracy\"\"\"\n",
    "        self.steps += 1\n",
    "        \n",
    "        # Calculate current discriminator accuracy\n",
    "        current_acc = real_pred.sigmoid().mean().item()\n",
    "        self.r_t = 0.9 * self.r_t + 0.1 * current_acc  # EMA smoothing\n",
    "\n",
    "        # Determine adjustment direction\n",
    "        sign = 1 if self.r_t > self.ada_target else -1\n",
    "        self.sign_history.append(sign)\n",
    "\n",
    "        # Only adjust p if:\n",
    "        # 1. Past warmup period AND\n",
    "        # 2. Consistent direction for 5 batches\n",
    "        if self.steps > self.warmup and \\\n",
    "           len(self.sign_history) > 5 and \\\n",
    "           sum(self.sign_history[-5:]) == 5 * sign:\n",
    "            \n",
    "            # More conservative adjustment when p is high\n",
    "            effective_speed = self.ada_speed * (1 - 0.5 * (self.augment_p / self.max_p))\n",
    "            \n",
    "            self.augment_p += sign * effective_speed\n",
    "            self.augment_p = min(max(self.augment_p, 0), self.max_p)  # Clamp to [0, max_p]\n",
    "\n",
    "        return self.augment_p\n",
    "\n",
    "    def apply_augment(self, x, p=None):\n",
    "        \"\"\"Apply augmentations with probability p (uses current p if None)\"\"\"\n",
    "        p = self.augment_p if p is None else min(p, self.max_p)\n",
    "        if p <= 0:\n",
    "            return x\n",
    "        \n",
    "        # Apply augmentations with probability p\n",
    "        if torch.rand(1, device=x.device).item() < p:\n",
    "            # Random horizontal flip (50% chance)\n",
    "            if torch.rand(1, device=x.device).item() > 0.5:\n",
    "                x = torch.flip(x, [3])\n",
    "            \n",
    "            # Smaller rotation range for stability (-10° to 10°)\n",
    "            angle = torch.empty(1, device=x.device).uniform_(-10, 10).item()\n",
    "            x = transforms.functional.rotate(x, angle)\n",
    "            \n",
    "            # More subtle color jitter\n",
    "            brightness = torch.empty(1, device=x.device).uniform_(0.9, 1.1).item()\n",
    "            contrast = torch.empty(1, device=x.device).uniform_(0.9, 1.1).item()\n",
    "            x = transforms.functional.adjust_brightness(x, brightness)\n",
    "            x = transforms.functional.adjust_contrast(x, contrast)\n",
    "            \n",
    "            # Smaller translation (2-3%)\n",
    "            translate_x = torch.empty(1, device=x.device).uniform_(-0.03, 0.03).item()\n",
    "            translate_y = torch.empty(1, device=x.device).uniform_(-0.03, 0.03).item()\n",
    "            x = transforms.functional.affine(\n",
    "                x, angle=0, translate=[translate_x, translate_y], \n",
    "                scale=1, shear=0\n",
    "            )\n",
    "        \n",
    "        return x\n",
    "\n",
    "from pytorch_fid import fid_score\n",
    "import tempfile\n",
    "import os\n",
    "def get_samples_from_dataloader(dataloader, num_samples):\n",
    "    \"\"\"Properly extracts samples from a DataLoader\"\"\"\n",
    "    samples = []\n",
    "    total_collected = 0\n",
    "    \n",
    "    # Iterate through batches\n",
    "    for batch in dataloader:\n",
    "        # Handle batched data (batch could be tensors or dicts)\n",
    "        if isinstance(batch, (torch.Tensor, np.ndarray)):\n",
    "            batch_samples = batch\n",
    "        elif isinstance(batch, dict):\n",
    "            batch_samples = batch['images']  # Adjust based on your dataset\n",
    "        else:\n",
    "            batch_samples = batch[0]  # Common for (data, label) tuples\n",
    "        \n",
    "        # Add samples until we have enough\n",
    "        remaining = num_samples - total_collected\n",
    "        if remaining <= 0:\n",
    "            break\n",
    "            \n",
    "        samples.append(batch_samples[:remaining])\n",
    "        total_collected += batch_samples.shape[0]\n",
    "    \n",
    "    return torch.cat(samples)[:num_samples]  # Ensures exact number\n",
    "def calculate_fid(generator, dataloader, device, num_samples=500):\n",
    "    with tempfile.TemporaryDirectory() as real_dir, tempfile.TemporaryDirectory() as fake_dir:\n",
    "        # 1. Save REAL images (correct DataLoader handling)\n",
    "        real_images = get_samples_from_dataloader(dataloader, num_samples)\n",
    "        for i, img in enumerate(real_images):\n",
    "            img_pil = transforms.ToPILImage()(img.cpu())\n",
    "            img_pil.save(os.path.join(real_dir, f'real_{i}.png'))\n",
    "        \n",
    "        # 2. Generate FAKE images\n",
    "        with torch.no_grad():\n",
    "            for i in range(0, num_samples, BATCH_SIZE):\n",
    "                batch_size = min(BATCH_SIZE, num_samples - i)\n",
    "                z = torch.randn(batch_size, LATENT_DIM, device=device)\n",
    "                fake_batch = generator(z).cpu()\n",
    "                \n",
    "                for j in range(batch_size):\n",
    "                    transforms.ToPILImage()(fake_batch[j]).save(\n",
    "                        os.path.join(fake_dir, f'fake_{i+j}.png'))\n",
    "        \n",
    "        # 3. Compute FID\n",
    "        return fid_score.calculate_fid_given_paths(\n",
    "            [real_dir, fake_dir],\n",
    "            batch_size=min(BATCH_SIZE, num_samples),\n",
    "            device=device,\n",
    "            dims=2048\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Network\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "            nn.BatchNorm2d(channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(channels, channels, 3, 1, 1),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)  # Skip connection\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.init_size = IMG_HEIGHT // 8\n",
    "        self.l1 = nn.Sequential(nn.Linear(latent_dim, 128 * self.init_size ** 2))\n",
    "        \n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            ResBlock(64),\n",
    "            \n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 32, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(32, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            ResBlock(32),\n",
    "            \n",
    "            nn.Conv2d(32, CHANNELS, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(CHANNELS, 0.8),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # self.conv_blocks = nn.Sequential(\n",
    "        #     nn.ConvTranspose2d(128, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        #     nn.BatchNorm2d(128, 0.8),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "        #     nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        #     nn.BatchNorm2d(64, 0.8),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "        #     nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        #     nn.BatchNorm2d(32, 0.8),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "        #     nn.ConvTranspose2d(32, CHANNELS, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "        #     nn.BatchNorm2d(CHANNELS, 0.8),\n",
    "        #     nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "        #     nn.ConvTranspose2d(CHANNELS, CHANNELS, kernel_size=3, stride=1, padding=1),\n",
    "        #     nn.BatchNorm2d(CHANNELS, 0.8),\n",
    "        #     nn.Tanh()\n",
    "        # )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        out = self.l1(z)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def discriminator_block(in_filters, out_filters, stride = 2, bn=True):\n",
    "            block = [nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=stride, padding=1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(CHANNELS, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128, stride=1),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # The height and width of downsampled image\n",
    "        ds_size = IMG_HEIGHT // 2 ** 4\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "    \n",
    "    def forward(self, img):\n",
    "        out = self.model(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        return validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "generator = Generator(LATENT_DIM).to(DEVICE)\n",
    "discriminator = Discriminator().to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss function\n",
    "adversarial_loss = nn.BCELoss()\n",
    "\n",
    "# ADA module\n",
    "ada = AdaptiveAugment(\n",
    "    ada_target=0.65,  # Slightly higher target\n",
    "    ada_speed=0.005,  # Slower adjustments\n",
    "    max_p=0.8,        # Conservative maximum\n",
    "    warmup=200       # Longer warmup for small datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/2000] [Batch 0/11] [D loss: 0.6934] [G loss: 0.6912] [ADA p: 0.000]\n",
      "[Epoch 1/2000] [Batch 0/11] [D loss: 0.6872] [G loss: 0.6969] [ADA p: 0.000]\n",
      "[Epoch 2/2000] [Batch 0/11] [D loss: 0.6786] [G loss: 0.6812] [ADA p: 0.000]\n",
      "[Epoch 3/2000] [Batch 0/11] [D loss: 0.6629] [G loss: 0.7232] [ADA p: 0.000]\n",
      "[Epoch 4/2000] [Batch 0/11] [D loss: 0.5973] [G loss: 0.6644] [ADA p: 0.000]\n",
      "[Epoch 5/2000] [Batch 0/11] [D loss: 0.6648] [G loss: 0.6236] [ADA p: 0.000]\n",
      "[Epoch 6/2000] [Batch 0/11] [D loss: 0.4877] [G loss: 0.8754] [ADA p: 0.000]\n",
      "[Epoch 7/2000] [Batch 0/11] [D loss: 0.6213] [G loss: 0.7166] [ADA p: 0.000]\n",
      "[Epoch 8/2000] [Batch 0/11] [D loss: 0.5177] [G loss: 1.1297] [ADA p: 0.000]\n",
      "[Epoch 9/2000] [Batch 0/11] [D loss: 0.4223] [G loss: 0.9126] [ADA p: 0.000]\n",
      "[Epoch 10/2000] [Batch 0/11] [D loss: 0.3744] [G loss: 1.1268] [ADA p: 0.000]\n",
      "[Epoch 11/2000] [Batch 0/11] [D loss: 0.4408] [G loss: 1.1330] [ADA p: 0.000]\n",
      "[Epoch 12/2000] [Batch 0/11] [D loss: 0.3827] [G loss: 1.2556] [ADA p: 0.000]\n",
      "[Epoch 13/2000] [Batch 0/11] [D loss: 0.4582] [G loss: 2.1815] [ADA p: 0.000]\n",
      "[Epoch 14/2000] [Batch 0/11] [D loss: 0.2255] [G loss: 1.4211] [ADA p: 0.000]\n",
      "[Epoch 15/2000] [Batch 0/11] [D loss: 0.2970] [G loss: 2.7398] [ADA p: 0.000]\n",
      "[Epoch 16/2000] [Batch 0/11] [D loss: 0.3026] [G loss: 1.8146] [ADA p: 0.000]\n",
      "[Epoch 17/2000] [Batch 0/11] [D loss: 0.2376] [G loss: 1.7896] [ADA p: 0.000]\n",
      "[Epoch 18/2000] [Batch 0/11] [D loss: 0.2727] [G loss: 1.9374] [ADA p: 0.000]\n",
      "[Epoch 19/2000] [Batch 0/11] [D loss: 0.3997] [G loss: 1.0609] [ADA p: 0.000]\n",
      "[Epoch 20/2000] [Batch 0/11] [D loss: 0.5712] [G loss: 2.4164] [ADA p: 0.000]\n",
      "[Epoch 21/2000] [Batch 0/11] [D loss: 0.3208] [G loss: 2.5349] [ADA p: 0.000]\n",
      "[Epoch 22/2000] [Batch 0/11] [D loss: 0.5510] [G loss: 0.3551] [ADA p: 0.000]\n",
      "[Epoch 23/2000] [Batch 0/11] [D loss: 0.3553] [G loss: 0.4227] [ADA p: 0.000]\n",
      "[Epoch 24/2000] [Batch 0/11] [D loss: 0.2904] [G loss: 1.1956] [ADA p: 0.000]\n",
      "[Epoch 25/2000] [Batch 0/11] [D loss: 0.5454] [G loss: 1.3670] [ADA p: 0.000]\n",
      "[Epoch 26/2000] [Batch 0/11] [D loss: 0.3736] [G loss: 1.5290] [ADA p: 0.000]\n",
      "[Epoch 27/2000] [Batch 0/11] [D loss: 0.4856] [G loss: 1.8415] [ADA p: 0.000]\n",
      "[Epoch 28/2000] [Batch 0/11] [D loss: 0.3760] [G loss: 1.4983] [ADA p: 0.000]\n",
      "[Epoch 29/2000] [Batch 0/11] [D loss: 0.5486] [G loss: 1.5499] [ADA p: 0.000]\n",
      "[Epoch 30/2000] [Batch 0/11] [D loss: 0.6754] [G loss: 1.6060] [ADA p: 0.000]\n",
      "[Epoch 31/2000] [Batch 0/11] [D loss: 0.6822] [G loss: 2.1279] [ADA p: 0.000]\n",
      "[Epoch 32/2000] [Batch 0/11] [D loss: 0.2883] [G loss: 0.9409] [ADA p: 0.000]\n",
      "[Epoch 33/2000] [Batch 0/11] [D loss: 0.3172] [G loss: 2.2825] [ADA p: 0.000]\n",
      "[Epoch 34/2000] [Batch 0/11] [D loss: 0.2664] [G loss: 1.6979] [ADA p: 0.000]\n",
      "[Epoch 35/2000] [Batch 0/11] [D loss: 0.4925] [G loss: 2.8318] [ADA p: 0.000]\n",
      "[Epoch 36/2000] [Batch 0/11] [D loss: 0.1636] [G loss: 2.6805] [ADA p: 0.000]\n",
      "[Epoch 37/2000] [Batch 0/11] [D loss: 0.2454] [G loss: 3.1195] [ADA p: 0.000]\n",
      "[Epoch 38/2000] [Batch 0/11] [D loss: 0.2034] [G loss: 2.2533] [ADA p: 0.000]\n",
      "[Epoch 39/2000] [Batch 0/11] [D loss: 0.0833] [G loss: 0.9533] [ADA p: 0.000]\n",
      "[Epoch 40/2000] [Batch 0/11] [D loss: 0.2203] [G loss: 2.8380] [ADA p: 0.000]\n",
      "[Epoch 41/2000] [Batch 0/11] [D loss: 0.2792] [G loss: 1.4534] [ADA p: 0.000]\n",
      "[Epoch 42/2000] [Batch 0/11] [D loss: 0.2470] [G loss: 0.6605] [ADA p: 0.000]\n",
      "[Epoch 43/2000] [Batch 0/11] [D loss: 0.1856] [G loss: 3.5002] [ADA p: 0.000]\n",
      "[Epoch 44/2000] [Batch 0/11] [D loss: 0.6543] [G loss: 2.9310] [ADA p: 0.000]\n",
      "[Epoch 45/2000] [Batch 0/11] [D loss: 0.3056] [G loss: 0.4409] [ADA p: 0.000]\n",
      "[Epoch 46/2000] [Batch 0/11] [D loss: 0.1251] [G loss: 2.1288] [ADA p: 0.000]\n",
      "[Epoch 47/2000] [Batch 0/11] [D loss: 0.6942] [G loss: 2.8553] [ADA p: 0.000]\n",
      "[Epoch 48/2000] [Batch 0/11] [D loss: 0.4183] [G loss: 1.5114] [ADA p: 0.000]\n",
      "[Epoch 49/2000] [Batch 0/11] [D loss: 0.9552] [G loss: 2.1178] [ADA p: 0.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:10<00:00,  3.06it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49 | FID: 391.78 | ADA p: 0.000\n",
      "[Epoch 50/2000] [Batch 0/11] [D loss: 1.4889] [G loss: 0.6463] [ADA p: 0.000]\n",
      "[Epoch 51/2000] [Batch 0/11] [D loss: 0.4836] [G loss: 1.7216] [ADA p: 0.000]\n",
      "[Epoch 52/2000] [Batch 0/11] [D loss: 0.4932] [G loss: 1.5185] [ADA p: 0.000]\n",
      "[Epoch 53/2000] [Batch 0/11] [D loss: 0.1030] [G loss: 2.6371] [ADA p: 0.000]\n",
      "[Epoch 54/2000] [Batch 0/11] [D loss: 0.4032] [G loss: 3.7463] [ADA p: 0.000]\n",
      "[Epoch 55/2000] [Batch 0/11] [D loss: 0.5828] [G loss: 1.7398] [ADA p: 0.000]\n",
      "[Epoch 56/2000] [Batch 0/11] [D loss: 0.2099] [G loss: 1.6861] [ADA p: 0.000]\n",
      "[Epoch 57/2000] [Batch 0/11] [D loss: 0.1199] [G loss: 0.6187] [ADA p: 0.000]\n",
      "[Epoch 58/2000] [Batch 0/11] [D loss: 0.3434] [G loss: 0.9703] [ADA p: 0.000]\n",
      "[Epoch 59/2000] [Batch 0/11] [D loss: 0.3593] [G loss: 2.8891] [ADA p: 0.000]\n",
      "[Epoch 60/2000] [Batch 0/11] [D loss: 0.1584] [G loss: 1.6314] [ADA p: 0.000]\n",
      "[Epoch 61/2000] [Batch 0/11] [D loss: 0.6051] [G loss: 1.4583] [ADA p: 0.000]\n",
      "[Epoch 62/2000] [Batch 0/11] [D loss: 0.4918] [G loss: 1.3602] [ADA p: 0.000]\n",
      "[Epoch 63/2000] [Batch 0/11] [D loss: 0.4038] [G loss: 2.8641] [ADA p: 0.000]\n",
      "[Epoch 64/2000] [Batch 0/11] [D loss: 0.3832] [G loss: 1.2082] [ADA p: 0.000]\n",
      "[Epoch 65/2000] [Batch 0/11] [D loss: 0.7125] [G loss: 2.1816] [ADA p: 0.000]\n",
      "[Epoch 66/2000] [Batch 0/11] [D loss: 0.3826] [G loss: 0.7310] [ADA p: 0.000]\n",
      "[Epoch 67/2000] [Batch 0/11] [D loss: 0.5849] [G loss: 1.3220] [ADA p: 0.010]\n",
      "[Epoch 68/2000] [Batch 0/11] [D loss: 0.3399] [G loss: 1.1457] [ADA p: 0.025]\n",
      "[Epoch 69/2000] [Batch 0/11] [D loss: 0.4251] [G loss: 1.2687] [ADA p: 0.040]\n",
      "[Epoch 70/2000] [Batch 0/11] [D loss: 0.8292] [G loss: 1.7968] [ADA p: 0.049]\n",
      "[Epoch 71/2000] [Batch 0/11] [D loss: 0.5442] [G loss: 1.5189] [ADA p: 0.049]\n",
      "[Epoch 72/2000] [Batch 0/11] [D loss: 0.4185] [G loss: 0.6171] [ADA p: 0.059]\n",
      "[Epoch 73/2000] [Batch 0/11] [D loss: 0.3462] [G loss: 1.5689] [ADA p: 0.073]\n",
      "[Epoch 74/2000] [Batch 0/11] [D loss: 0.4579] [G loss: 1.6699] [ADA p: 0.083]\n",
      "[Epoch 75/2000] [Batch 0/11] [D loss: 0.0780] [G loss: 1.3325] [ADA p: 0.083]\n",
      "[Epoch 76/2000] [Batch 0/11] [D loss: 0.1841] [G loss: 2.2547] [ADA p: 0.083]\n",
      "[Epoch 77/2000] [Batch 0/11] [D loss: 0.4507] [G loss: 2.2499] [ADA p: 0.092]\n",
      "[Epoch 78/2000] [Batch 0/11] [D loss: 1.2727] [G loss: 1.1054] [ADA p: 0.106]\n",
      "[Epoch 79/2000] [Batch 0/11] [D loss: 0.5924] [G loss: 2.9454] [ADA p: 0.120]\n",
      "[Epoch 80/2000] [Batch 0/11] [D loss: 0.0724] [G loss: 1.8882] [ADA p: 0.134]\n",
      "[Epoch 81/2000] [Batch 0/11] [D loss: 0.3336] [G loss: 3.5849] [ADA p: 0.148]\n",
      "[Epoch 82/2000] [Batch 0/11] [D loss: 0.0625] [G loss: 2.8027] [ADA p: 0.162]\n",
      "[Epoch 83/2000] [Batch 0/11] [D loss: 0.1283] [G loss: 2.6199] [ADA p: 0.175]\n",
      "[Epoch 84/2000] [Batch 0/11] [D loss: 0.1120] [G loss: 2.3669] [ADA p: 0.188]\n",
      "[Epoch 85/2000] [Batch 0/11] [D loss: 0.7143] [G loss: 1.4453] [ADA p: 0.201]\n",
      "[Epoch 86/2000] [Batch 0/11] [D loss: 1.0772] [G loss: 2.6481] [ADA p: 0.215]\n",
      "[Epoch 87/2000] [Batch 0/11] [D loss: 0.1971] [G loss: 1.4046] [ADA p: 0.227]\n",
      "[Epoch 88/2000] [Batch 0/11] [D loss: 0.4079] [G loss: 1.9081] [ADA p: 0.240]\n",
      "[Epoch 89/2000] [Batch 0/11] [D loss: 0.1438] [G loss: 3.9810] [ADA p: 0.253]\n",
      "[Epoch 90/2000] [Batch 0/11] [D loss: 0.0896] [G loss: 1.6545] [ADA p: 0.266]\n",
      "[Epoch 91/2000] [Batch 0/11] [D loss: 0.5365] [G loss: 1.0896] [ADA p: 0.278]\n",
      "[Epoch 92/2000] [Batch 0/11] [D loss: 0.9476] [G loss: 1.1613] [ADA p: 0.290]\n",
      "[Epoch 93/2000] [Batch 0/11] [D loss: 0.6463] [G loss: 2.8543] [ADA p: 0.303]\n",
      "[Epoch 94/2000] [Batch 0/11] [D loss: 0.1838] [G loss: 2.1657] [ADA p: 0.315]\n",
      "[Epoch 95/2000] [Batch 0/11] [D loss: 0.4985] [G loss: 0.5813] [ADA p: 0.327]\n",
      "[Epoch 96/2000] [Batch 0/11] [D loss: 0.4456] [G loss: 1.0271] [ADA p: 0.339]\n",
      "[Epoch 97/2000] [Batch 0/11] [D loss: 0.8323] [G loss: 0.7244] [ADA p: 0.351]\n",
      "[Epoch 98/2000] [Batch 0/11] [D loss: 0.3324] [G loss: 0.5842] [ADA p: 0.354]\n",
      "[Epoch 99/2000] [Batch 0/11] [D loss: 0.1713] [G loss: 0.9984] [ADA p: 0.354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.56it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 | FID: 393.70 | ADA p: 0.354\n",
      "[Epoch 100/2000] [Batch 0/11] [D loss: 0.3240] [G loss: 1.4428] [ADA p: 0.354]\n",
      "[Epoch 101/2000] [Batch 0/11] [D loss: 0.5549] [G loss: 1.0672] [ADA p: 0.354]\n",
      "[Epoch 102/2000] [Batch 0/11] [D loss: 0.7307] [G loss: 1.4890] [ADA p: 0.354]\n",
      "[Epoch 103/2000] [Batch 0/11] [D loss: 0.6885] [G loss: 0.7758] [ADA p: 0.354]\n",
      "[Epoch 104/2000] [Batch 0/11] [D loss: 0.4883] [G loss: 1.2099] [ADA p: 0.343]\n",
      "[Epoch 105/2000] [Batch 0/11] [D loss: 0.6135] [G loss: 0.7581] [ADA p: 0.331]\n",
      "[Epoch 106/2000] [Batch 0/11] [D loss: 0.3207] [G loss: 1.2475] [ADA p: 0.319]\n",
      "[Epoch 107/2000] [Batch 0/11] [D loss: 0.5420] [G loss: 1.7690] [ADA p: 0.307]\n",
      "[Epoch 108/2000] [Batch 0/11] [D loss: 0.4714] [G loss: 0.5721] [ADA p: 0.299]\n",
      "[Epoch 109/2000] [Batch 0/11] [D loss: 0.5943] [G loss: 0.6970] [ADA p: 0.299]\n",
      "[Epoch 110/2000] [Batch 0/11] [D loss: 0.4084] [G loss: 0.7254] [ADA p: 0.299]\n",
      "[Epoch 111/2000] [Batch 0/11] [D loss: 1.1866] [G loss: 0.6871] [ADA p: 0.291]\n",
      "[Epoch 112/2000] [Batch 0/11] [D loss: 0.2299] [G loss: 1.6243] [ADA p: 0.282]\n",
      "[Epoch 113/2000] [Batch 0/11] [D loss: 0.1246] [G loss: 1.3519] [ADA p: 0.282]\n",
      "[Epoch 114/2000] [Batch 0/11] [D loss: 0.6487] [G loss: 1.0534] [ADA p: 0.282]\n",
      "[Epoch 115/2000] [Batch 0/11] [D loss: 0.3187] [G loss: 1.1692] [ADA p: 0.291]\n",
      "[Epoch 116/2000] [Batch 0/11] [D loss: 0.5058] [G loss: 1.5756] [ADA p: 0.303]\n",
      "[Epoch 117/2000] [Batch 0/11] [D loss: 0.6665] [G loss: 0.6711] [ADA p: 0.315]\n",
      "[Epoch 118/2000] [Batch 0/11] [D loss: 0.5469] [G loss: 1.0705] [ADA p: 0.327]\n",
      "[Epoch 119/2000] [Batch 0/11] [D loss: 0.5661] [G loss: 1.3327] [ADA p: 0.339]\n",
      "[Epoch 120/2000] [Batch 0/11] [D loss: 0.4361] [G loss: 1.1076] [ADA p: 0.351]\n",
      "[Epoch 121/2000] [Batch 0/11] [D loss: 0.6481] [G loss: 1.6392] [ADA p: 0.362]\n",
      "[Epoch 122/2000] [Batch 0/11] [D loss: 0.9228] [G loss: 1.9347] [ADA p: 0.374]\n",
      "[Epoch 123/2000] [Batch 0/11] [D loss: 0.6648] [G loss: 2.0039] [ADA p: 0.385]\n",
      "[Epoch 124/2000] [Batch 0/11] [D loss: 0.4651] [G loss: 1.0461] [ADA p: 0.385]\n",
      "[Epoch 125/2000] [Batch 0/11] [D loss: 1.4102] [G loss: 0.6884] [ADA p: 0.385]\n",
      "[Epoch 126/2000] [Batch 0/11] [D loss: 0.3661] [G loss: 0.8706] [ADA p: 0.385]\n",
      "[Epoch 127/2000] [Batch 0/11] [D loss: 0.8208] [G loss: 1.8885] [ADA p: 0.385]\n",
      "[Epoch 128/2000] [Batch 0/11] [D loss: 0.3534] [G loss: 0.8103] [ADA p: 0.393]\n",
      "[Epoch 129/2000] [Batch 0/11] [D loss: 0.5563] [G loss: 1.1917] [ADA p: 0.404]\n",
      "[Epoch 130/2000] [Batch 0/11] [D loss: 0.7122] [G loss: 0.3066] [ADA p: 0.404]\n",
      "[Epoch 131/2000] [Batch 0/11] [D loss: 0.9121] [G loss: 0.7053] [ADA p: 0.397]\n",
      "[Epoch 132/2000] [Batch 0/11] [D loss: 0.7835] [G loss: 0.2390] [ADA p: 0.385]\n",
      "[Epoch 133/2000] [Batch 0/11] [D loss: 0.5401] [G loss: 0.4967] [ADA p: 0.374]\n",
      "[Epoch 134/2000] [Batch 0/11] [D loss: 0.3205] [G loss: 0.5588] [ADA p: 0.363]\n",
      "[Epoch 135/2000] [Batch 0/11] [D loss: 0.6980] [G loss: 1.3025] [ADA p: 0.351]\n",
      "[Epoch 136/2000] [Batch 0/11] [D loss: 0.5790] [G loss: 1.0718] [ADA p: 0.339]\n",
      "[Epoch 137/2000] [Batch 0/11] [D loss: 0.7719] [G loss: 0.7653] [ADA p: 0.331]\n",
      "[Epoch 138/2000] [Batch 0/11] [D loss: 0.7938] [G loss: 0.8400] [ADA p: 0.331]\n",
      "[Epoch 139/2000] [Batch 0/11] [D loss: 0.9369] [G loss: 1.2050] [ADA p: 0.323]\n",
      "[Epoch 140/2000] [Batch 0/11] [D loss: 0.6795] [G loss: 1.1929] [ADA p: 0.311]\n",
      "[Epoch 141/2000] [Batch 0/11] [D loss: 0.6863] [G loss: 0.8829] [ADA p: 0.299]\n",
      "[Epoch 142/2000] [Batch 0/11] [D loss: 0.6061] [G loss: 0.6503] [ADA p: 0.287]\n",
      "[Epoch 143/2000] [Batch 0/11] [D loss: 0.5989] [G loss: 1.2873] [ADA p: 0.275]\n",
      "[Epoch 144/2000] [Batch 0/11] [D loss: 0.6600] [G loss: 1.3179] [ADA p: 0.262]\n",
      "[Epoch 145/2000] [Batch 0/11] [D loss: 0.7248] [G loss: 1.2367] [ADA p: 0.250]\n",
      "[Epoch 146/2000] [Batch 0/11] [D loss: 0.6269] [G loss: 0.6392] [ADA p: 0.237]\n",
      "[Epoch 147/2000] [Batch 0/11] [D loss: 0.6950] [G loss: 1.8143] [ADA p: 0.224]\n",
      "[Epoch 148/2000] [Batch 0/11] [D loss: 1.0133] [G loss: 1.6114] [ADA p: 0.211]\n",
      "[Epoch 149/2000] [Batch 0/11] [D loss: 0.5166] [G loss: 1.4537] [ADA p: 0.198]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.34it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149 | FID: 365.21 | ADA p: 0.189\n",
      "[Epoch 150/2000] [Batch 0/11] [D loss: 0.6288] [G loss: 1.5780] [ADA p: 0.185]\n",
      "[Epoch 151/2000] [Batch 0/11] [D loss: 0.3199] [G loss: 0.6099] [ADA p: 0.172]\n",
      "[Epoch 152/2000] [Batch 0/11] [D loss: 0.5064] [G loss: 0.9499] [ADA p: 0.158]\n",
      "[Epoch 153/2000] [Batch 0/11] [D loss: 0.6290] [G loss: 1.4900] [ADA p: 0.145]\n",
      "[Epoch 154/2000] [Batch 0/11] [D loss: 0.6070] [G loss: 1.1239] [ADA p: 0.131]\n",
      "[Epoch 155/2000] [Batch 0/11] [D loss: 0.6447] [G loss: 0.8485] [ADA p: 0.117]\n",
      "[Epoch 156/2000] [Batch 0/11] [D loss: 0.6220] [G loss: 0.9691] [ADA p: 0.103]\n",
      "[Epoch 157/2000] [Batch 0/11] [D loss: 0.7177] [G loss: 1.1929] [ADA p: 0.089]\n",
      "[Epoch 158/2000] [Batch 0/11] [D loss: 0.4199] [G loss: 0.8983] [ADA p: 0.075]\n",
      "[Epoch 159/2000] [Batch 0/11] [D loss: 0.4522] [G loss: 0.7930] [ADA p: 0.060]\n",
      "[Epoch 160/2000] [Batch 0/11] [D loss: 0.6656] [G loss: 0.8666] [ADA p: 0.056]\n",
      "[Epoch 161/2000] [Batch 0/11] [D loss: 0.4030] [G loss: 0.5824] [ADA p: 0.056]\n",
      "[Epoch 162/2000] [Batch 0/11] [D loss: 0.5346] [G loss: 1.3163] [ADA p: 0.056]\n",
      "[Epoch 163/2000] [Batch 0/11] [D loss: 0.4211] [G loss: 0.7459] [ADA p: 0.056]\n",
      "[Epoch 164/2000] [Batch 0/11] [D loss: 0.4478] [G loss: 0.7906] [ADA p: 0.041]\n",
      "[Epoch 165/2000] [Batch 0/11] [D loss: 0.5369] [G loss: 1.2465] [ADA p: 0.026]\n",
      "[Epoch 166/2000] [Batch 0/11] [D loss: 0.5939] [G loss: 0.6615] [ADA p: 0.012]\n",
      "[Epoch 167/2000] [Batch 0/11] [D loss: 0.5529] [G loss: 1.3753] [ADA p: 0.000]\n",
      "[Epoch 168/2000] [Batch 0/11] [D loss: 0.6838] [G loss: 0.7470] [ADA p: 0.000]\n",
      "[Epoch 169/2000] [Batch 0/11] [D loss: 0.7707] [G loss: 0.8298] [ADA p: 0.000]\n",
      "[Epoch 170/2000] [Batch 0/11] [D loss: 0.4066] [G loss: 1.0894] [ADA p: 0.000]\n",
      "[Epoch 171/2000] [Batch 0/11] [D loss: 0.6097] [G loss: 0.9025] [ADA p: 0.000]\n",
      "[Epoch 172/2000] [Batch 0/11] [D loss: 0.5067] [G loss: 0.9391] [ADA p: 0.000]\n",
      "[Epoch 173/2000] [Batch 0/11] [D loss: 0.6058] [G loss: 1.2302] [ADA p: 0.000]\n",
      "[Epoch 174/2000] [Batch 0/11] [D loss: 0.6030] [G loss: 0.9694] [ADA p: 0.000]\n",
      "[Epoch 175/2000] [Batch 0/11] [D loss: 0.5066] [G loss: 1.5573] [ADA p: 0.000]\n",
      "[Epoch 176/2000] [Batch 0/11] [D loss: 0.4415] [G loss: 0.4272] [ADA p: 0.000]\n",
      "[Epoch 177/2000] [Batch 0/11] [D loss: 0.3289] [G loss: 0.6944] [ADA p: 0.000]\n",
      "[Epoch 178/2000] [Batch 0/11] [D loss: 0.5115] [G loss: 0.7382] [ADA p: 0.000]\n",
      "[Epoch 179/2000] [Batch 0/11] [D loss: 0.5477] [G loss: 0.9222] [ADA p: 0.000]\n",
      "[Epoch 180/2000] [Batch 0/11] [D loss: 0.9406] [G loss: 1.1870] [ADA p: 0.000]\n",
      "[Epoch 181/2000] [Batch 0/11] [D loss: 0.5808] [G loss: 0.9970] [ADA p: 0.000]\n",
      "[Epoch 182/2000] [Batch 0/11] [D loss: 0.5278] [G loss: 0.7631] [ADA p: 0.000]\n",
      "[Epoch 183/2000] [Batch 0/11] [D loss: 0.6776] [G loss: 1.0825] [ADA p: 0.000]\n",
      "[Epoch 184/2000] [Batch 0/11] [D loss: 0.5578] [G loss: 1.3701] [ADA p: 0.000]\n",
      "[Epoch 185/2000] [Batch 0/11] [D loss: 0.7756] [G loss: 0.9502] [ADA p: 0.000]\n",
      "[Epoch 186/2000] [Batch 0/11] [D loss: 0.7582] [G loss: 1.2608] [ADA p: 0.000]\n",
      "[Epoch 187/2000] [Batch 0/11] [D loss: 0.5980] [G loss: 1.1549] [ADA p: 0.000]\n",
      "[Epoch 188/2000] [Batch 0/11] [D loss: 0.5314] [G loss: 1.3583] [ADA p: 0.000]\n",
      "[Epoch 189/2000] [Batch 0/11] [D loss: 0.5210] [G loss: 1.4515] [ADA p: 0.000]\n",
      "[Epoch 190/2000] [Batch 0/11] [D loss: 0.5070] [G loss: 0.8901] [ADA p: 0.000]\n",
      "[Epoch 191/2000] [Batch 0/11] [D loss: 0.7106] [G loss: 0.9735] [ADA p: 0.000]\n",
      "[Epoch 192/2000] [Batch 0/11] [D loss: 0.6086] [G loss: 1.0293] [ADA p: 0.000]\n",
      "[Epoch 193/2000] [Batch 0/11] [D loss: 0.7975] [G loss: 1.2276] [ADA p: 0.000]\n",
      "[Epoch 194/2000] [Batch 0/11] [D loss: 0.7403] [G loss: 1.0263] [ADA p: 0.000]\n",
      "[Epoch 195/2000] [Batch 0/11] [D loss: 0.8959] [G loss: 0.7135] [ADA p: 0.000]\n",
      "[Epoch 196/2000] [Batch 0/11] [D loss: 0.4230] [G loss: 1.1606] [ADA p: 0.000]\n",
      "[Epoch 197/2000] [Batch 0/11] [D loss: 0.8731] [G loss: 0.8312] [ADA p: 0.000]\n",
      "[Epoch 198/2000] [Batch 0/11] [D loss: 0.5024] [G loss: 1.3167] [ADA p: 0.000]\n",
      "[Epoch 199/2000] [Batch 0/11] [D loss: 0.6049] [G loss: 0.8006] [ADA p: 0.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.38it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199 | FID: 342.72 | ADA p: 0.000\n",
      "[Epoch 200/2000] [Batch 0/11] [D loss: 0.6730] [G loss: 1.0416] [ADA p: 0.000]\n",
      "[Epoch 201/2000] [Batch 0/11] [D loss: 0.6961] [G loss: 0.7980] [ADA p: 0.000]\n",
      "[Epoch 202/2000] [Batch 0/11] [D loss: 0.5624] [G loss: 0.8133] [ADA p: 0.000]\n",
      "[Epoch 203/2000] [Batch 0/11] [D loss: 0.7716] [G loss: 0.7059] [ADA p: 0.000]\n",
      "[Epoch 204/2000] [Batch 0/11] [D loss: 0.6115] [G loss: 0.6323] [ADA p: 0.000]\n",
      "[Epoch 205/2000] [Batch 0/11] [D loss: 0.7176] [G loss: 0.8666] [ADA p: 0.000]\n",
      "[Epoch 206/2000] [Batch 0/11] [D loss: 0.7080] [G loss: 0.8904] [ADA p: 0.000]\n",
      "[Epoch 207/2000] [Batch 0/11] [D loss: 0.5629] [G loss: 0.6928] [ADA p: 0.000]\n",
      "[Epoch 208/2000] [Batch 0/11] [D loss: 0.5010] [G loss: 0.9384] [ADA p: 0.000]\n",
      "[Epoch 209/2000] [Batch 0/11] [D loss: 0.3686] [G loss: 1.7761] [ADA p: 0.000]\n",
      "[Epoch 210/2000] [Batch 0/11] [D loss: 0.6677] [G loss: 0.9562] [ADA p: 0.000]\n",
      "[Epoch 211/2000] [Batch 0/11] [D loss: 0.5501] [G loss: 0.6636] [ADA p: 0.000]\n",
      "[Epoch 212/2000] [Batch 0/11] [D loss: 0.8345] [G loss: 1.0324] [ADA p: 0.000]\n",
      "[Epoch 213/2000] [Batch 0/11] [D loss: 0.5310] [G loss: 1.3671] [ADA p: 0.000]\n",
      "[Epoch 214/2000] [Batch 0/11] [D loss: 0.6520] [G loss: 1.2155] [ADA p: 0.000]\n",
      "[Epoch 215/2000] [Batch 0/11] [D loss: 0.6612] [G loss: 1.0245] [ADA p: 0.000]\n",
      "[Epoch 216/2000] [Batch 0/11] [D loss: 0.5491] [G loss: 0.9920] [ADA p: 0.000]\n",
      "[Epoch 217/2000] [Batch 0/11] [D loss: 0.4539] [G loss: 1.0149] [ADA p: 0.000]\n",
      "[Epoch 218/2000] [Batch 0/11] [D loss: 0.4909] [G loss: 1.4369] [ADA p: 0.000]\n",
      "[Epoch 219/2000] [Batch 0/11] [D loss: 0.4626] [G loss: 0.9654] [ADA p: 0.000]\n",
      "[Epoch 220/2000] [Batch 0/11] [D loss: 0.4847] [G loss: 0.9282] [ADA p: 0.000]\n",
      "[Epoch 221/2000] [Batch 0/11] [D loss: 0.6103] [G loss: 1.2793] [ADA p: 0.000]\n",
      "[Epoch 222/2000] [Batch 0/11] [D loss: 0.5559] [G loss: 1.5546] [ADA p: 0.000]\n",
      "[Epoch 223/2000] [Batch 0/11] [D loss: 0.5361] [G loss: 1.1578] [ADA p: 0.000]\n",
      "[Epoch 224/2000] [Batch 0/11] [D loss: 0.4900] [G loss: 1.2669] [ADA p: 0.000]\n",
      "[Epoch 225/2000] [Batch 0/11] [D loss: 0.5327] [G loss: 1.2526] [ADA p: 0.000]\n",
      "[Epoch 226/2000] [Batch 0/11] [D loss: 0.5159] [G loss: 1.6474] [ADA p: 0.000]\n",
      "[Epoch 227/2000] [Batch 0/11] [D loss: 0.6100] [G loss: 0.7276] [ADA p: 0.000]\n",
      "[Epoch 228/2000] [Batch 0/11] [D loss: 0.5934] [G loss: 1.3355] [ADA p: 0.000]\n",
      "[Epoch 229/2000] [Batch 0/11] [D loss: 0.6929] [G loss: 0.8714] [ADA p: 0.000]\n",
      "[Epoch 230/2000] [Batch 0/11] [D loss: 0.4987] [G loss: 0.9517] [ADA p: 0.000]\n",
      "[Epoch 231/2000] [Batch 0/11] [D loss: 0.5919] [G loss: 0.5319] [ADA p: 0.000]\n",
      "[Epoch 232/2000] [Batch 0/11] [D loss: 0.5734] [G loss: 1.3382] [ADA p: 0.000]\n",
      "[Epoch 233/2000] [Batch 0/11] [D loss: 0.5045] [G loss: 1.3608] [ADA p: 0.000]\n",
      "[Epoch 234/2000] [Batch 0/11] [D loss: 0.5307] [G loss: 1.3080] [ADA p: 0.000]\n",
      "[Epoch 235/2000] [Batch 0/11] [D loss: 0.5329] [G loss: 1.3924] [ADA p: 0.000]\n",
      "[Epoch 236/2000] [Batch 0/11] [D loss: 0.5386] [G loss: 1.0057] [ADA p: 0.000]\n",
      "[Epoch 237/2000] [Batch 0/11] [D loss: 0.5762] [G loss: 0.6053] [ADA p: 0.000]\n",
      "[Epoch 238/2000] [Batch 0/11] [D loss: 0.5431] [G loss: 1.9154] [ADA p: 0.000]\n",
      "[Epoch 239/2000] [Batch 0/11] [D loss: 0.5213] [G loss: 0.5564] [ADA p: 0.000]\n",
      "[Epoch 240/2000] [Batch 0/11] [D loss: 0.5451] [G loss: 0.7780] [ADA p: 0.000]\n",
      "[Epoch 241/2000] [Batch 0/11] [D loss: 0.5506] [G loss: 0.8951] [ADA p: 0.000]\n",
      "[Epoch 242/2000] [Batch 0/11] [D loss: 0.7450] [G loss: 0.7133] [ADA p: 0.000]\n",
      "[Epoch 243/2000] [Batch 0/11] [D loss: 0.4379] [G loss: 0.8676] [ADA p: 0.000]\n",
      "[Epoch 244/2000] [Batch 0/11] [D loss: 0.4764] [G loss: 1.1608] [ADA p: 0.000]\n",
      "[Epoch 245/2000] [Batch 0/11] [D loss: 0.4495] [G loss: 0.7028] [ADA p: 0.000]\n",
      "[Epoch 246/2000] [Batch 0/11] [D loss: 0.5218] [G loss: 1.0447] [ADA p: 0.000]\n",
      "[Epoch 247/2000] [Batch 0/11] [D loss: 0.5538] [G loss: 1.1241] [ADA p: 0.000]\n",
      "[Epoch 248/2000] [Batch 0/11] [D loss: 0.4259] [G loss: 1.1704] [ADA p: 0.000]\n",
      "[Epoch 249/2000] [Batch 0/11] [D loss: 0.8016] [G loss: 1.0739] [ADA p: 0.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.34it/s]\n",
      "100%|██████████| 63/63 [00:12<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 249 | FID: 363.54 | ADA p: 0.000\n",
      "[Epoch 250/2000] [Batch 0/11] [D loss: 0.6433] [G loss: 1.5422] [ADA p: 0.000]\n",
      "[Epoch 251/2000] [Batch 0/11] [D loss: 0.4612] [G loss: 0.4498] [ADA p: 0.000]\n",
      "[Epoch 252/2000] [Batch 0/11] [D loss: 0.6207] [G loss: 1.1952] [ADA p: 0.000]\n",
      "[Epoch 253/2000] [Batch 0/11] [D loss: 0.5101] [G loss: 0.8832] [ADA p: 0.000]\n",
      "[Epoch 254/2000] [Batch 0/11] [D loss: 0.5490] [G loss: 1.4810] [ADA p: 0.000]\n",
      "[Epoch 255/2000] [Batch 0/11] [D loss: 0.3379] [G loss: 1.5508] [ADA p: 0.000]\n",
      "[Epoch 256/2000] [Batch 0/11] [D loss: 0.5038] [G loss: 1.2118] [ADA p: 0.000]\n",
      "[Epoch 257/2000] [Batch 0/11] [D loss: 0.3445] [G loss: 1.2328] [ADA p: 0.000]\n",
      "[Epoch 258/2000] [Batch 0/11] [D loss: 0.7307] [G loss: 1.8232] [ADA p: 0.000]\n",
      "[Epoch 259/2000] [Batch 0/11] [D loss: 0.2936] [G loss: 1.5350] [ADA p: 0.000]\n",
      "[Epoch 260/2000] [Batch 0/11] [D loss: 0.3977] [G loss: 1.3613] [ADA p: 0.000]\n",
      "[Epoch 261/2000] [Batch 0/11] [D loss: 0.3377] [G loss: 1.0659] [ADA p: 0.005]\n",
      "[Epoch 262/2000] [Batch 0/11] [D loss: 0.5165] [G loss: 1.0653] [ADA p: 0.005]\n",
      "[Epoch 263/2000] [Batch 0/11] [D loss: 0.8435] [G loss: 1.1189] [ADA p: 0.005]\n",
      "[Epoch 264/2000] [Batch 0/11] [D loss: 0.4436] [G loss: 1.0945] [ADA p: 0.000]\n",
      "[Epoch 265/2000] [Batch 0/11] [D loss: 0.6192] [G loss: 0.7972] [ADA p: 0.000]\n",
      "[Epoch 266/2000] [Batch 0/11] [D loss: 0.3743] [G loss: 1.0994] [ADA p: 0.000]\n",
      "[Epoch 267/2000] [Batch 0/11] [D loss: 0.5132] [G loss: 1.4265] [ADA p: 0.000]\n",
      "[Epoch 268/2000] [Batch 0/11] [D loss: 0.3543] [G loss: 1.3040] [ADA p: 0.000]\n",
      "[Epoch 269/2000] [Batch 0/11] [D loss: 0.6472] [G loss: 1.0102] [ADA p: 0.000]\n",
      "[Epoch 270/2000] [Batch 0/11] [D loss: 0.5209] [G loss: 0.7580] [ADA p: 0.000]\n",
      "[Epoch 271/2000] [Batch 0/11] [D loss: 0.6773] [G loss: 1.0544] [ADA p: 0.000]\n",
      "[Epoch 272/2000] [Batch 0/11] [D loss: 0.4281] [G loss: 1.0038] [ADA p: 0.000]\n",
      "[Epoch 273/2000] [Batch 0/11] [D loss: 0.6527] [G loss: 0.5802] [ADA p: 0.000]\n",
      "[Epoch 274/2000] [Batch 0/11] [D loss: 0.5131] [G loss: 0.6451] [ADA p: 0.000]\n",
      "[Epoch 275/2000] [Batch 0/11] [D loss: 0.4794] [G loss: 0.6697] [ADA p: 0.000]\n",
      "[Epoch 276/2000] [Batch 0/11] [D loss: 0.3628] [G loss: 1.7508] [ADA p: 0.000]\n",
      "[Epoch 277/2000] [Batch 0/11] [D loss: 0.4703] [G loss: 1.3280] [ADA p: 0.000]\n",
      "[Epoch 278/2000] [Batch 0/11] [D loss: 0.4802] [G loss: 0.4797] [ADA p: 0.010]\n",
      "[Epoch 279/2000] [Batch 0/11] [D loss: 0.6402] [G loss: 1.7681] [ADA p: 0.025]\n",
      "[Epoch 280/2000] [Batch 0/11] [D loss: 0.8979] [G loss: 1.1615] [ADA p: 0.040]\n",
      "[Epoch 281/2000] [Batch 0/11] [D loss: 0.4892] [G loss: 2.1313] [ADA p: 0.044]\n",
      "[Epoch 282/2000] [Batch 0/11] [D loss: 0.4225] [G loss: 1.0643] [ADA p: 0.040]\n",
      "[Epoch 283/2000] [Batch 0/11] [D loss: 0.5931] [G loss: 0.9562] [ADA p: 0.025]\n",
      "[Epoch 284/2000] [Batch 0/11] [D loss: 0.4456] [G loss: 1.0369] [ADA p: 0.015]\n",
      "[Epoch 285/2000] [Batch 0/11] [D loss: 0.7210] [G loss: 1.4584] [ADA p: 0.015]\n",
      "[Epoch 286/2000] [Batch 0/11] [D loss: 0.5296] [G loss: 0.9206] [ADA p: 0.015]\n",
      "[Epoch 287/2000] [Batch 0/11] [D loss: 0.5419] [G loss: 0.8101] [ADA p: 0.015]\n",
      "[Epoch 288/2000] [Batch 0/11] [D loss: 0.4291] [G loss: 1.0243] [ADA p: 0.025]\n",
      "[Epoch 289/2000] [Batch 0/11] [D loss: 0.6826] [G loss: 0.6142] [ADA p: 0.040]\n",
      "[Epoch 290/2000] [Batch 0/11] [D loss: 0.4000] [G loss: 1.7891] [ADA p: 0.054]\n",
      "[Epoch 291/2000] [Batch 0/11] [D loss: 0.5293] [G loss: 0.9815] [ADA p: 0.069]\n",
      "[Epoch 292/2000] [Batch 0/11] [D loss: 0.7968] [G loss: 1.3584] [ADA p: 0.069]\n",
      "[Epoch 293/2000] [Batch 0/11] [D loss: 0.4471] [G loss: 0.9161] [ADA p: 0.059]\n",
      "[Epoch 294/2000] [Batch 0/11] [D loss: 0.4898] [G loss: 0.8743] [ADA p: 0.054]\n",
      "[Epoch 295/2000] [Batch 0/11] [D loss: 0.5577] [G loss: 0.9404] [ADA p: 0.054]\n",
      "[Epoch 296/2000] [Batch 0/11] [D loss: 0.5454] [G loss: 1.0303] [ADA p: 0.045]\n",
      "[Epoch 297/2000] [Batch 0/11] [D loss: 0.3797] [G loss: 1.0364] [ADA p: 0.030]\n",
      "[Epoch 298/2000] [Batch 0/11] [D loss: 0.5235] [G loss: 1.0343] [ADA p: 0.015]\n",
      "[Epoch 299/2000] [Batch 0/11] [D loss: 0.4814] [G loss: 1.3173] [ADA p: 0.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.80it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299 | FID: 356.87 | ADA p: 0.000\n",
      "[Epoch 300/2000] [Batch 0/11] [D loss: 0.4831] [G loss: 1.0091] [ADA p: 0.000]\n",
      "[Epoch 301/2000] [Batch 0/11] [D loss: 0.5734] [G loss: 0.9480] [ADA p: 0.000]\n",
      "[Epoch 302/2000] [Batch 0/11] [D loss: 0.6905] [G loss: 0.5503] [ADA p: 0.000]\n",
      "[Epoch 303/2000] [Batch 0/11] [D loss: 0.6438] [G loss: 1.3657] [ADA p: 0.000]\n",
      "[Epoch 304/2000] [Batch 0/11] [D loss: 0.4328] [G loss: 0.9545] [ADA p: 0.000]\n",
      "[Epoch 305/2000] [Batch 0/11] [D loss: 0.4626] [G loss: 1.3736] [ADA p: 0.000]\n",
      "[Epoch 306/2000] [Batch 0/11] [D loss: 0.4791] [G loss: 1.0530] [ADA p: 0.000]\n",
      "[Epoch 307/2000] [Batch 0/11] [D loss: 0.6572] [G loss: 0.7822] [ADA p: 0.000]\n",
      "[Epoch 308/2000] [Batch 0/11] [D loss: 0.4233] [G loss: 1.2331] [ADA p: 0.015]\n",
      "[Epoch 309/2000] [Batch 0/11] [D loss: 0.5360] [G loss: 1.5796] [ADA p: 0.025]\n",
      "[Epoch 310/2000] [Batch 0/11] [D loss: 0.4173] [G loss: 0.7546] [ADA p: 0.025]\n",
      "[Epoch 311/2000] [Batch 0/11] [D loss: 0.5538] [G loss: 1.3323] [ADA p: 0.025]\n",
      "[Epoch 312/2000] [Batch 0/11] [D loss: 0.7034] [G loss: 0.8978] [ADA p: 0.025]\n",
      "[Epoch 313/2000] [Batch 0/11] [D loss: 0.3463] [G loss: 0.9195] [ADA p: 0.020]\n",
      "[Epoch 314/2000] [Batch 0/11] [D loss: 0.7060] [G loss: 0.9197] [ADA p: 0.005]\n",
      "[Epoch 315/2000] [Batch 0/11] [D loss: 0.5432] [G loss: 1.1708] [ADA p: 0.000]\n",
      "[Epoch 316/2000] [Batch 0/11] [D loss: 0.7185] [G loss: 1.1549] [ADA p: 0.000]\n",
      "[Epoch 317/2000] [Batch 0/11] [D loss: 0.3620] [G loss: 0.9242] [ADA p: 0.000]\n",
      "[Epoch 318/2000] [Batch 0/11] [D loss: 0.4262] [G loss: 0.7978] [ADA p: 0.000]\n",
      "[Epoch 319/2000] [Batch 0/11] [D loss: 0.5035] [G loss: 1.2304] [ADA p: 0.000]\n",
      "[Epoch 320/2000] [Batch 0/11] [D loss: 0.5554] [G loss: 1.1880] [ADA p: 0.000]\n",
      "[Epoch 321/2000] [Batch 0/11] [D loss: 0.5020] [G loss: 1.4860] [ADA p: 0.000]\n",
      "[Epoch 322/2000] [Batch 0/11] [D loss: 0.5267] [G loss: 1.0621] [ADA p: 0.000]\n",
      "[Epoch 323/2000] [Batch 0/11] [D loss: 0.5246] [G loss: 1.8052] [ADA p: 0.000]\n",
      "[Epoch 324/2000] [Batch 0/11] [D loss: 0.6657] [G loss: 1.0170] [ADA p: 0.000]\n",
      "[Epoch 325/2000] [Batch 0/11] [D loss: 0.6069] [G loss: 0.8847] [ADA p: 0.000]\n",
      "[Epoch 326/2000] [Batch 0/11] [D loss: 0.6195] [G loss: 0.9156] [ADA p: 0.000]\n",
      "[Epoch 327/2000] [Batch 0/11] [D loss: 0.5150] [G loss: 1.0762] [ADA p: 0.000]\n",
      "[Epoch 328/2000] [Batch 0/11] [D loss: 0.6747] [G loss: 1.0031] [ADA p: 0.000]\n",
      "[Epoch 329/2000] [Batch 0/11] [D loss: 0.6251] [G loss: 1.3192] [ADA p: 0.000]\n",
      "[Epoch 330/2000] [Batch 0/11] [D loss: 0.4601] [G loss: 1.1951] [ADA p: 0.000]\n",
      "[Epoch 331/2000] [Batch 0/11] [D loss: 0.6939] [G loss: 1.1305] [ADA p: 0.000]\n",
      "[Epoch 332/2000] [Batch 0/11] [D loss: 0.5723] [G loss: 1.0617] [ADA p: 0.000]\n",
      "[Epoch 333/2000] [Batch 0/11] [D loss: 0.5806] [G loss: 1.2286] [ADA p: 0.000]\n",
      "[Epoch 334/2000] [Batch 0/11] [D loss: 0.6083] [G loss: 0.9187] [ADA p: 0.000]\n",
      "[Epoch 335/2000] [Batch 0/11] [D loss: 0.8515] [G loss: 0.6568] [ADA p: 0.000]\n",
      "[Epoch 336/2000] [Batch 0/11] [D loss: 0.6221] [G loss: 1.2912] [ADA p: 0.000]\n",
      "[Epoch 337/2000] [Batch 0/11] [D loss: 0.4335] [G loss: 1.3759] [ADA p: 0.000]\n",
      "[Epoch 338/2000] [Batch 0/11] [D loss: 0.8139] [G loss: 1.0373] [ADA p: 0.000]\n",
      "[Epoch 339/2000] [Batch 0/11] [D loss: 0.4779] [G loss: 0.9402] [ADA p: 0.000]\n",
      "[Epoch 340/2000] [Batch 0/11] [D loss: 0.5275] [G loss: 0.6750] [ADA p: 0.000]\n",
      "[Epoch 341/2000] [Batch 0/11] [D loss: 0.3870] [G loss: 1.5969] [ADA p: 0.000]\n",
      "[Epoch 342/2000] [Batch 0/11] [D loss: 0.4541] [G loss: 1.3042] [ADA p: 0.000]\n",
      "[Epoch 343/2000] [Batch 0/11] [D loss: 0.5020] [G loss: 0.7418] [ADA p: 0.000]\n",
      "[Epoch 344/2000] [Batch 0/11] [D loss: 0.5871] [G loss: 0.7246] [ADA p: 0.000]\n",
      "[Epoch 345/2000] [Batch 0/11] [D loss: 0.4949] [G loss: 0.6000] [ADA p: 0.000]\n",
      "[Epoch 346/2000] [Batch 0/11] [D loss: 0.4689] [G loss: 0.9692] [ADA p: 0.000]\n",
      "[Epoch 347/2000] [Batch 0/11] [D loss: 0.9737] [G loss: 0.4919] [ADA p: 0.000]\n",
      "[Epoch 348/2000] [Batch 0/11] [D loss: 0.6846] [G loss: 0.8139] [ADA p: 0.000]\n",
      "[Epoch 349/2000] [Batch 0/11] [D loss: 0.6033] [G loss: 1.1126] [ADA p: 0.000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.33it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 349 | FID: 348.99 | ADA p: 0.000\n",
      "[Epoch 350/2000] [Batch 0/11] [D loss: 0.7973] [G loss: 0.9446] [ADA p: 0.000]\n",
      "[Epoch 351/2000] [Batch 0/11] [D loss: 0.5904] [G loss: 1.1426] [ADA p: 0.000]\n",
      "[Epoch 352/2000] [Batch 0/11] [D loss: 0.6384] [G loss: 0.8478] [ADA p: 0.000]\n",
      "[Epoch 353/2000] [Batch 0/11] [D loss: 0.6337] [G loss: 1.0058] [ADA p: 0.000]\n",
      "[Epoch 354/2000] [Batch 0/11] [D loss: 0.4713] [G loss: 1.2877] [ADA p: 0.000]\n",
      "[Epoch 355/2000] [Batch 0/11] [D loss: 0.4820] [G loss: 0.7344] [ADA p: 0.000]\n",
      "[Epoch 356/2000] [Batch 0/11] [D loss: 0.7237] [G loss: 1.2164] [ADA p: 0.000]\n",
      "[Epoch 357/2000] [Batch 0/11] [D loss: 0.5641] [G loss: 0.9994] [ADA p: 0.000]\n",
      "[Epoch 358/2000] [Batch 0/11] [D loss: 0.5198] [G loss: 0.8539] [ADA p: 0.000]\n",
      "[Epoch 359/2000] [Batch 0/11] [D loss: 0.6746] [G loss: 1.3311] [ADA p: 0.000]\n",
      "[Epoch 360/2000] [Batch 0/11] [D loss: 0.7718] [G loss: 0.4108] [ADA p: 0.000]\n",
      "[Epoch 361/2000] [Batch 0/11] [D loss: 0.6019] [G loss: 0.9977] [ADA p: 0.000]\n",
      "[Epoch 362/2000] [Batch 0/11] [D loss: 0.6468] [G loss: 1.2400] [ADA p: 0.000]\n",
      "[Epoch 363/2000] [Batch 0/11] [D loss: 0.5623] [G loss: 0.6624] [ADA p: 0.000]\n",
      "[Epoch 364/2000] [Batch 0/11] [D loss: 0.6574] [G loss: 1.0524] [ADA p: 0.000]\n",
      "[Epoch 365/2000] [Batch 0/11] [D loss: 0.3752] [G loss: 0.9546] [ADA p: 0.000]\n",
      "[Epoch 366/2000] [Batch 0/11] [D loss: 0.5886] [G loss: 0.7856] [ADA p: 0.000]\n",
      "[Epoch 367/2000] [Batch 0/11] [D loss: 0.5048] [G loss: 0.9169] [ADA p: 0.000]\n",
      "[Epoch 368/2000] [Batch 0/11] [D loss: 0.6990] [G loss: 0.6291] [ADA p: 0.000]\n",
      "[Epoch 369/2000] [Batch 0/11] [D loss: 0.4933] [G loss: 1.4913] [ADA p: 0.000]\n",
      "[Epoch 370/2000] [Batch 0/11] [D loss: 0.4498] [G loss: 1.4350] [ADA p: 0.000]\n",
      "[Epoch 371/2000] [Batch 0/11] [D loss: 0.4862] [G loss: 0.8285] [ADA p: 0.000]\n",
      "[Epoch 372/2000] [Batch 0/11] [D loss: 0.3553] [G loss: 1.0111] [ADA p: 0.015]\n",
      "[Epoch 373/2000] [Batch 0/11] [D loss: 0.8728] [G loss: 1.0746] [ADA p: 0.015]\n",
      "[Epoch 374/2000] [Batch 0/11] [D loss: 0.4673] [G loss: 1.5284] [ADA p: 0.005]\n",
      "[Epoch 375/2000] [Batch 0/11] [D loss: 0.5664] [G loss: 0.4051] [ADA p: 0.000]\n",
      "[Epoch 376/2000] [Batch 0/11] [D loss: 0.4796] [G loss: 0.7662] [ADA p: 0.000]\n",
      "[Epoch 377/2000] [Batch 0/11] [D loss: 0.8272] [G loss: 1.1039] [ADA p: 0.000]\n",
      "[Epoch 378/2000] [Batch 0/11] [D loss: 0.6071] [G loss: 0.8271] [ADA p: 0.000]\n",
      "[Epoch 379/2000] [Batch 0/11] [D loss: 0.5283] [G loss: 1.1709] [ADA p: 0.000]\n",
      "[Epoch 380/2000] [Batch 0/11] [D loss: 0.6475] [G loss: 0.6707] [ADA p: 0.000]\n",
      "[Epoch 381/2000] [Batch 0/11] [D loss: 0.5768] [G loss: 0.5984] [ADA p: 0.000]\n",
      "[Epoch 382/2000] [Batch 0/11] [D loss: 0.5522] [G loss: 0.7078] [ADA p: 0.000]\n",
      "[Epoch 383/2000] [Batch 0/11] [D loss: 0.4752] [G loss: 0.7942] [ADA p: 0.000]\n",
      "[Epoch 384/2000] [Batch 0/11] [D loss: 0.6237] [G loss: 0.4726] [ADA p: 0.015]\n",
      "[Epoch 385/2000] [Batch 0/11] [D loss: 0.5269] [G loss: 1.1895] [ADA p: 0.020]\n",
      "[Epoch 386/2000] [Batch 0/11] [D loss: 0.5318] [G loss: 0.7083] [ADA p: 0.020]\n",
      "[Epoch 387/2000] [Batch 0/11] [D loss: 0.4881] [G loss: 1.5934] [ADA p: 0.020]\n",
      "[Epoch 388/2000] [Batch 0/11] [D loss: 0.6312] [G loss: 1.0121] [ADA p: 0.030]\n",
      "[Epoch 389/2000] [Batch 0/11] [D loss: 0.4860] [G loss: 0.6547] [ADA p: 0.044]\n",
      "[Epoch 390/2000] [Batch 0/11] [D loss: 0.6019] [G loss: 0.8066] [ADA p: 0.059]\n",
      "[Epoch 391/2000] [Batch 0/11] [D loss: 0.3812] [G loss: 0.7305] [ADA p: 0.073]\n",
      "[Epoch 392/2000] [Batch 0/11] [D loss: 0.4646] [G loss: 1.0551] [ADA p: 0.088]\n",
      "[Epoch 393/2000] [Batch 0/11] [D loss: 0.7869] [G loss: 1.2033] [ADA p: 0.102]\n",
      "[Epoch 394/2000] [Batch 0/11] [D loss: 0.5935] [G loss: 1.0385] [ADA p: 0.106]\n",
      "[Epoch 395/2000] [Batch 0/11] [D loss: 0.6242] [G loss: 1.1173] [ADA p: 0.106]\n",
      "[Epoch 396/2000] [Batch 0/11] [D loss: 0.9230] [G loss: 1.2349] [ADA p: 0.106]\n",
      "[Epoch 397/2000] [Batch 0/11] [D loss: 0.5628] [G loss: 0.8736] [ADA p: 0.106]\n",
      "[Epoch 398/2000] [Batch 0/11] [D loss: 0.4577] [G loss: 0.6971] [ADA p: 0.106]\n",
      "[Epoch 399/2000] [Batch 0/11] [D loss: 0.5368] [G loss: 0.8796] [ADA p: 0.097]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:10<00:00,  3.06it/s]\n",
      "100%|██████████| 63/63 [00:13<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399 | FID: 341.31 | ADA p: 0.092\n",
      "[Epoch 400/2000] [Batch 0/11] [D loss: 0.5875] [G loss: 1.1829] [ADA p: 0.092]\n",
      "[Epoch 401/2000] [Batch 0/11] [D loss: 0.3973] [G loss: 0.7485] [ADA p: 0.092]\n",
      "[Epoch 402/2000] [Batch 0/11] [D loss: 0.4927] [G loss: 1.0899] [ADA p: 0.092]\n",
      "[Epoch 403/2000] [Batch 0/11] [D loss: 0.3741] [G loss: 1.2620] [ADA p: 0.097]\n",
      "[Epoch 404/2000] [Batch 0/11] [D loss: 0.4441] [G loss: 0.8588] [ADA p: 0.097]\n",
      "[Epoch 405/2000] [Batch 0/11] [D loss: 0.5097] [G loss: 1.0915] [ADA p: 0.092]\n",
      "[Epoch 406/2000] [Batch 0/11] [D loss: 0.5082] [G loss: 1.0399] [ADA p: 0.092]\n",
      "[Epoch 407/2000] [Batch 0/11] [D loss: 0.4823] [G loss: 0.9450] [ADA p: 0.092]\n",
      "[Epoch 408/2000] [Batch 0/11] [D loss: 0.5965] [G loss: 1.6938] [ADA p: 0.092]\n",
      "[Epoch 409/2000] [Batch 0/11] [D loss: 0.4099] [G loss: 0.5116] [ADA p: 0.088]\n",
      "[Epoch 410/2000] [Batch 0/11] [D loss: 0.8959] [G loss: 1.3067] [ADA p: 0.074]\n",
      "[Epoch 411/2000] [Batch 0/11] [D loss: 1.0319] [G loss: 0.9701] [ADA p: 0.059]\n",
      "[Epoch 412/2000] [Batch 0/11] [D loss: 0.6767] [G loss: 0.9126] [ADA p: 0.045]\n",
      "[Epoch 413/2000] [Batch 0/11] [D loss: 0.6481] [G loss: 1.4101] [ADA p: 0.030]\n",
      "[Epoch 414/2000] [Batch 0/11] [D loss: 0.3944] [G loss: 0.9123] [ADA p: 0.015]\n",
      "[Epoch 415/2000] [Batch 0/11] [D loss: 0.4924] [G loss: 1.2990] [ADA p: 0.000]\n",
      "[Epoch 416/2000] [Batch 0/11] [D loss: 0.6300] [G loss: 1.1243] [ADA p: 0.000]\n",
      "[Epoch 417/2000] [Batch 0/11] [D loss: 0.5643] [G loss: 1.5670] [ADA p: 0.000]\n",
      "[Epoch 418/2000] [Batch 0/11] [D loss: 0.4574] [G loss: 1.1904] [ADA p: 0.000]\n",
      "[Epoch 419/2000] [Batch 0/11] [D loss: 0.5443] [G loss: 0.6897] [ADA p: 0.000]\n",
      "[Epoch 420/2000] [Batch 0/11] [D loss: 0.6387] [G loss: 1.1513] [ADA p: 0.000]\n",
      "[Epoch 421/2000] [Batch 0/11] [D loss: 0.3933] [G loss: 1.5770] [ADA p: 0.010]\n",
      "[Epoch 422/2000] [Batch 0/11] [D loss: 0.5309] [G loss: 1.7881] [ADA p: 0.025]\n",
      "[Epoch 423/2000] [Batch 0/11] [D loss: 0.7059] [G loss: 1.1242] [ADA p: 0.040]\n",
      "[Epoch 424/2000] [Batch 0/11] [D loss: 0.2414] [G loss: 1.3844] [ADA p: 0.054]\n",
      "[Epoch 425/2000] [Batch 0/11] [D loss: 0.5696] [G loss: 1.8814] [ADA p: 0.069]\n",
      "[Epoch 426/2000] [Batch 0/11] [D loss: 0.4567] [G loss: 1.5996] [ADA p: 0.083]\n",
      "[Epoch 427/2000] [Batch 0/11] [D loss: 0.2817] [G loss: 1.2946] [ADA p: 0.097]\n",
      "[Epoch 428/2000] [Batch 0/11] [D loss: 0.4529] [G loss: 1.1411] [ADA p: 0.111]\n",
      "[Epoch 429/2000] [Batch 0/11] [D loss: 0.6240] [G loss: 0.5305] [ADA p: 0.125]\n",
      "[Epoch 430/2000] [Batch 0/11] [D loss: 0.5065] [G loss: 0.7551] [ADA p: 0.139]\n",
      "[Epoch 431/2000] [Batch 0/11] [D loss: 0.4524] [G loss: 1.1549] [ADA p: 0.152]\n",
      "[Epoch 432/2000] [Batch 0/11] [D loss: 0.3991] [G loss: 1.2149] [ADA p: 0.166]\n",
      "[Epoch 433/2000] [Batch 0/11] [D loss: 0.3662] [G loss: 1.2158] [ADA p: 0.179]\n",
      "[Epoch 434/2000] [Batch 0/11] [D loss: 0.4070] [G loss: 1.1050] [ADA p: 0.193]\n",
      "[Epoch 435/2000] [Batch 0/11] [D loss: 0.5512] [G loss: 1.3312] [ADA p: 0.206]\n",
      "[Epoch 436/2000] [Batch 0/11] [D loss: 0.4835] [G loss: 1.5338] [ADA p: 0.219]\n",
      "[Epoch 437/2000] [Batch 0/11] [D loss: 0.6332] [G loss: 1.1858] [ADA p: 0.232]\n",
      "[Epoch 438/2000] [Batch 0/11] [D loss: 0.2600] [G loss: 1.3693] [ADA p: 0.245]\n",
      "[Epoch 439/2000] [Batch 0/11] [D loss: 0.2997] [G loss: 0.7064] [ADA p: 0.257]\n",
      "[Epoch 440/2000] [Batch 0/11] [D loss: 0.6342] [G loss: 1.1019] [ADA p: 0.257]\n",
      "[Epoch 441/2000] [Batch 0/11] [D loss: 0.4744] [G loss: 1.1351] [ADA p: 0.249]\n",
      "[Epoch 442/2000] [Batch 0/11] [D loss: 0.6053] [G loss: 1.2312] [ADA p: 0.245]\n",
      "[Epoch 443/2000] [Batch 0/11] [D loss: 0.5422] [G loss: 0.9178] [ADA p: 0.245]\n",
      "[Epoch 444/2000] [Batch 0/11] [D loss: 0.3153] [G loss: 1.1650] [ADA p: 0.240]\n",
      "[Epoch 445/2000] [Batch 0/11] [D loss: 0.3301] [G loss: 0.4844] [ADA p: 0.240]\n",
      "[Epoch 446/2000] [Batch 0/11] [D loss: 0.3829] [G loss: 0.5678] [ADA p: 0.253]\n",
      "[Epoch 447/2000] [Batch 0/11] [D loss: 0.4168] [G loss: 0.9807] [ADA p: 0.266]\n",
      "[Epoch 448/2000] [Batch 0/11] [D loss: 0.5335] [G loss: 1.1877] [ADA p: 0.278]\n",
      "[Epoch 449/2000] [Batch 0/11] [D loss: 0.7625] [G loss: 0.8720] [ADA p: 0.290]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.36it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 449 | FID: 339.54 | ADA p: 0.299\n",
      "[Epoch 450/2000] [Batch 0/11] [D loss: 0.3753] [G loss: 1.0393] [ADA p: 0.303]\n",
      "[Epoch 451/2000] [Batch 0/11] [D loss: 0.4399] [G loss: 0.8648] [ADA p: 0.315]\n",
      "[Epoch 452/2000] [Batch 0/11] [D loss: 0.8840] [G loss: 1.4373] [ADA p: 0.327]\n",
      "[Epoch 453/2000] [Batch 0/11] [D loss: 0.2739] [G loss: 1.0216] [ADA p: 0.339]\n",
      "[Epoch 454/2000] [Batch 0/11] [D loss: 0.4600] [G loss: 1.2890] [ADA p: 0.351]\n",
      "[Epoch 455/2000] [Batch 0/11] [D loss: 0.4308] [G loss: 1.0083] [ADA p: 0.362]\n",
      "[Epoch 456/2000] [Batch 0/11] [D loss: 0.4800] [G loss: 0.7346] [ADA p: 0.366]\n",
      "[Epoch 457/2000] [Batch 0/11] [D loss: 0.6043] [G loss: 0.9212] [ADA p: 0.366]\n",
      "[Epoch 458/2000] [Batch 0/11] [D loss: 1.0662] [G loss: 1.4520] [ADA p: 0.362]\n",
      "[Epoch 459/2000] [Batch 0/11] [D loss: 0.5400] [G loss: 0.9254] [ADA p: 0.351]\n",
      "[Epoch 460/2000] [Batch 0/11] [D loss: 0.6115] [G loss: 0.7831] [ADA p: 0.339]\n",
      "[Epoch 461/2000] [Batch 0/11] [D loss: 0.5369] [G loss: 0.5081] [ADA p: 0.327]\n",
      "[Epoch 462/2000] [Batch 0/11] [D loss: 0.4615] [G loss: 0.8638] [ADA p: 0.323]\n",
      "[Epoch 463/2000] [Batch 0/11] [D loss: 0.3430] [G loss: 1.3290] [ADA p: 0.323]\n",
      "[Epoch 464/2000] [Batch 0/11] [D loss: 0.4077] [G loss: 0.7989] [ADA p: 0.315]\n",
      "[Epoch 465/2000] [Batch 0/11] [D loss: 0.7757] [G loss: 1.2206] [ADA p: 0.315]\n",
      "[Epoch 466/2000] [Batch 0/11] [D loss: 0.3311] [G loss: 0.6480] [ADA p: 0.315]\n",
      "[Epoch 467/2000] [Batch 0/11] [D loss: 0.5757] [G loss: 1.1462] [ADA p: 0.315]\n",
      "[Epoch 468/2000] [Batch 0/11] [D loss: 0.9502] [G loss: 1.2741] [ADA p: 0.315]\n",
      "[Epoch 469/2000] [Batch 0/11] [D loss: 0.4645] [G loss: 1.3569] [ADA p: 0.307]\n",
      "[Epoch 470/2000] [Batch 0/11] [D loss: 0.3584] [G loss: 1.1978] [ADA p: 0.295]\n",
      "[Epoch 471/2000] [Batch 0/11] [D loss: 1.0567] [G loss: 1.0230] [ADA p: 0.283]\n",
      "[Epoch 472/2000] [Batch 0/11] [D loss: 0.7899] [G loss: 0.7249] [ADA p: 0.270]\n",
      "[Epoch 473/2000] [Batch 0/11] [D loss: 0.5557] [G loss: 1.7213] [ADA p: 0.258]\n",
      "[Epoch 474/2000] [Batch 0/11] [D loss: 0.6193] [G loss: 1.1404] [ADA p: 0.245]\n",
      "[Epoch 475/2000] [Batch 0/11] [D loss: 0.4828] [G loss: 0.3176] [ADA p: 0.237]\n",
      "[Epoch 476/2000] [Batch 0/11] [D loss: 0.5957] [G loss: 1.2728] [ADA p: 0.237]\n",
      "[Epoch 477/2000] [Batch 0/11] [D loss: 0.9497] [G loss: 0.8068] [ADA p: 0.237]\n",
      "[Epoch 478/2000] [Batch 0/11] [D loss: 0.5043] [G loss: 1.0007] [ADA p: 0.228]\n",
      "[Epoch 479/2000] [Batch 0/11] [D loss: 0.4228] [G loss: 0.5808] [ADA p: 0.215]\n",
      "[Epoch 480/2000] [Batch 0/11] [D loss: 0.5765] [G loss: 0.5477] [ADA p: 0.202]\n",
      "[Epoch 481/2000] [Batch 0/11] [D loss: 0.5122] [G loss: 0.6513] [ADA p: 0.189]\n",
      "[Epoch 482/2000] [Batch 0/11] [D loss: 0.2885] [G loss: 0.8767] [ADA p: 0.189]\n",
      "[Epoch 483/2000] [Batch 0/11] [D loss: 0.3777] [G loss: 0.8641] [ADA p: 0.189]\n",
      "[Epoch 484/2000] [Batch 0/11] [D loss: 0.3210] [G loss: 0.9062] [ADA p: 0.189]\n",
      "[Epoch 485/2000] [Batch 0/11] [D loss: 0.6681] [G loss: 0.7976] [ADA p: 0.189]\n",
      "[Epoch 486/2000] [Batch 0/11] [D loss: 0.4456] [G loss: 0.9639] [ADA p: 0.180]\n",
      "[Epoch 487/2000] [Batch 0/11] [D loss: 0.5760] [G loss: 1.1228] [ADA p: 0.167]\n",
      "[Epoch 488/2000] [Batch 0/11] [D loss: 0.6050] [G loss: 1.1204] [ADA p: 0.162]\n",
      "[Epoch 489/2000] [Batch 0/11] [D loss: 0.6065] [G loss: 0.9093] [ADA p: 0.167]\n",
      "[Epoch 490/2000] [Batch 0/11] [D loss: 0.3981] [G loss: 1.3893] [ADA p: 0.180]\n",
      "[Epoch 491/2000] [Batch 0/11] [D loss: 0.4372] [G loss: 1.0336] [ADA p: 0.193]\n",
      "[Epoch 492/2000] [Batch 0/11] [D loss: 0.3396] [G loss: 1.2543] [ADA p: 0.207]\n",
      "[Epoch 493/2000] [Batch 0/11] [D loss: 0.5613] [G loss: 0.8552] [ADA p: 0.220]\n",
      "[Epoch 494/2000] [Batch 0/11] [D loss: 0.3706] [G loss: 1.4470] [ADA p: 0.220]\n",
      "[Epoch 495/2000] [Batch 0/11] [D loss: 0.5436] [G loss: 1.1116] [ADA p: 0.220]\n",
      "[Epoch 496/2000] [Batch 0/11] [D loss: 0.5838] [G loss: 1.1839] [ADA p: 0.220]\n",
      "[Epoch 497/2000] [Batch 0/11] [D loss: 0.5610] [G loss: 0.7833] [ADA p: 0.220]\n",
      "[Epoch 498/2000] [Batch 0/11] [D loss: 0.3676] [G loss: 1.4717] [ADA p: 0.228]\n",
      "[Epoch 499/2000] [Batch 0/11] [D loss: 0.4664] [G loss: 0.8162] [ADA p: 0.241]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.24it/s]\n",
      "100%|██████████| 63/63 [00:12<00:00,  5.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 499 | FID: 331.66 | ADA p: 0.241\n",
      "[Epoch 500/2000] [Batch 0/11] [D loss: 0.5572] [G loss: 1.3939] [ADA p: 0.241]\n",
      "[Epoch 501/2000] [Batch 0/11] [D loss: 0.4223] [G loss: 0.6826] [ADA p: 0.245]\n",
      "[Epoch 502/2000] [Batch 0/11] [D loss: 0.5440] [G loss: 0.7198] [ADA p: 0.258]\n",
      "[Epoch 503/2000] [Batch 0/11] [D loss: 0.4911] [G loss: 0.7828] [ADA p: 0.270]\n",
      "[Epoch 504/2000] [Batch 0/11] [D loss: 0.4986] [G loss: 0.7723] [ADA p: 0.283]\n",
      "[Epoch 505/2000] [Batch 0/11] [D loss: 0.5685] [G loss: 1.0449] [ADA p: 0.295]\n",
      "[Epoch 506/2000] [Batch 0/11] [D loss: 0.6157] [G loss: 1.2176] [ADA p: 0.307]\n",
      "[Epoch 507/2000] [Batch 0/11] [D loss: 0.5148] [G loss: 0.8608] [ADA p: 0.319]\n",
      "[Epoch 508/2000] [Batch 0/11] [D loss: 0.5515] [G loss: 1.0099] [ADA p: 0.331]\n",
      "[Epoch 509/2000] [Batch 0/11] [D loss: 0.3651] [G loss: 0.8134] [ADA p: 0.343]\n",
      "[Epoch 510/2000] [Batch 0/11] [D loss: 0.5045] [G loss: 1.1955] [ADA p: 0.355]\n",
      "[Epoch 511/2000] [Batch 0/11] [D loss: 0.7245] [G loss: 1.3360] [ADA p: 0.367]\n",
      "[Epoch 512/2000] [Batch 0/11] [D loss: 0.8273] [G loss: 0.8935] [ADA p: 0.367]\n",
      "[Epoch 513/2000] [Batch 0/11] [D loss: 0.2727] [G loss: 0.8680] [ADA p: 0.367]\n",
      "[Epoch 514/2000] [Batch 0/11] [D loss: 0.2604] [G loss: 1.2459] [ADA p: 0.355]\n",
      "[Epoch 515/2000] [Batch 0/11] [D loss: 1.0029] [G loss: 1.0109] [ADA p: 0.355]\n",
      "[Epoch 516/2000] [Batch 0/11] [D loss: 0.8925] [G loss: 0.9126] [ADA p: 0.351]\n",
      "[Epoch 517/2000] [Batch 0/11] [D loss: 1.1408] [G loss: 0.6161] [ADA p: 0.339]\n",
      "[Epoch 518/2000] [Batch 0/11] [D loss: 0.4829] [G loss: 0.8223] [ADA p: 0.328]\n",
      "[Epoch 519/2000] [Batch 0/11] [D loss: 0.5045] [G loss: 0.9437] [ADA p: 0.316]\n",
      "[Epoch 520/2000] [Batch 0/11] [D loss: 0.4050] [G loss: 1.1300] [ADA p: 0.312]\n",
      "[Epoch 521/2000] [Batch 0/11] [D loss: 0.4939] [G loss: 0.9037] [ADA p: 0.312]\n",
      "[Epoch 522/2000] [Batch 0/11] [D loss: 0.4862] [G loss: 1.3073] [ADA p: 0.308]\n",
      "[Epoch 523/2000] [Batch 0/11] [D loss: 0.5771] [G loss: 1.6871] [ADA p: 0.308]\n",
      "[Epoch 524/2000] [Batch 0/11] [D loss: 0.3818] [G loss: 0.9212] [ADA p: 0.316]\n",
      "[Epoch 525/2000] [Batch 0/11] [D loss: 0.5032] [G loss: 1.1201] [ADA p: 0.328]\n",
      "[Epoch 526/2000] [Batch 0/11] [D loss: 0.5549] [G loss: 1.3052] [ADA p: 0.340]\n",
      "[Epoch 527/2000] [Batch 0/11] [D loss: 0.4823] [G loss: 0.7200] [ADA p: 0.343]\n",
      "[Epoch 528/2000] [Batch 0/11] [D loss: 0.3997] [G loss: 0.7941] [ADA p: 0.343]\n",
      "[Epoch 529/2000] [Batch 0/11] [D loss: 0.6068] [G loss: 0.7007] [ADA p: 0.355]\n",
      "[Epoch 530/2000] [Batch 0/11] [D loss: 0.1936] [G loss: 0.6831] [ADA p: 0.367]\n",
      "[Epoch 531/2000] [Batch 0/11] [D loss: 0.3297] [G loss: 1.0061] [ADA p: 0.378]\n",
      "[Epoch 532/2000] [Batch 0/11] [D loss: 0.7199] [G loss: 1.4955] [ADA p: 0.390]\n",
      "[Epoch 533/2000] [Batch 0/11] [D loss: 0.8893] [G loss: 1.0574] [ADA p: 0.401]\n",
      "[Epoch 534/2000] [Batch 0/11] [D loss: 0.2556] [G loss: 0.8319] [ADA p: 0.412]\n",
      "[Epoch 535/2000] [Batch 0/11] [D loss: 0.7785] [G loss: 0.5924] [ADA p: 0.420]\n",
      "[Epoch 536/2000] [Batch 0/11] [D loss: 0.6211] [G loss: 1.1749] [ADA p: 0.420]\n",
      "[Epoch 537/2000] [Batch 0/11] [D loss: 0.2955] [G loss: 1.4823] [ADA p: 0.427]\n",
      "[Epoch 538/2000] [Batch 0/11] [D loss: 0.4972] [G loss: 0.6033] [ADA p: 0.438]\n",
      "[Epoch 539/2000] [Batch 0/11] [D loss: 0.7636] [G loss: 0.9814] [ADA p: 0.445]\n",
      "[Epoch 540/2000] [Batch 0/11] [D loss: 0.5081] [G loss: 0.7248] [ADA p: 0.445]\n",
      "[Epoch 541/2000] [Batch 0/11] [D loss: 0.5827] [G loss: 1.1863] [ADA p: 0.445]\n",
      "[Epoch 542/2000] [Batch 0/11] [D loss: 0.7928] [G loss: 0.6721] [ADA p: 0.445]\n",
      "[Epoch 543/2000] [Batch 0/11] [D loss: 0.7000] [G loss: 1.3505] [ADA p: 0.445]\n",
      "[Epoch 544/2000] [Batch 0/11] [D loss: 0.3640] [G loss: 0.9783] [ADA p: 0.438]\n",
      "[Epoch 545/2000] [Batch 0/11] [D loss: 0.6032] [G loss: 1.4334] [ADA p: 0.438]\n",
      "[Epoch 546/2000] [Batch 0/11] [D loss: 0.5518] [G loss: 0.9801] [ADA p: 0.438]\n",
      "[Epoch 547/2000] [Batch 0/11] [D loss: 0.3686] [G loss: 0.6373] [ADA p: 0.438]\n",
      "[Epoch 548/2000] [Batch 0/11] [D loss: 0.5820] [G loss: 0.8573] [ADA p: 0.445]\n",
      "[Epoch 549/2000] [Batch 0/11] [D loss: 0.4865] [G loss: 1.1862] [ADA p: 0.449]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.41it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 549 | FID: 349.24 | ADA p: 0.449\n",
      "[Epoch 550/2000] [Batch 0/11] [D loss: 0.7528] [G loss: 1.4295] [ADA p: 0.445]\n",
      "[Epoch 551/2000] [Batch 0/11] [D loss: 0.8338] [G loss: 1.3313] [ADA p: 0.434]\n",
      "[Epoch 552/2000] [Batch 0/11] [D loss: 0.4469] [G loss: 0.9361] [ADA p: 0.424]\n",
      "[Epoch 553/2000] [Batch 0/11] [D loss: 0.3493] [G loss: 0.9769] [ADA p: 0.420]\n",
      "[Epoch 554/2000] [Batch 0/11] [D loss: 0.6319] [G loss: 0.9136] [ADA p: 0.420]\n",
      "[Epoch 555/2000] [Batch 0/11] [D loss: 0.9257] [G loss: 0.9625] [ADA p: 0.416]\n",
      "[Epoch 556/2000] [Batch 0/11] [D loss: 0.5316] [G loss: 0.6531] [ADA p: 0.405]\n",
      "[Epoch 557/2000] [Batch 0/11] [D loss: 0.7545] [G loss: 1.5002] [ADA p: 0.394]\n",
      "[Epoch 558/2000] [Batch 0/11] [D loss: 0.4287] [G loss: 0.9994] [ADA p: 0.382]\n",
      "[Epoch 559/2000] [Batch 0/11] [D loss: 0.3909] [G loss: 1.0933] [ADA p: 0.371]\n",
      "[Epoch 560/2000] [Batch 0/11] [D loss: 0.5711] [G loss: 1.0167] [ADA p: 0.359]\n",
      "[Epoch 561/2000] [Batch 0/11] [D loss: 0.4545] [G loss: 1.6383] [ADA p: 0.348]\n",
      "[Epoch 562/2000] [Batch 0/11] [D loss: 0.3789] [G loss: 1.1893] [ADA p: 0.340]\n",
      "[Epoch 563/2000] [Batch 0/11] [D loss: 0.9420] [G loss: 1.1646] [ADA p: 0.340]\n",
      "[Epoch 564/2000] [Batch 0/11] [D loss: 0.5657] [G loss: 0.8355] [ADA p: 0.340]\n",
      "[Epoch 565/2000] [Batch 0/11] [D loss: 0.4922] [G loss: 1.0332] [ADA p: 0.328]\n",
      "[Epoch 566/2000] [Batch 0/11] [D loss: 0.7145] [G loss: 0.6513] [ADA p: 0.316]\n",
      "[Epoch 567/2000] [Batch 0/11] [D loss: 0.4810] [G loss: 0.8739] [ADA p: 0.308]\n",
      "[Epoch 568/2000] [Batch 0/11] [D loss: 0.4865] [G loss: 1.0374] [ADA p: 0.308]\n",
      "[Epoch 569/2000] [Batch 0/11] [D loss: 0.6238] [G loss: 0.8582] [ADA p: 0.320]\n",
      "[Epoch 570/2000] [Batch 0/11] [D loss: 0.3806] [G loss: 0.7369] [ADA p: 0.332]\n",
      "[Epoch 571/2000] [Batch 0/11] [D loss: 0.5833] [G loss: 0.5333] [ADA p: 0.344]\n",
      "[Epoch 572/2000] [Batch 0/11] [D loss: 0.4542] [G loss: 1.3034] [ADA p: 0.356]\n",
      "[Epoch 573/2000] [Batch 0/11] [D loss: 0.4969] [G loss: 1.1118] [ADA p: 0.360]\n",
      "[Epoch 574/2000] [Batch 0/11] [D loss: 0.3401] [G loss: 1.3413] [ADA p: 0.360]\n",
      "[Epoch 575/2000] [Batch 0/11] [D loss: 0.6380] [G loss: 0.9929] [ADA p: 0.371]\n",
      "[Epoch 576/2000] [Batch 0/11] [D loss: 0.7539] [G loss: 0.7250] [ADA p: 0.383]\n",
      "[Epoch 577/2000] [Batch 0/11] [D loss: 0.5431] [G loss: 0.4832] [ADA p: 0.386]\n",
      "[Epoch 578/2000] [Batch 0/11] [D loss: 0.5479] [G loss: 1.0996] [ADA p: 0.383]\n",
      "[Epoch 579/2000] [Batch 0/11] [D loss: 0.6046] [G loss: 0.9407] [ADA p: 0.371]\n",
      "[Epoch 580/2000] [Batch 0/11] [D loss: 0.4547] [G loss: 1.0174] [ADA p: 0.371]\n",
      "[Epoch 581/2000] [Batch 0/11] [D loss: 0.3622] [G loss: 0.7078] [ADA p: 0.379]\n",
      "[Epoch 582/2000] [Batch 0/11] [D loss: 0.5144] [G loss: 1.1780] [ADA p: 0.390]\n",
      "[Epoch 583/2000] [Batch 0/11] [D loss: 0.5988] [G loss: 1.3750] [ADA p: 0.402]\n",
      "[Epoch 584/2000] [Batch 0/11] [D loss: 0.3263] [G loss: 1.5138] [ADA p: 0.413]\n",
      "[Epoch 585/2000] [Batch 0/11] [D loss: 0.2672] [G loss: 1.2022] [ADA p: 0.424]\n",
      "[Epoch 586/2000] [Batch 0/11] [D loss: 0.5180] [G loss: 0.6883] [ADA p: 0.435]\n",
      "[Epoch 587/2000] [Batch 0/11] [D loss: 0.3814] [G loss: 1.1082] [ADA p: 0.446]\n",
      "[Epoch 588/2000] [Batch 0/11] [D loss: 0.4049] [G loss: 1.0845] [ADA p: 0.457]\n",
      "[Epoch 589/2000] [Batch 0/11] [D loss: 0.3398] [G loss: 0.7243] [ADA p: 0.467]\n",
      "[Epoch 590/2000] [Batch 0/11] [D loss: 0.2948] [G loss: 0.9360] [ADA p: 0.478]\n",
      "[Epoch 591/2000] [Batch 0/11] [D loss: 0.3991] [G loss: 0.5688] [ADA p: 0.488]\n",
      "[Epoch 592/2000] [Batch 0/11] [D loss: 0.4807] [G loss: 0.6015] [ADA p: 0.499]\n",
      "[Epoch 593/2000] [Batch 0/11] [D loss: 0.5751] [G loss: 0.5844] [ADA p: 0.509]\n",
      "[Epoch 594/2000] [Batch 0/11] [D loss: 0.5183] [G loss: 1.3801] [ADA p: 0.512]\n",
      "[Epoch 595/2000] [Batch 0/11] [D loss: 0.8830] [G loss: 1.1451] [ADA p: 0.512]\n",
      "[Epoch 596/2000] [Batch 0/11] [D loss: 0.4995] [G loss: 0.8487] [ADA p: 0.509]\n",
      "[Epoch 597/2000] [Batch 0/11] [D loss: 0.4558] [G loss: 1.0791] [ADA p: 0.499]\n",
      "[Epoch 598/2000] [Batch 0/11] [D loss: 0.5999] [G loss: 1.2914] [ADA p: 0.488]\n",
      "[Epoch 599/2000] [Batch 0/11] [D loss: 0.3008] [G loss: 0.7364] [ADA p: 0.478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.42it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 599 | FID: 339.10 | ADA p: 0.474\n",
      "[Epoch 600/2000] [Batch 0/11] [D loss: 0.7054] [G loss: 0.4799] [ADA p: 0.474]\n",
      "[Epoch 601/2000] [Batch 0/11] [D loss: 0.4287] [G loss: 1.3542] [ADA p: 0.474]\n",
      "[Epoch 602/2000] [Batch 0/11] [D loss: 0.1987] [G loss: 0.6562] [ADA p: 0.474]\n",
      "[Epoch 603/2000] [Batch 0/11] [D loss: 0.5687] [G loss: 0.5462] [ADA p: 0.478]\n",
      "[Epoch 604/2000] [Batch 0/11] [D loss: 0.4425] [G loss: 1.3452] [ADA p: 0.478]\n",
      "[Epoch 605/2000] [Batch 0/11] [D loss: 0.4701] [G loss: 1.1031] [ADA p: 0.478]\n",
      "[Epoch 606/2000] [Batch 0/11] [D loss: 0.3891] [G loss: 1.4234] [ADA p: 0.478]\n",
      "[Epoch 607/2000] [Batch 0/11] [D loss: 0.1742] [G loss: 0.5955] [ADA p: 0.478]\n",
      "[Epoch 608/2000] [Batch 0/11] [D loss: 0.8985] [G loss: 1.2341] [ADA p: 0.481]\n",
      "[Epoch 609/2000] [Batch 0/11] [D loss: 0.1817] [G loss: 0.7515] [ADA p: 0.492]\n",
      "[Epoch 610/2000] [Batch 0/11] [D loss: 0.5685] [G loss: 1.1017] [ADA p: 0.495]\n",
      "[Epoch 611/2000] [Batch 0/11] [D loss: 0.4983] [G loss: 0.7909] [ADA p: 0.495]\n",
      "[Epoch 612/2000] [Batch 0/11] [D loss: 0.5167] [G loss: 1.2109] [ADA p: 0.499]\n",
      "[Epoch 613/2000] [Batch 0/11] [D loss: 0.5975] [G loss: 1.0791] [ADA p: 0.509]\n",
      "[Epoch 614/2000] [Batch 0/11] [D loss: 0.4157] [G loss: 1.5898] [ADA p: 0.519]\n",
      "[Epoch 615/2000] [Batch 0/11] [D loss: 0.4762] [G loss: 0.9667] [ADA p: 0.529]\n",
      "[Epoch 616/2000] [Batch 0/11] [D loss: 0.6731] [G loss: 1.2437] [ADA p: 0.539]\n",
      "[Epoch 617/2000] [Batch 0/11] [D loss: 0.4669] [G loss: 0.5346] [ADA p: 0.543]\n",
      "[Epoch 618/2000] [Batch 0/11] [D loss: 0.6808] [G loss: 0.7773] [ADA p: 0.543]\n",
      "[Epoch 619/2000] [Batch 0/11] [D loss: 0.4730] [G loss: 0.6010] [ADA p: 0.546]\n",
      "[Epoch 620/2000] [Batch 0/11] [D loss: 0.4302] [G loss: 1.0815] [ADA p: 0.556]\n",
      "[Epoch 621/2000] [Batch 0/11] [D loss: 0.3120] [G loss: 0.4886] [ADA p: 0.566]\n",
      "[Epoch 622/2000] [Batch 0/11] [D loss: 0.3727] [G loss: 0.6558] [ADA p: 0.575]\n",
      "[Epoch 623/2000] [Batch 0/11] [D loss: 0.6361] [G loss: 1.1788] [ADA p: 0.585]\n",
      "[Epoch 624/2000] [Batch 0/11] [D loss: 0.7723] [G loss: 0.7310] [ADA p: 0.585]\n",
      "[Epoch 625/2000] [Batch 0/11] [D loss: 0.4647] [G loss: 0.9093] [ADA p: 0.579]\n",
      "[Epoch 626/2000] [Batch 0/11] [D loss: 0.5194] [G loss: 0.6657] [ADA p: 0.579]\n",
      "[Epoch 627/2000] [Batch 0/11] [D loss: 0.5553] [G loss: 0.9464] [ADA p: 0.579]\n",
      "[Epoch 628/2000] [Batch 0/11] [D loss: 0.7601] [G loss: 0.8342] [ADA p: 0.579]\n",
      "[Epoch 629/2000] [Batch 0/11] [D loss: 0.6843] [G loss: 1.3442] [ADA p: 0.588]\n",
      "[Epoch 630/2000] [Batch 0/11] [D loss: 0.4434] [G loss: 1.2137] [ADA p: 0.598]\n",
      "[Epoch 631/2000] [Batch 0/11] [D loss: 0.5027] [G loss: 1.0968] [ADA p: 0.607]\n",
      "[Epoch 632/2000] [Batch 0/11] [D loss: 0.5418] [G loss: 0.5193] [ADA p: 0.616]\n",
      "[Epoch 633/2000] [Batch 0/11] [D loss: 0.6009] [G loss: 1.0421] [ADA p: 0.625]\n",
      "[Epoch 634/2000] [Batch 0/11] [D loss: 0.7272] [G loss: 1.2452] [ADA p: 0.635]\n",
      "[Epoch 635/2000] [Batch 0/11] [D loss: 0.6394] [G loss: 1.1977] [ADA p: 0.644]\n",
      "[Epoch 636/2000] [Batch 0/11] [D loss: 0.7136] [G loss: 0.5905] [ADA p: 0.652]\n",
      "[Epoch 637/2000] [Batch 0/11] [D loss: 0.6089] [G loss: 0.7412] [ADA p: 0.661]\n",
      "[Epoch 638/2000] [Batch 0/11] [D loss: 0.5571] [G loss: 1.2089] [ADA p: 0.670]\n",
      "[Epoch 639/2000] [Batch 0/11] [D loss: 0.5551] [G loss: 0.9201] [ADA p: 0.679]\n",
      "[Epoch 640/2000] [Batch 0/11] [D loss: 0.3540] [G loss: 0.8679] [ADA p: 0.687]\n",
      "[Epoch 641/2000] [Batch 0/11] [D loss: 0.6908] [G loss: 1.2326] [ADA p: 0.696]\n",
      "[Epoch 642/2000] [Batch 0/11] [D loss: 0.3751] [G loss: 1.1737] [ADA p: 0.704]\n",
      "[Epoch 643/2000] [Batch 0/11] [D loss: 0.5190] [G loss: 0.7873] [ADA p: 0.713]\n",
      "[Epoch 644/2000] [Batch 0/11] [D loss: 0.4322] [G loss: 0.9859] [ADA p: 0.721]\n",
      "[Epoch 645/2000] [Batch 0/11] [D loss: 0.8575] [G loss: 1.7549] [ADA p: 0.729]\n",
      "[Epoch 646/2000] [Batch 0/11] [D loss: 0.6680] [G loss: 0.9697] [ADA p: 0.737]\n",
      "[Epoch 647/2000] [Batch 0/11] [D loss: 0.6620] [G loss: 1.1406] [ADA p: 0.745]\n",
      "[Epoch 648/2000] [Batch 0/11] [D loss: 0.7848] [G loss: 0.8503] [ADA p: 0.745]\n",
      "[Epoch 649/2000] [Batch 0/11] [D loss: 0.3544] [G loss: 0.7888] [ADA p: 0.740]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.40it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649 | FID: 358.17 | ADA p: 0.735\n",
      "[Epoch 650/2000] [Batch 0/11] [D loss: 0.6832] [G loss: 0.9286] [ADA p: 0.732]\n",
      "[Epoch 651/2000] [Batch 0/11] [D loss: 0.3905] [G loss: 1.2196] [ADA p: 0.727]\n",
      "[Epoch 652/2000] [Batch 0/11] [D loss: 0.8790] [G loss: 0.9295] [ADA p: 0.727]\n",
      "[Epoch 653/2000] [Batch 0/11] [D loss: 0.5936] [G loss: 0.7291] [ADA p: 0.735]\n",
      "[Epoch 654/2000] [Batch 0/11] [D loss: 0.4005] [G loss: 1.0460] [ADA p: 0.743]\n",
      "[Epoch 655/2000] [Batch 0/11] [D loss: 0.6034] [G loss: 1.2829] [ADA p: 0.751]\n",
      "[Epoch 656/2000] [Batch 0/11] [D loss: 0.5452] [G loss: 0.5495] [ADA p: 0.759]\n",
      "[Epoch 657/2000] [Batch 0/11] [D loss: 0.8884] [G loss: 0.6070] [ADA p: 0.764]\n",
      "[Epoch 658/2000] [Batch 0/11] [D loss: 0.3089] [G loss: 1.0399] [ADA p: 0.764]\n",
      "[Epoch 659/2000] [Batch 0/11] [D loss: 0.2838] [G loss: 0.6968] [ADA p: 0.764]\n",
      "[Epoch 660/2000] [Batch 0/11] [D loss: 0.4109] [G loss: 1.2220] [ADA p: 0.764]\n",
      "[Epoch 661/2000] [Batch 0/11] [D loss: 0.4902] [G loss: 0.5239] [ADA p: 0.764]\n",
      "[Epoch 662/2000] [Batch 0/11] [D loss: 0.6233] [G loss: 0.9639] [ADA p: 0.764]\n",
      "[Epoch 663/2000] [Batch 0/11] [D loss: 0.6124] [G loss: 0.9327] [ADA p: 0.761]\n",
      "[Epoch 664/2000] [Batch 0/11] [D loss: 0.3665] [G loss: 0.9734] [ADA p: 0.761]\n",
      "[Epoch 665/2000] [Batch 0/11] [D loss: 0.7679] [G loss: 0.6102] [ADA p: 0.761]\n",
      "[Epoch 666/2000] [Batch 0/11] [D loss: 0.6911] [G loss: 1.6442] [ADA p: 0.759]\n",
      "[Epoch 667/2000] [Batch 0/11] [D loss: 0.4185] [G loss: 1.4378] [ADA p: 0.751]\n",
      "[Epoch 668/2000] [Batch 0/11] [D loss: 0.4948] [G loss: 0.9801] [ADA p: 0.743]\n",
      "[Epoch 669/2000] [Batch 0/11] [D loss: 0.4603] [G loss: 0.7059] [ADA p: 0.735]\n",
      "[Epoch 670/2000] [Batch 0/11] [D loss: 0.3322] [G loss: 0.7841] [ADA p: 0.727]\n",
      "[Epoch 671/2000] [Batch 0/11] [D loss: 0.4509] [G loss: 1.0294] [ADA p: 0.721]\n",
      "[Epoch 672/2000] [Batch 0/11] [D loss: 0.4780] [G loss: 1.3385] [ADA p: 0.721]\n",
      "[Epoch 673/2000] [Batch 0/11] [D loss: 0.4611] [G loss: 0.6514] [ADA p: 0.729]\n",
      "[Epoch 674/2000] [Batch 0/11] [D loss: 0.4287] [G loss: 1.4705] [ADA p: 0.738]\n",
      "[Epoch 675/2000] [Batch 0/11] [D loss: 0.5001] [G loss: 0.4705] [ADA p: 0.746]\n",
      "[Epoch 676/2000] [Batch 0/11] [D loss: 0.4638] [G loss: 0.8644] [ADA p: 0.754]\n",
      "[Epoch 677/2000] [Batch 0/11] [D loss: 0.2607] [G loss: 1.3334] [ADA p: 0.756]\n",
      "[Epoch 678/2000] [Batch 0/11] [D loss: 0.2988] [G loss: 0.8713] [ADA p: 0.756]\n",
      "[Epoch 679/2000] [Batch 0/11] [D loss: 0.3022] [G loss: 0.9864] [ADA p: 0.759]\n",
      "[Epoch 680/2000] [Batch 0/11] [D loss: 0.4353] [G loss: 0.5290] [ADA p: 0.767]\n",
      "[Epoch 681/2000] [Batch 0/11] [D loss: 0.3574] [G loss: 0.6505] [ADA p: 0.775]\n",
      "[Epoch 682/2000] [Batch 0/11] [D loss: 0.3745] [G loss: 1.0937] [ADA p: 0.782]\n",
      "[Epoch 683/2000] [Batch 0/11] [D loss: 0.3475] [G loss: 1.1676] [ADA p: 0.790]\n",
      "[Epoch 684/2000] [Batch 0/11] [D loss: 0.4160] [G loss: 1.1985] [ADA p: 0.797]\n",
      "[Epoch 685/2000] [Batch 0/11] [D loss: 0.3753] [G loss: 0.9502] [ADA p: 0.800]\n",
      "[Epoch 686/2000] [Batch 0/11] [D loss: 0.5161] [G loss: 1.0143] [ADA p: 0.800]\n",
      "[Epoch 687/2000] [Batch 0/11] [D loss: 0.3548] [G loss: 1.5512] [ADA p: 0.800]\n",
      "[Epoch 688/2000] [Batch 0/11] [D loss: 0.4347] [G loss: 1.0220] [ADA p: 0.800]\n",
      "[Epoch 689/2000] [Batch 0/11] [D loss: 0.4255] [G loss: 0.7251] [ADA p: 0.800]\n",
      "[Epoch 690/2000] [Batch 0/11] [D loss: 0.4252] [G loss: 1.4124] [ADA p: 0.800]\n",
      "[Epoch 691/2000] [Batch 0/11] [D loss: 0.3907] [G loss: 1.5140] [ADA p: 0.800]\n",
      "[Epoch 692/2000] [Batch 0/11] [D loss: 0.9322] [G loss: 0.7892] [ADA p: 0.800]\n",
      "[Epoch 693/2000] [Batch 0/11] [D loss: 0.5084] [G loss: 0.8095] [ADA p: 0.800]\n",
      "[Epoch 694/2000] [Batch 0/11] [D loss: 0.3994] [G loss: 0.3994] [ADA p: 0.800]\n",
      "[Epoch 695/2000] [Batch 0/11] [D loss: 0.4484] [G loss: 0.7100] [ADA p: 0.800]\n",
      "[Epoch 696/2000] [Batch 0/11] [D loss: 0.4468] [G loss: 0.8567] [ADA p: 0.800]\n",
      "[Epoch 697/2000] [Batch 0/11] [D loss: 0.1694] [G loss: 0.8034] [ADA p: 0.800]\n",
      "[Epoch 698/2000] [Batch 0/11] [D loss: 0.2781] [G loss: 1.1591] [ADA p: 0.800]\n",
      "[Epoch 699/2000] [Batch 0/11] [D loss: 0.3904] [G loss: 0.4075] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.35it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 699 | FID: 360.88 | ADA p: 0.800\n",
      "[Epoch 700/2000] [Batch 0/11] [D loss: 0.1678] [G loss: 1.5838] [ADA p: 0.800]\n",
      "[Epoch 701/2000] [Batch 0/11] [D loss: 0.7798] [G loss: 1.0194] [ADA p: 0.800]\n",
      "[Epoch 702/2000] [Batch 0/11] [D loss: 0.4945] [G loss: 0.4558] [ADA p: 0.800]\n",
      "[Epoch 703/2000] [Batch 0/11] [D loss: 0.3384] [G loss: 0.7746] [ADA p: 0.800]\n",
      "[Epoch 704/2000] [Batch 0/11] [D loss: 0.2792] [G loss: 0.3843] [ADA p: 0.800]\n",
      "[Epoch 705/2000] [Batch 0/11] [D loss: 0.6443] [G loss: 1.0354] [ADA p: 0.800]\n",
      "[Epoch 706/2000] [Batch 0/11] [D loss: 0.4247] [G loss: 0.8560] [ADA p: 0.800]\n",
      "[Epoch 707/2000] [Batch 0/11] [D loss: 0.3988] [G loss: 1.2158] [ADA p: 0.800]\n",
      "[Epoch 708/2000] [Batch 0/11] [D loss: 0.4696] [G loss: 0.5576] [ADA p: 0.800]\n",
      "[Epoch 709/2000] [Batch 0/11] [D loss: 0.2855] [G loss: 0.9935] [ADA p: 0.800]\n",
      "[Epoch 710/2000] [Batch 0/11] [D loss: 0.2854] [G loss: 0.8207] [ADA p: 0.795]\n",
      "[Epoch 711/2000] [Batch 0/11] [D loss: 0.2884] [G loss: 0.6852] [ADA p: 0.795]\n",
      "[Epoch 712/2000] [Batch 0/11] [D loss: 0.3015] [G loss: 1.0177] [ADA p: 0.798]\n",
      "[Epoch 713/2000] [Batch 0/11] [D loss: 0.5299] [G loss: 0.5202] [ADA p: 0.798]\n",
      "[Epoch 714/2000] [Batch 0/11] [D loss: 0.4673] [G loss: 0.9469] [ADA p: 0.800]\n",
      "[Epoch 715/2000] [Batch 0/11] [D loss: 0.5665] [G loss: 0.7132] [ADA p: 0.800]\n",
      "[Epoch 716/2000] [Batch 0/11] [D loss: 0.5246] [G loss: 0.4049] [ADA p: 0.800]\n",
      "[Epoch 717/2000] [Batch 0/11] [D loss: 0.5407] [G loss: 0.3745] [ADA p: 0.800]\n",
      "[Epoch 718/2000] [Batch 0/11] [D loss: 0.5215] [G loss: 0.4290] [ADA p: 0.800]\n",
      "[Epoch 719/2000] [Batch 0/11] [D loss: 0.4272] [G loss: 0.7522] [ADA p: 0.800]\n",
      "[Epoch 720/2000] [Batch 0/11] [D loss: 0.2886] [G loss: 0.8301] [ADA p: 0.800]\n",
      "[Epoch 721/2000] [Batch 0/11] [D loss: 0.1908] [G loss: 0.5589] [ADA p: 0.800]\n",
      "[Epoch 722/2000] [Batch 0/11] [D loss: 0.6768] [G loss: 1.4460] [ADA p: 0.800]\n",
      "[Epoch 723/2000] [Batch 0/11] [D loss: 0.2609] [G loss: 0.4592] [ADA p: 0.800]\n",
      "[Epoch 724/2000] [Batch 0/11] [D loss: 0.3737] [G loss: 1.0892] [ADA p: 0.800]\n",
      "[Epoch 725/2000] [Batch 0/11] [D loss: 0.6682] [G loss: 0.8020] [ADA p: 0.800]\n",
      "[Epoch 726/2000] [Batch 0/11] [D loss: 0.7241] [G loss: 1.3002] [ADA p: 0.800]\n",
      "[Epoch 727/2000] [Batch 0/11] [D loss: 0.7490] [G loss: 0.6622] [ADA p: 0.800]\n",
      "[Epoch 728/2000] [Batch 0/11] [D loss: 0.7669] [G loss: 0.8740] [ADA p: 0.800]\n",
      "[Epoch 729/2000] [Batch 0/11] [D loss: 0.7777] [G loss: 1.3464] [ADA p: 0.800]\n",
      "[Epoch 730/2000] [Batch 0/11] [D loss: 0.3060] [G loss: 0.9271] [ADA p: 0.800]\n",
      "[Epoch 731/2000] [Batch 0/11] [D loss: 0.3642] [G loss: 0.7528] [ADA p: 0.800]\n",
      "[Epoch 732/2000] [Batch 0/11] [D loss: 0.8068] [G loss: 0.9797] [ADA p: 0.800]\n",
      "[Epoch 733/2000] [Batch 0/11] [D loss: 0.4080] [G loss: 1.2201] [ADA p: 0.800]\n",
      "[Epoch 734/2000] [Batch 0/11] [D loss: 0.4353] [G loss: 1.7171] [ADA p: 0.800]\n",
      "[Epoch 735/2000] [Batch 0/11] [D loss: 0.2595] [G loss: 1.2931] [ADA p: 0.800]\n",
      "[Epoch 736/2000] [Batch 0/11] [D loss: 0.5219] [G loss: 1.1786] [ADA p: 0.800]\n",
      "[Epoch 737/2000] [Batch 0/11] [D loss: 0.2006] [G loss: 0.8107] [ADA p: 0.800]\n",
      "[Epoch 738/2000] [Batch 0/11] [D loss: 0.5544] [G loss: 1.0567] [ADA p: 0.800]\n",
      "[Epoch 739/2000] [Batch 0/11] [D loss: 0.3940] [G loss: 1.1164] [ADA p: 0.800]\n",
      "[Epoch 740/2000] [Batch 0/11] [D loss: 0.8293] [G loss: 1.3214] [ADA p: 0.800]\n",
      "[Epoch 741/2000] [Batch 0/11] [D loss: 0.4141] [G loss: 0.9705] [ADA p: 0.800]\n",
      "[Epoch 742/2000] [Batch 0/11] [D loss: 0.3291] [G loss: 0.9150] [ADA p: 0.800]\n",
      "[Epoch 743/2000] [Batch 0/11] [D loss: 0.5167] [G loss: 0.3319] [ADA p: 0.800]\n",
      "[Epoch 744/2000] [Batch 0/11] [D loss: 0.3235] [G loss: 0.7016] [ADA p: 0.800]\n",
      "[Epoch 745/2000] [Batch 0/11] [D loss: 0.3007] [G loss: 0.4469] [ADA p: 0.800]\n",
      "[Epoch 746/2000] [Batch 0/11] [D loss: 0.4474] [G loss: 0.8453] [ADA p: 0.800]\n",
      "[Epoch 747/2000] [Batch 0/11] [D loss: 0.2657] [G loss: 1.7501] [ADA p: 0.800]\n",
      "[Epoch 748/2000] [Batch 0/11] [D loss: 0.7935] [G loss: 1.1193] [ADA p: 0.800]\n",
      "[Epoch 749/2000] [Batch 0/11] [D loss: 0.3919] [G loss: 0.7819] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.46it/s]\n",
      "100%|██████████| 63/63 [00:12<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 749 | FID: 375.62 | ADA p: 0.800\n",
      "[Epoch 750/2000] [Batch 0/11] [D loss: 0.1455] [G loss: 0.9550] [ADA p: 0.800]\n",
      "[Epoch 751/2000] [Batch 0/11] [D loss: 0.6864] [G loss: 0.8719] [ADA p: 0.800]\n",
      "[Epoch 752/2000] [Batch 0/11] [D loss: 0.9815] [G loss: 0.5014] [ADA p: 0.800]\n",
      "[Epoch 753/2000] [Batch 0/11] [D loss: 0.4188] [G loss: 1.6915] [ADA p: 0.800]\n",
      "[Epoch 754/2000] [Batch 0/11] [D loss: 0.6030] [G loss: 0.5664] [ADA p: 0.800]\n",
      "[Epoch 755/2000] [Batch 0/11] [D loss: 0.7827] [G loss: 0.8287] [ADA p: 0.800]\n",
      "[Epoch 756/2000] [Batch 0/11] [D loss: 0.7463] [G loss: 1.0828] [ADA p: 0.800]\n",
      "[Epoch 757/2000] [Batch 0/11] [D loss: 0.7707] [G loss: 0.6504] [ADA p: 0.800]\n",
      "[Epoch 758/2000] [Batch 0/11] [D loss: 0.2172] [G loss: 1.1877] [ADA p: 0.800]\n",
      "[Epoch 759/2000] [Batch 0/11] [D loss: 0.5125] [G loss: 0.7402] [ADA p: 0.800]\n",
      "[Epoch 760/2000] [Batch 0/11] [D loss: 0.6291] [G loss: 1.1945] [ADA p: 0.800]\n",
      "[Epoch 761/2000] [Batch 0/11] [D loss: 0.4121] [G loss: 1.1351] [ADA p: 0.800]\n",
      "[Epoch 762/2000] [Batch 0/11] [D loss: 0.5602] [G loss: 1.0070] [ADA p: 0.800]\n",
      "[Epoch 763/2000] [Batch 0/11] [D loss: 0.6047] [G loss: 0.7449] [ADA p: 0.800]\n",
      "[Epoch 764/2000] [Batch 0/11] [D loss: 0.3893] [G loss: 0.9340] [ADA p: 0.800]\n",
      "[Epoch 765/2000] [Batch 0/11] [D loss: 0.2600] [G loss: 0.5960] [ADA p: 0.800]\n",
      "[Epoch 766/2000] [Batch 0/11] [D loss: 0.6472] [G loss: 1.0027] [ADA p: 0.800]\n",
      "[Epoch 767/2000] [Batch 0/11] [D loss: 0.1460] [G loss: 0.6567] [ADA p: 0.800]\n",
      "[Epoch 768/2000] [Batch 0/11] [D loss: 0.3818] [G loss: 0.6775] [ADA p: 0.800]\n",
      "[Epoch 769/2000] [Batch 0/11] [D loss: 0.8582] [G loss: 0.7282] [ADA p: 0.800]\n",
      "[Epoch 770/2000] [Batch 0/11] [D loss: 0.3032] [G loss: 0.7041] [ADA p: 0.800]\n",
      "[Epoch 771/2000] [Batch 0/11] [D loss: 0.4519] [G loss: 0.9548] [ADA p: 0.800]\n",
      "[Epoch 772/2000] [Batch 0/11] [D loss: 0.2943] [G loss: 0.9240] [ADA p: 0.800]\n",
      "[Epoch 773/2000] [Batch 0/11] [D loss: 0.4566] [G loss: 1.5163] [ADA p: 0.800]\n",
      "[Epoch 774/2000] [Batch 0/11] [D loss: 0.3996] [G loss: 1.9731] [ADA p: 0.800]\n",
      "[Epoch 775/2000] [Batch 0/11] [D loss: 0.4146] [G loss: 0.9156] [ADA p: 0.800]\n",
      "[Epoch 776/2000] [Batch 0/11] [D loss: 0.4848] [G loss: 1.2043] [ADA p: 0.800]\n",
      "[Epoch 777/2000] [Batch 0/11] [D loss: 0.3492] [G loss: 1.3717] [ADA p: 0.800]\n",
      "[Epoch 778/2000] [Batch 0/11] [D loss: 0.6001] [G loss: 0.6860] [ADA p: 0.800]\n",
      "[Epoch 779/2000] [Batch 0/11] [D loss: 0.6860] [G loss: 0.7669] [ADA p: 0.800]\n",
      "[Epoch 780/2000] [Batch 0/11] [D loss: 0.3591] [G loss: 1.0918] [ADA p: 0.800]\n",
      "[Epoch 781/2000] [Batch 0/11] [D loss: 0.8661] [G loss: 0.5812] [ADA p: 0.800]\n",
      "[Epoch 782/2000] [Batch 0/11] [D loss: 0.4188] [G loss: 1.2270] [ADA p: 0.800]\n",
      "[Epoch 783/2000] [Batch 0/11] [D loss: 0.5643] [G loss: 1.4518] [ADA p: 0.800]\n",
      "[Epoch 784/2000] [Batch 0/11] [D loss: 0.5003] [G loss: 0.7134] [ADA p: 0.800]\n",
      "[Epoch 785/2000] [Batch 0/11] [D loss: 0.5407] [G loss: 0.9566] [ADA p: 0.800]\n",
      "[Epoch 786/2000] [Batch 0/11] [D loss: 0.2266] [G loss: 1.0457] [ADA p: 0.800]\n",
      "[Epoch 787/2000] [Batch 0/11] [D loss: 0.5810] [G loss: 0.8316] [ADA p: 0.800]\n",
      "[Epoch 788/2000] [Batch 0/11] [D loss: 0.5927] [G loss: 1.4404] [ADA p: 0.800]\n",
      "[Epoch 789/2000] [Batch 0/11] [D loss: 0.6416] [G loss: 0.7304] [ADA p: 0.800]\n",
      "[Epoch 790/2000] [Batch 0/11] [D loss: 0.5283] [G loss: 0.3895] [ADA p: 0.800]\n",
      "[Epoch 791/2000] [Batch 0/11] [D loss: 0.5742] [G loss: 0.7984] [ADA p: 0.800]\n",
      "[Epoch 792/2000] [Batch 0/11] [D loss: 0.5694] [G loss: 0.5991] [ADA p: 0.800]\n",
      "[Epoch 793/2000] [Batch 0/11] [D loss: 0.6129] [G loss: 1.2556] [ADA p: 0.800]\n",
      "[Epoch 794/2000] [Batch 0/11] [D loss: 0.2656] [G loss: 0.6469] [ADA p: 0.800]\n",
      "[Epoch 795/2000] [Batch 0/11] [D loss: 0.2242] [G loss: 1.1634] [ADA p: 0.800]\n",
      "[Epoch 796/2000] [Batch 0/11] [D loss: 0.4575] [G loss: 0.3289] [ADA p: 0.800]\n",
      "[Epoch 797/2000] [Batch 0/11] [D loss: 0.3657] [G loss: 0.9682] [ADA p: 0.800]\n",
      "[Epoch 798/2000] [Batch 0/11] [D loss: 0.6497] [G loss: 1.2923] [ADA p: 0.800]\n",
      "[Epoch 799/2000] [Batch 0/11] [D loss: 0.6480] [G loss: 1.0682] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.50it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 799 | FID: 359.50 | ADA p: 0.800\n",
      "[Epoch 800/2000] [Batch 0/11] [D loss: 0.7129] [G loss: 0.5360] [ADA p: 0.800]\n",
      "[Epoch 801/2000] [Batch 0/11] [D loss: 0.4657] [G loss: 1.4133] [ADA p: 0.800]\n",
      "[Epoch 802/2000] [Batch 0/11] [D loss: 0.3172] [G loss: 0.9997] [ADA p: 0.800]\n",
      "[Epoch 803/2000] [Batch 0/11] [D loss: 0.5500] [G loss: 0.2671] [ADA p: 0.800]\n",
      "[Epoch 804/2000] [Batch 0/11] [D loss: 0.4582] [G loss: 0.3231] [ADA p: 0.800]\n",
      "[Epoch 805/2000] [Batch 0/11] [D loss: 0.6500] [G loss: 0.4452] [ADA p: 0.800]\n",
      "[Epoch 806/2000] [Batch 0/11] [D loss: 0.5414] [G loss: 0.5640] [ADA p: 0.800]\n",
      "[Epoch 807/2000] [Batch 0/11] [D loss: 0.3332] [G loss: 1.2912] [ADA p: 0.800]\n",
      "[Epoch 808/2000] [Batch 0/11] [D loss: 0.6083] [G loss: 0.4280] [ADA p: 0.800]\n",
      "[Epoch 809/2000] [Batch 0/11] [D loss: 0.2515] [G loss: 0.9367] [ADA p: 0.800]\n",
      "[Epoch 810/2000] [Batch 0/11] [D loss: 0.3305] [G loss: 1.2038] [ADA p: 0.800]\n",
      "[Epoch 811/2000] [Batch 0/11] [D loss: 0.2448] [G loss: 0.8424] [ADA p: 0.800]\n",
      "[Epoch 812/2000] [Batch 0/11] [D loss: 0.2246] [G loss: 0.6570] [ADA p: 0.800]\n",
      "[Epoch 813/2000] [Batch 0/11] [D loss: 0.2966] [G loss: 0.8952] [ADA p: 0.800]\n",
      "[Epoch 814/2000] [Batch 0/11] [D loss: 0.5226] [G loss: 1.3659] [ADA p: 0.800]\n",
      "[Epoch 815/2000] [Batch 0/11] [D loss: 0.3620] [G loss: 1.2690] [ADA p: 0.800]\n",
      "[Epoch 816/2000] [Batch 0/11] [D loss: 0.6296] [G loss: 0.6539] [ADA p: 0.800]\n",
      "[Epoch 817/2000] [Batch 0/11] [D loss: 0.5891] [G loss: 0.4886] [ADA p: 0.800]\n",
      "[Epoch 818/2000] [Batch 0/11] [D loss: 0.2070] [G loss: 1.1065] [ADA p: 0.800]\n",
      "[Epoch 819/2000] [Batch 0/11] [D loss: 0.2709] [G loss: 0.5368] [ADA p: 0.800]\n",
      "[Epoch 820/2000] [Batch 0/11] [D loss: 0.2801] [G loss: 0.7744] [ADA p: 0.800]\n",
      "[Epoch 821/2000] [Batch 0/11] [D loss: 0.3140] [G loss: 0.6453] [ADA p: 0.800]\n",
      "[Epoch 822/2000] [Batch 0/11] [D loss: 0.3379] [G loss: 0.6208] [ADA p: 0.800]\n",
      "[Epoch 823/2000] [Batch 0/11] [D loss: 0.1921] [G loss: 2.5756] [ADA p: 0.800]\n",
      "[Epoch 824/2000] [Batch 0/11] [D loss: 0.7128] [G loss: 1.3050] [ADA p: 0.800]\n",
      "[Epoch 825/2000] [Batch 0/11] [D loss: 0.3472] [G loss: 1.3526] [ADA p: 0.800]\n",
      "[Epoch 826/2000] [Batch 0/11] [D loss: 0.3477] [G loss: 0.5658] [ADA p: 0.800]\n",
      "[Epoch 827/2000] [Batch 0/11] [D loss: 0.5708] [G loss: 0.5080] [ADA p: 0.800]\n",
      "[Epoch 828/2000] [Batch 0/11] [D loss: 0.3531] [G loss: 0.9354] [ADA p: 0.800]\n",
      "[Epoch 829/2000] [Batch 0/11] [D loss: 0.3259] [G loss: 1.3166] [ADA p: 0.800]\n",
      "[Epoch 830/2000] [Batch 0/11] [D loss: 0.2835] [G loss: 0.7477] [ADA p: 0.800]\n",
      "[Epoch 831/2000] [Batch 0/11] [D loss: 0.8290] [G loss: 0.5469] [ADA p: 0.800]\n",
      "[Epoch 832/2000] [Batch 0/11] [D loss: 0.5126] [G loss: 0.5660] [ADA p: 0.800]\n",
      "[Epoch 833/2000] [Batch 0/11] [D loss: 0.5817] [G loss: 1.1955] [ADA p: 0.800]\n",
      "[Epoch 834/2000] [Batch 0/11] [D loss: 0.3637] [G loss: 1.2380] [ADA p: 0.800]\n",
      "[Epoch 835/2000] [Batch 0/11] [D loss: 0.5601] [G loss: 1.0369] [ADA p: 0.800]\n",
      "[Epoch 836/2000] [Batch 0/11] [D loss: 0.3193] [G loss: 0.9150] [ADA p: 0.800]\n",
      "[Epoch 837/2000] [Batch 0/11] [D loss: 0.3730] [G loss: 1.1872] [ADA p: 0.800]\n",
      "[Epoch 838/2000] [Batch 0/11] [D loss: 0.3326] [G loss: 1.1950] [ADA p: 0.800]\n",
      "[Epoch 839/2000] [Batch 0/11] [D loss: 0.5288] [G loss: 1.6136] [ADA p: 0.800]\n",
      "[Epoch 840/2000] [Batch 0/11] [D loss: 0.3391] [G loss: 0.9065] [ADA p: 0.800]\n",
      "[Epoch 841/2000] [Batch 0/11] [D loss: 0.6661] [G loss: 0.6725] [ADA p: 0.800]\n",
      "[Epoch 842/2000] [Batch 0/11] [D loss: 0.4841] [G loss: 0.6013] [ADA p: 0.800]\n",
      "[Epoch 843/2000] [Batch 0/11] [D loss: 0.4679] [G loss: 1.2434] [ADA p: 0.800]\n",
      "[Epoch 844/2000] [Batch 0/11] [D loss: 0.4707] [G loss: 1.3542] [ADA p: 0.800]\n",
      "[Epoch 845/2000] [Batch 0/11] [D loss: 0.8151] [G loss: 1.3549] [ADA p: 0.800]\n",
      "[Epoch 846/2000] [Batch 0/11] [D loss: 0.4150] [G loss: 1.3479] [ADA p: 0.800]\n",
      "[Epoch 847/2000] [Batch 0/11] [D loss: 0.3619] [G loss: 1.1844] [ADA p: 0.800]\n",
      "[Epoch 848/2000] [Batch 0/11] [D loss: 0.5258] [G loss: 1.0123] [ADA p: 0.800]\n",
      "[Epoch 849/2000] [Batch 0/11] [D loss: 0.4334] [G loss: 1.1063] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.65it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 849 | FID: 369.77 | ADA p: 0.800\n",
      "[Epoch 850/2000] [Batch 0/11] [D loss: 0.4524] [G loss: 1.2440] [ADA p: 0.800]\n",
      "[Epoch 851/2000] [Batch 0/11] [D loss: 0.4624] [G loss: 0.7414] [ADA p: 0.800]\n",
      "[Epoch 852/2000] [Batch 0/11] [D loss: 0.2265] [G loss: 1.2851] [ADA p: 0.800]\n",
      "[Epoch 853/2000] [Batch 0/11] [D loss: 0.5484] [G loss: 0.4267] [ADA p: 0.800]\n",
      "[Epoch 854/2000] [Batch 0/11] [D loss: 0.2566] [G loss: 1.0506] [ADA p: 0.800]\n",
      "[Epoch 855/2000] [Batch 0/11] [D loss: 0.6452] [G loss: 0.8576] [ADA p: 0.800]\n",
      "[Epoch 856/2000] [Batch 0/11] [D loss: 0.3408] [G loss: 1.5533] [ADA p: 0.800]\n",
      "[Epoch 857/2000] [Batch 0/11] [D loss: 0.2014] [G loss: 0.5489] [ADA p: 0.800]\n",
      "[Epoch 858/2000] [Batch 0/11] [D loss: 0.5895] [G loss: 2.2714] [ADA p: 0.800]\n",
      "[Epoch 859/2000] [Batch 0/11] [D loss: 0.3806] [G loss: 1.1403] [ADA p: 0.800]\n",
      "[Epoch 860/2000] [Batch 0/11] [D loss: 0.5309] [G loss: 0.6000] [ADA p: 0.800]\n",
      "[Epoch 861/2000] [Batch 0/11] [D loss: 0.3054] [G loss: 0.6331] [ADA p: 0.800]\n",
      "[Epoch 862/2000] [Batch 0/11] [D loss: 0.3686] [G loss: 0.6643] [ADA p: 0.800]\n",
      "[Epoch 863/2000] [Batch 0/11] [D loss: 0.2297] [G loss: 0.9147] [ADA p: 0.800]\n",
      "[Epoch 864/2000] [Batch 0/11] [D loss: 0.4589] [G loss: 1.9715] [ADA p: 0.800]\n",
      "[Epoch 865/2000] [Batch 0/11] [D loss: 0.5484] [G loss: 0.6976] [ADA p: 0.800]\n",
      "[Epoch 866/2000] [Batch 0/11] [D loss: 0.4266] [G loss: 0.4578] [ADA p: 0.800]\n",
      "[Epoch 867/2000] [Batch 0/11] [D loss: 0.2457] [G loss: 1.6932] [ADA p: 0.800]\n",
      "[Epoch 868/2000] [Batch 0/11] [D loss: 0.1615] [G loss: 0.5816] [ADA p: 0.800]\n",
      "[Epoch 869/2000] [Batch 0/11] [D loss: 0.4846] [G loss: 0.5797] [ADA p: 0.800]\n",
      "[Epoch 870/2000] [Batch 0/11] [D loss: 0.3826] [G loss: 0.5057] [ADA p: 0.800]\n",
      "[Epoch 871/2000] [Batch 0/11] [D loss: 0.2328] [G loss: 1.0416] [ADA p: 0.800]\n",
      "[Epoch 872/2000] [Batch 0/11] [D loss: 0.2894] [G loss: 1.7485] [ADA p: 0.800]\n",
      "[Epoch 873/2000] [Batch 0/11] [D loss: 0.5905] [G loss: 1.5024] [ADA p: 0.800]\n",
      "[Epoch 874/2000] [Batch 0/11] [D loss: 0.5336] [G loss: 0.5933] [ADA p: 0.800]\n",
      "[Epoch 875/2000] [Batch 0/11] [D loss: 0.5320] [G loss: 1.3552] [ADA p: 0.800]\n",
      "[Epoch 876/2000] [Batch 0/11] [D loss: 0.7104] [G loss: 1.8735] [ADA p: 0.800]\n",
      "[Epoch 877/2000] [Batch 0/11] [D loss: 0.3578] [G loss: 0.7726] [ADA p: 0.800]\n",
      "[Epoch 878/2000] [Batch 0/11] [D loss: 0.1921] [G loss: 0.9833] [ADA p: 0.800]\n",
      "[Epoch 879/2000] [Batch 0/11] [D loss: 0.3970] [G loss: 0.9147] [ADA p: 0.800]\n",
      "[Epoch 880/2000] [Batch 0/11] [D loss: 0.3685] [G loss: 1.1979] [ADA p: 0.800]\n",
      "[Epoch 881/2000] [Batch 0/11] [D loss: 0.5691] [G loss: 1.1683] [ADA p: 0.800]\n",
      "[Epoch 882/2000] [Batch 0/11] [D loss: 0.5998] [G loss: 0.8692] [ADA p: 0.800]\n",
      "[Epoch 883/2000] [Batch 0/11] [D loss: 0.4659] [G loss: 0.7629] [ADA p: 0.800]\n",
      "[Epoch 884/2000] [Batch 0/11] [D loss: 0.6304] [G loss: 1.0878] [ADA p: 0.800]\n",
      "[Epoch 885/2000] [Batch 0/11] [D loss: 0.5206] [G loss: 0.8489] [ADA p: 0.800]\n",
      "[Epoch 886/2000] [Batch 0/11] [D loss: 0.3996] [G loss: 1.3543] [ADA p: 0.800]\n",
      "[Epoch 887/2000] [Batch 0/11] [D loss: 0.7319] [G loss: 0.9356] [ADA p: 0.800]\n",
      "[Epoch 888/2000] [Batch 0/11] [D loss: 0.6010] [G loss: 2.0095] [ADA p: 0.800]\n",
      "[Epoch 889/2000] [Batch 0/11] [D loss: 0.7217] [G loss: 1.6555] [ADA p: 0.800]\n",
      "[Epoch 890/2000] [Batch 0/11] [D loss: 0.2293] [G loss: 0.4252] [ADA p: 0.800]\n",
      "[Epoch 891/2000] [Batch 0/11] [D loss: 0.2002] [G loss: 1.3554] [ADA p: 0.800]\n",
      "[Epoch 892/2000] [Batch 0/11] [D loss: 0.4123] [G loss: 1.9057] [ADA p: 0.800]\n",
      "[Epoch 893/2000] [Batch 0/11] [D loss: 0.7703] [G loss: 1.0159] [ADA p: 0.800]\n",
      "[Epoch 894/2000] [Batch 0/11] [D loss: 0.6487] [G loss: 1.0511] [ADA p: 0.800]\n",
      "[Epoch 895/2000] [Batch 0/11] [D loss: 0.3600] [G loss: 0.7795] [ADA p: 0.800]\n",
      "[Epoch 896/2000] [Batch 0/11] [D loss: 0.2475] [G loss: 0.8205] [ADA p: 0.800]\n",
      "[Epoch 897/2000] [Batch 0/11] [D loss: 1.0567] [G loss: 0.9015] [ADA p: 0.800]\n",
      "[Epoch 898/2000] [Batch 0/11] [D loss: 0.6554] [G loss: 0.9886] [ADA p: 0.800]\n",
      "[Epoch 899/2000] [Batch 0/11] [D loss: 0.4019] [G loss: 1.5601] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.61it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 899 | FID: 364.57 | ADA p: 0.800\n",
      "[Epoch 900/2000] [Batch 0/11] [D loss: 0.3394] [G loss: 0.9447] [ADA p: 0.800]\n",
      "[Epoch 901/2000] [Batch 0/11] [D loss: 0.2378] [G loss: 1.0593] [ADA p: 0.800]\n",
      "[Epoch 902/2000] [Batch 0/11] [D loss: 0.2805] [G loss: 0.9164] [ADA p: 0.800]\n",
      "[Epoch 903/2000] [Batch 0/11] [D loss: 0.5578] [G loss: 0.8953] [ADA p: 0.800]\n",
      "[Epoch 904/2000] [Batch 0/11] [D loss: 0.1750] [G loss: 1.3625] [ADA p: 0.800]\n",
      "[Epoch 905/2000] [Batch 0/11] [D loss: 0.2945] [G loss: 1.3608] [ADA p: 0.800]\n",
      "[Epoch 906/2000] [Batch 0/11] [D loss: 0.3282] [G loss: 1.2861] [ADA p: 0.800]\n",
      "[Epoch 907/2000] [Batch 0/11] [D loss: 0.4858] [G loss: 1.6094] [ADA p: 0.800]\n",
      "[Epoch 908/2000] [Batch 0/11] [D loss: 0.5994] [G loss: 1.8774] [ADA p: 0.800]\n",
      "[Epoch 909/2000] [Batch 0/11] [D loss: 0.3113] [G loss: 0.6406] [ADA p: 0.800]\n",
      "[Epoch 910/2000] [Batch 0/11] [D loss: 0.7178] [G loss: 1.4513] [ADA p: 0.800]\n",
      "[Epoch 911/2000] [Batch 0/11] [D loss: 0.2789] [G loss: 1.3032] [ADA p: 0.800]\n",
      "[Epoch 912/2000] [Batch 0/11] [D loss: 0.6123] [G loss: 0.8319] [ADA p: 0.800]\n",
      "[Epoch 913/2000] [Batch 0/11] [D loss: 0.2670] [G loss: 1.3840] [ADA p: 0.800]\n",
      "[Epoch 914/2000] [Batch 0/11] [D loss: 0.6438] [G loss: 1.1963] [ADA p: 0.800]\n",
      "[Epoch 915/2000] [Batch 0/11] [D loss: 0.4873] [G loss: 0.9028] [ADA p: 0.800]\n",
      "[Epoch 916/2000] [Batch 0/11] [D loss: 0.3369] [G loss: 1.0278] [ADA p: 0.800]\n",
      "[Epoch 917/2000] [Batch 0/11] [D loss: 0.3388] [G loss: 0.7750] [ADA p: 0.800]\n",
      "[Epoch 918/2000] [Batch 0/11] [D loss: 0.2125] [G loss: 0.9306] [ADA p: 0.800]\n",
      "[Epoch 919/2000] [Batch 0/11] [D loss: 0.3885] [G loss: 0.5704] [ADA p: 0.800]\n",
      "[Epoch 920/2000] [Batch 0/11] [D loss: 0.2558] [G loss: 0.4302] [ADA p: 0.800]\n",
      "[Epoch 921/2000] [Batch 0/11] [D loss: 0.4303] [G loss: 1.0311] [ADA p: 0.800]\n",
      "[Epoch 922/2000] [Batch 0/11] [D loss: 0.7558] [G loss: 0.8005] [ADA p: 0.800]\n",
      "[Epoch 923/2000] [Batch 0/11] [D loss: 0.4101] [G loss: 0.7010] [ADA p: 0.800]\n",
      "[Epoch 924/2000] [Batch 0/11] [D loss: 0.5055] [G loss: 0.7026] [ADA p: 0.800]\n",
      "[Epoch 925/2000] [Batch 0/11] [D loss: 0.3523] [G loss: 0.9794] [ADA p: 0.800]\n",
      "[Epoch 926/2000] [Batch 0/11] [D loss: 0.4263] [G loss: 0.4104] [ADA p: 0.800]\n",
      "[Epoch 927/2000] [Batch 0/11] [D loss: 0.3110] [G loss: 0.7466] [ADA p: 0.800]\n",
      "[Epoch 928/2000] [Batch 0/11] [D loss: 0.4058] [G loss: 0.6557] [ADA p: 0.800]\n",
      "[Epoch 929/2000] [Batch 0/11] [D loss: 0.3361] [G loss: 1.1464] [ADA p: 0.800]\n",
      "[Epoch 930/2000] [Batch 0/11] [D loss: 0.3562] [G loss: 1.5748] [ADA p: 0.800]\n",
      "[Epoch 931/2000] [Batch 0/11] [D loss: 0.3561] [G loss: 0.5511] [ADA p: 0.800]\n",
      "[Epoch 932/2000] [Batch 0/11] [D loss: 0.1898] [G loss: 1.3223] [ADA p: 0.800]\n",
      "[Epoch 933/2000] [Batch 0/11] [D loss: 0.3473] [G loss: 1.9908] [ADA p: 0.800]\n",
      "[Epoch 934/2000] [Batch 0/11] [D loss: 0.3443] [G loss: 1.2141] [ADA p: 0.800]\n",
      "[Epoch 935/2000] [Batch 0/11] [D loss: 0.6261] [G loss: 1.5258] [ADA p: 0.800]\n",
      "[Epoch 936/2000] [Batch 0/11] [D loss: 0.4577] [G loss: 0.9384] [ADA p: 0.800]\n",
      "[Epoch 937/2000] [Batch 0/11] [D loss: 0.5080] [G loss: 0.9470] [ADA p: 0.800]\n",
      "[Epoch 938/2000] [Batch 0/11] [D loss: 0.4562] [G loss: 0.3973] [ADA p: 0.800]\n",
      "[Epoch 939/2000] [Batch 0/11] [D loss: 0.3738] [G loss: 0.8629] [ADA p: 0.800]\n",
      "[Epoch 940/2000] [Batch 0/11] [D loss: 0.3136] [G loss: 0.8821] [ADA p: 0.800]\n",
      "[Epoch 941/2000] [Batch 0/11] [D loss: 0.2583] [G loss: 2.0871] [ADA p: 0.800]\n",
      "[Epoch 942/2000] [Batch 0/11] [D loss: 0.5334] [G loss: 1.0069] [ADA p: 0.800]\n",
      "[Epoch 943/2000] [Batch 0/11] [D loss: 0.4892] [G loss: 0.7574] [ADA p: 0.800]\n",
      "[Epoch 944/2000] [Batch 0/11] [D loss: 0.4431] [G loss: 0.7853] [ADA p: 0.800]\n",
      "[Epoch 945/2000] [Batch 0/11] [D loss: 0.4729] [G loss: 0.3803] [ADA p: 0.800]\n",
      "[Epoch 946/2000] [Batch 0/11] [D loss: 0.5676] [G loss: 1.7384] [ADA p: 0.800]\n",
      "[Epoch 947/2000] [Batch 0/11] [D loss: 0.2558] [G loss: 1.0383] [ADA p: 0.800]\n",
      "[Epoch 948/2000] [Batch 0/11] [D loss: 0.6309] [G loss: 0.6838] [ADA p: 0.800]\n",
      "[Epoch 949/2000] [Batch 0/11] [D loss: 0.3095] [G loss: 0.9308] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.59it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 949 | FID: 351.73 | ADA p: 0.800\n",
      "[Epoch 950/2000] [Batch 0/11] [D loss: 0.2304] [G loss: 1.0509] [ADA p: 0.800]\n",
      "[Epoch 951/2000] [Batch 0/11] [D loss: 0.4171] [G loss: 2.2497] [ADA p: 0.800]\n",
      "[Epoch 952/2000] [Batch 0/11] [D loss: 0.2649] [G loss: 0.8274] [ADA p: 0.800]\n",
      "[Epoch 953/2000] [Batch 0/11] [D loss: 0.2716] [G loss: 0.9062] [ADA p: 0.800]\n",
      "[Epoch 954/2000] [Batch 0/11] [D loss: 0.4822] [G loss: 0.8118] [ADA p: 0.800]\n",
      "[Epoch 955/2000] [Batch 0/11] [D loss: 0.3289] [G loss: 0.5592] [ADA p: 0.800]\n",
      "[Epoch 956/2000] [Batch 0/11] [D loss: 0.9733] [G loss: 1.3911] [ADA p: 0.800]\n",
      "[Epoch 957/2000] [Batch 0/11] [D loss: 0.6651] [G loss: 1.7974] [ADA p: 0.800]\n",
      "[Epoch 958/2000] [Batch 0/11] [D loss: 0.2269] [G loss: 1.0706] [ADA p: 0.800]\n",
      "[Epoch 959/2000] [Batch 0/11] [D loss: 0.5804] [G loss: 0.9208] [ADA p: 0.800]\n",
      "[Epoch 960/2000] [Batch 0/11] [D loss: 0.2038] [G loss: 1.8864] [ADA p: 0.800]\n",
      "[Epoch 961/2000] [Batch 0/11] [D loss: 0.5285] [G loss: 0.9928] [ADA p: 0.800]\n",
      "[Epoch 962/2000] [Batch 0/11] [D loss: 0.9058] [G loss: 0.9277] [ADA p: 0.800]\n",
      "[Epoch 963/2000] [Batch 0/11] [D loss: 0.2264] [G loss: 0.7451] [ADA p: 0.800]\n",
      "[Epoch 964/2000] [Batch 0/11] [D loss: 0.3594] [G loss: 0.9674] [ADA p: 0.800]\n",
      "[Epoch 965/2000] [Batch 0/11] [D loss: 0.2049] [G loss: 0.6814] [ADA p: 0.800]\n",
      "[Epoch 966/2000] [Batch 0/11] [D loss: 0.1607] [G loss: 1.4250] [ADA p: 0.800]\n",
      "[Epoch 967/2000] [Batch 0/11] [D loss: 0.4812] [G loss: 0.7118] [ADA p: 0.800]\n",
      "[Epoch 968/2000] [Batch 0/11] [D loss: 0.8194] [G loss: 1.6327] [ADA p: 0.800]\n",
      "[Epoch 969/2000] [Batch 0/11] [D loss: 0.4278] [G loss: 0.8198] [ADA p: 0.800]\n",
      "[Epoch 970/2000] [Batch 0/11] [D loss: 0.4945] [G loss: 0.4286] [ADA p: 0.800]\n",
      "[Epoch 971/2000] [Batch 0/11] [D loss: 0.4453] [G loss: 0.9444] [ADA p: 0.800]\n",
      "[Epoch 972/2000] [Batch 0/11] [D loss: 0.4276] [G loss: 0.7432] [ADA p: 0.800]\n",
      "[Epoch 973/2000] [Batch 0/11] [D loss: 0.6230] [G loss: 0.8543] [ADA p: 0.800]\n",
      "[Epoch 974/2000] [Batch 0/11] [D loss: 0.5675] [G loss: 1.9590] [ADA p: 0.800]\n",
      "[Epoch 975/2000] [Batch 0/11] [D loss: 0.2991] [G loss: 0.4591] [ADA p: 0.800]\n",
      "[Epoch 976/2000] [Batch 0/11] [D loss: 0.3061] [G loss: 0.6581] [ADA p: 0.800]\n",
      "[Epoch 977/2000] [Batch 0/11] [D loss: 0.3661] [G loss: 1.4501] [ADA p: 0.800]\n",
      "[Epoch 978/2000] [Batch 0/11] [D loss: 0.1686] [G loss: 0.6989] [ADA p: 0.800]\n",
      "[Epoch 979/2000] [Batch 0/11] [D loss: 0.6538] [G loss: 0.9841] [ADA p: 0.800]\n",
      "[Epoch 980/2000] [Batch 0/11] [D loss: 0.3333] [G loss: 1.7965] [ADA p: 0.800]\n",
      "[Epoch 981/2000] [Batch 0/11] [D loss: 0.7413] [G loss: 0.8308] [ADA p: 0.800]\n",
      "[Epoch 982/2000] [Batch 0/11] [D loss: 0.8894] [G loss: 0.8332] [ADA p: 0.800]\n",
      "[Epoch 983/2000] [Batch 0/11] [D loss: 0.2384] [G loss: 1.0684] [ADA p: 0.800]\n",
      "[Epoch 984/2000] [Batch 0/11] [D loss: 0.5291] [G loss: 0.9475] [ADA p: 0.800]\n",
      "[Epoch 985/2000] [Batch 0/11] [D loss: 0.3347] [G loss: 0.7354] [ADA p: 0.800]\n",
      "[Epoch 986/2000] [Batch 0/11] [D loss: 0.6200] [G loss: 0.5260] [ADA p: 0.800]\n",
      "[Epoch 987/2000] [Batch 0/11] [D loss: 0.3911] [G loss: 1.1249] [ADA p: 0.800]\n",
      "[Epoch 988/2000] [Batch 0/11] [D loss: 0.2609] [G loss: 0.8848] [ADA p: 0.800]\n",
      "[Epoch 989/2000] [Batch 0/11] [D loss: 0.3877] [G loss: 0.6402] [ADA p: 0.800]\n",
      "[Epoch 990/2000] [Batch 0/11] [D loss: 0.2607] [G loss: 0.7558] [ADA p: 0.800]\n",
      "[Epoch 991/2000] [Batch 0/11] [D loss: 0.5433] [G loss: 1.2011] [ADA p: 0.800]\n",
      "[Epoch 992/2000] [Batch 0/11] [D loss: 0.3488] [G loss: 1.2864] [ADA p: 0.800]\n",
      "[Epoch 993/2000] [Batch 0/11] [D loss: 0.3113] [G loss: 1.1079] [ADA p: 0.800]\n",
      "[Epoch 994/2000] [Batch 0/11] [D loss: 0.3483] [G loss: 1.2806] [ADA p: 0.800]\n",
      "[Epoch 995/2000] [Batch 0/11] [D loss: 0.3039] [G loss: 0.7089] [ADA p: 0.800]\n",
      "[Epoch 996/2000] [Batch 0/11] [D loss: 0.5808] [G loss: 0.7489] [ADA p: 0.800]\n",
      "[Epoch 997/2000] [Batch 0/11] [D loss: 0.2836] [G loss: 1.2443] [ADA p: 0.800]\n",
      "[Epoch 998/2000] [Batch 0/11] [D loss: 0.2862] [G loss: 0.8777] [ADA p: 0.800]\n",
      "[Epoch 999/2000] [Batch 0/11] [D loss: 0.4127] [G loss: 1.6668] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.54it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 999 | FID: 366.39 | ADA p: 0.800\n",
      "[Epoch 1000/2000] [Batch 0/11] [D loss: 0.3410] [G loss: 0.5988] [ADA p: 0.800]\n",
      "[Epoch 1001/2000] [Batch 0/11] [D loss: 0.4479] [G loss: 0.6713] [ADA p: 0.800]\n",
      "[Epoch 1002/2000] [Batch 0/11] [D loss: 0.4314] [G loss: 1.2762] [ADA p: 0.800]\n",
      "[Epoch 1003/2000] [Batch 0/11] [D loss: 0.2135] [G loss: 0.8825] [ADA p: 0.800]\n",
      "[Epoch 1004/2000] [Batch 0/11] [D loss: 0.6631] [G loss: 1.6537] [ADA p: 0.800]\n",
      "[Epoch 1005/2000] [Batch 0/11] [D loss: 0.1555] [G loss: 1.9169] [ADA p: 0.800]\n",
      "[Epoch 1006/2000] [Batch 0/11] [D loss: 0.4460] [G loss: 1.7415] [ADA p: 0.800]\n",
      "[Epoch 1007/2000] [Batch 0/11] [D loss: 0.2806] [G loss: 0.9706] [ADA p: 0.800]\n",
      "[Epoch 1008/2000] [Batch 0/11] [D loss: 0.5603] [G loss: 1.2091] [ADA p: 0.800]\n",
      "[Epoch 1009/2000] [Batch 0/11] [D loss: 0.1689] [G loss: 0.5595] [ADA p: 0.800]\n",
      "[Epoch 1010/2000] [Batch 0/11] [D loss: 0.6170] [G loss: 1.8661] [ADA p: 0.800]\n",
      "[Epoch 1011/2000] [Batch 0/11] [D loss: 0.1429] [G loss: 1.3085] [ADA p: 0.800]\n",
      "[Epoch 1012/2000] [Batch 0/11] [D loss: 0.3628] [G loss: 0.5958] [ADA p: 0.800]\n",
      "[Epoch 1013/2000] [Batch 0/11] [D loss: 1.0346] [G loss: 0.8086] [ADA p: 0.800]\n",
      "[Epoch 1014/2000] [Batch 0/11] [D loss: 0.4025] [G loss: 1.7751] [ADA p: 0.800]\n",
      "[Epoch 1015/2000] [Batch 0/11] [D loss: 0.3290] [G loss: 0.2932] [ADA p: 0.800]\n",
      "[Epoch 1016/2000] [Batch 0/11] [D loss: 0.3850] [G loss: 0.9010] [ADA p: 0.800]\n",
      "[Epoch 1017/2000] [Batch 0/11] [D loss: 0.2328] [G loss: 1.4396] [ADA p: 0.800]\n",
      "[Epoch 1018/2000] [Batch 0/11] [D loss: 0.4305] [G loss: 0.6270] [ADA p: 0.800]\n",
      "[Epoch 1019/2000] [Batch 0/11] [D loss: 0.2570] [G loss: 0.7237] [ADA p: 0.800]\n",
      "[Epoch 1020/2000] [Batch 0/11] [D loss: 0.3593] [G loss: 0.9901] [ADA p: 0.800]\n",
      "[Epoch 1021/2000] [Batch 0/11] [D loss: 0.6706] [G loss: 0.9259] [ADA p: 0.800]\n",
      "[Epoch 1022/2000] [Batch 0/11] [D loss: 0.1924] [G loss: 0.5313] [ADA p: 0.800]\n",
      "[Epoch 1023/2000] [Batch 0/11] [D loss: 0.5331] [G loss: 0.5364] [ADA p: 0.800]\n",
      "[Epoch 1024/2000] [Batch 0/11] [D loss: 0.4322] [G loss: 0.7078] [ADA p: 0.800]\n",
      "[Epoch 1025/2000] [Batch 0/11] [D loss: 0.3505] [G loss: 2.2588] [ADA p: 0.800]\n",
      "[Epoch 1026/2000] [Batch 0/11] [D loss: 0.4560] [G loss: 1.2112] [ADA p: 0.800]\n",
      "[Epoch 1027/2000] [Batch 0/11] [D loss: 0.1521] [G loss: 0.8518] [ADA p: 0.800]\n",
      "[Epoch 1028/2000] [Batch 0/11] [D loss: 0.2224] [G loss: 0.5967] [ADA p: 0.800]\n",
      "[Epoch 1029/2000] [Batch 0/11] [D loss: 0.5386] [G loss: 0.8541] [ADA p: 0.800]\n",
      "[Epoch 1030/2000] [Batch 0/11] [D loss: 0.2332] [G loss: 0.6827] [ADA p: 0.800]\n",
      "[Epoch 1031/2000] [Batch 0/11] [D loss: 0.3453] [G loss: 1.4304] [ADA p: 0.800]\n",
      "[Epoch 1032/2000] [Batch 0/11] [D loss: 0.2725] [G loss: 1.6234] [ADA p: 0.800]\n",
      "[Epoch 1033/2000] [Batch 0/11] [D loss: 0.2755] [G loss: 1.8548] [ADA p: 0.800]\n",
      "[Epoch 1034/2000] [Batch 0/11] [D loss: 0.3487] [G loss: 0.7541] [ADA p: 0.800]\n",
      "[Epoch 1035/2000] [Batch 0/11] [D loss: 0.1970] [G loss: 0.6150] [ADA p: 0.800]\n",
      "[Epoch 1036/2000] [Batch 0/11] [D loss: 0.4026] [G loss: 0.9100] [ADA p: 0.800]\n",
      "[Epoch 1037/2000] [Batch 0/11] [D loss: 0.4325] [G loss: 1.3422] [ADA p: 0.800]\n",
      "[Epoch 1038/2000] [Batch 0/11] [D loss: 0.3007] [G loss: 1.2893] [ADA p: 0.800]\n",
      "[Epoch 1039/2000] [Batch 0/11] [D loss: 0.3284] [G loss: 1.4218] [ADA p: 0.800]\n",
      "[Epoch 1040/2000] [Batch 0/11] [D loss: 0.2869] [G loss: 0.5754] [ADA p: 0.800]\n",
      "[Epoch 1041/2000] [Batch 0/11] [D loss: 0.4081] [G loss: 0.8271] [ADA p: 0.800]\n",
      "[Epoch 1042/2000] [Batch 0/11] [D loss: 0.3331] [G loss: 2.0433] [ADA p: 0.800]\n",
      "[Epoch 1043/2000] [Batch 0/11] [D loss: 0.3292] [G loss: 1.0457] [ADA p: 0.800]\n",
      "[Epoch 1044/2000] [Batch 0/11] [D loss: 0.3018] [G loss: 0.9386] [ADA p: 0.800]\n",
      "[Epoch 1045/2000] [Batch 0/11] [D loss: 0.2608] [G loss: 1.3683] [ADA p: 0.800]\n",
      "[Epoch 1046/2000] [Batch 0/11] [D loss: 0.2923] [G loss: 0.5344] [ADA p: 0.800]\n",
      "[Epoch 1047/2000] [Batch 0/11] [D loss: 0.4513] [G loss: 0.1817] [ADA p: 0.800]\n",
      "[Epoch 1048/2000] [Batch 0/11] [D loss: 0.4715] [G loss: 0.7282] [ADA p: 0.800]\n",
      "[Epoch 1049/2000] [Batch 0/11] [D loss: 0.4243] [G loss: 0.4994] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.63it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1049 | FID: 349.97 | ADA p: 0.800\n",
      "[Epoch 1050/2000] [Batch 0/11] [D loss: 0.5170] [G loss: 1.1208] [ADA p: 0.800]\n",
      "[Epoch 1051/2000] [Batch 0/11] [D loss: 0.3440] [G loss: 1.6034] [ADA p: 0.800]\n",
      "[Epoch 1052/2000] [Batch 0/11] [D loss: 0.1191] [G loss: 0.4836] [ADA p: 0.800]\n",
      "[Epoch 1053/2000] [Batch 0/11] [D loss: 0.2626] [G loss: 0.6874] [ADA p: 0.800]\n",
      "[Epoch 1054/2000] [Batch 0/11] [D loss: 0.3501] [G loss: 0.8248] [ADA p: 0.800]\n",
      "[Epoch 1055/2000] [Batch 0/11] [D loss: 0.7370] [G loss: 0.9568] [ADA p: 0.800]\n",
      "[Epoch 1056/2000] [Batch 0/11] [D loss: 0.2429] [G loss: 1.4002] [ADA p: 0.800]\n",
      "[Epoch 1057/2000] [Batch 0/11] [D loss: 0.3559] [G loss: 1.0825] [ADA p: 0.800]\n",
      "[Epoch 1058/2000] [Batch 0/11] [D loss: 0.5927] [G loss: 1.7688] [ADA p: 0.800]\n",
      "[Epoch 1059/2000] [Batch 0/11] [D loss: 0.2653] [G loss: 0.8613] [ADA p: 0.800]\n",
      "[Epoch 1060/2000] [Batch 0/11] [D loss: 0.1678] [G loss: 0.5927] [ADA p: 0.800]\n",
      "[Epoch 1061/2000] [Batch 0/11] [D loss: 0.5694] [G loss: 0.9352] [ADA p: 0.800]\n",
      "[Epoch 1062/2000] [Batch 0/11] [D loss: 0.4750] [G loss: 1.2299] [ADA p: 0.800]\n",
      "[Epoch 1063/2000] [Batch 0/11] [D loss: 0.4112] [G loss: 1.8047] [ADA p: 0.800]\n",
      "[Epoch 1064/2000] [Batch 0/11] [D loss: 0.4825] [G loss: 0.9022] [ADA p: 0.800]\n",
      "[Epoch 1065/2000] [Batch 0/11] [D loss: 0.2437] [G loss: 0.5967] [ADA p: 0.800]\n",
      "[Epoch 1066/2000] [Batch 0/11] [D loss: 0.3001] [G loss: 1.3968] [ADA p: 0.800]\n",
      "[Epoch 1067/2000] [Batch 0/11] [D loss: 0.3603] [G loss: 0.8117] [ADA p: 0.800]\n",
      "[Epoch 1068/2000] [Batch 0/11] [D loss: 0.4605] [G loss: 1.2125] [ADA p: 0.800]\n",
      "[Epoch 1069/2000] [Batch 0/11] [D loss: 0.4502] [G loss: 1.0032] [ADA p: 0.800]\n",
      "[Epoch 1070/2000] [Batch 0/11] [D loss: 0.4873] [G loss: 1.7472] [ADA p: 0.800]\n",
      "[Epoch 1071/2000] [Batch 0/11] [D loss: 0.5090] [G loss: 0.6347] [ADA p: 0.800]\n",
      "[Epoch 1072/2000] [Batch 0/11] [D loss: 0.7367] [G loss: 0.4765] [ADA p: 0.800]\n",
      "[Epoch 1073/2000] [Batch 0/11] [D loss: 0.2961] [G loss: 0.9077] [ADA p: 0.800]\n",
      "[Epoch 1074/2000] [Batch 0/11] [D loss: 0.3202] [G loss: 1.4468] [ADA p: 0.800]\n",
      "[Epoch 1075/2000] [Batch 0/11] [D loss: 0.2392] [G loss: 1.3403] [ADA p: 0.800]\n",
      "[Epoch 1076/2000] [Batch 0/11] [D loss: 0.8411] [G loss: 1.5754] [ADA p: 0.800]\n",
      "[Epoch 1077/2000] [Batch 0/11] [D loss: 0.1221] [G loss: 1.9365] [ADA p: 0.800]\n",
      "[Epoch 1078/2000] [Batch 0/11] [D loss: 0.6276] [G loss: 1.1448] [ADA p: 0.800]\n",
      "[Epoch 1079/2000] [Batch 0/11] [D loss: 0.4175] [G loss: 1.1417] [ADA p: 0.800]\n",
      "[Epoch 1080/2000] [Batch 0/11] [D loss: 0.2050] [G loss: 1.1946] [ADA p: 0.800]\n",
      "[Epoch 1081/2000] [Batch 0/11] [D loss: 0.4696] [G loss: 2.0729] [ADA p: 0.800]\n",
      "[Epoch 1082/2000] [Batch 0/11] [D loss: 0.4823] [G loss: 1.1951] [ADA p: 0.800]\n",
      "[Epoch 1083/2000] [Batch 0/11] [D loss: 0.7328] [G loss: 1.4567] [ADA p: 0.800]\n",
      "[Epoch 1084/2000] [Batch 0/11] [D loss: 0.3038] [G loss: 1.7943] [ADA p: 0.800]\n",
      "[Epoch 1085/2000] [Batch 0/11] [D loss: 0.3610] [G loss: 0.5208] [ADA p: 0.800]\n",
      "[Epoch 1086/2000] [Batch 0/11] [D loss: 0.6231] [G loss: 0.4917] [ADA p: 0.800]\n",
      "[Epoch 1087/2000] [Batch 0/11] [D loss: 0.4428] [G loss: 0.8461] [ADA p: 0.800]\n",
      "[Epoch 1088/2000] [Batch 0/11] [D loss: 0.3931] [G loss: 1.4033] [ADA p: 0.800]\n",
      "[Epoch 1089/2000] [Batch 0/11] [D loss: 0.3091] [G loss: 0.5188] [ADA p: 0.800]\n",
      "[Epoch 1090/2000] [Batch 0/11] [D loss: 0.0877] [G loss: 0.9272] [ADA p: 0.800]\n",
      "[Epoch 1091/2000] [Batch 0/11] [D loss: 0.5328] [G loss: 1.8973] [ADA p: 0.800]\n",
      "[Epoch 1092/2000] [Batch 0/11] [D loss: 0.3142] [G loss: 1.3737] [ADA p: 0.800]\n",
      "[Epoch 1093/2000] [Batch 0/11] [D loss: 0.2906] [G loss: 1.7115] [ADA p: 0.800]\n",
      "[Epoch 1094/2000] [Batch 0/11] [D loss: 0.4803] [G loss: 1.5506] [ADA p: 0.800]\n",
      "[Epoch 1095/2000] [Batch 0/11] [D loss: 0.5374] [G loss: 1.2656] [ADA p: 0.800]\n",
      "[Epoch 1096/2000] [Batch 0/11] [D loss: 0.4493] [G loss: 0.9382] [ADA p: 0.800]\n",
      "[Epoch 1097/2000] [Batch 0/11] [D loss: 0.4687] [G loss: 1.1656] [ADA p: 0.800]\n",
      "[Epoch 1098/2000] [Batch 0/11] [D loss: 0.4961] [G loss: 0.6059] [ADA p: 0.800]\n",
      "[Epoch 1099/2000] [Batch 0/11] [D loss: 0.9245] [G loss: 0.5041] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.44it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1099 | FID: 337.95 | ADA p: 0.800\n",
      "[Epoch 1100/2000] [Batch 0/11] [D loss: 0.5028] [G loss: 3.0170] [ADA p: 0.800]\n",
      "[Epoch 1101/2000] [Batch 0/11] [D loss: 0.7409] [G loss: 0.7920] [ADA p: 0.800]\n",
      "[Epoch 1102/2000] [Batch 0/11] [D loss: 0.4302] [G loss: 1.3865] [ADA p: 0.800]\n",
      "[Epoch 1103/2000] [Batch 0/11] [D loss: 0.4854] [G loss: 1.5200] [ADA p: 0.800]\n",
      "[Epoch 1104/2000] [Batch 0/11] [D loss: 0.5244] [G loss: 1.3895] [ADA p: 0.800]\n",
      "[Epoch 1105/2000] [Batch 0/11] [D loss: 0.2464] [G loss: 1.6511] [ADA p: 0.800]\n",
      "[Epoch 1106/2000] [Batch 0/11] [D loss: 1.0750] [G loss: 0.8558] [ADA p: 0.800]\n",
      "[Epoch 1107/2000] [Batch 0/11] [D loss: 0.4162] [G loss: 0.3846] [ADA p: 0.800]\n",
      "[Epoch 1108/2000] [Batch 0/11] [D loss: 0.2334] [G loss: 1.0696] [ADA p: 0.800]\n",
      "[Epoch 1109/2000] [Batch 0/11] [D loss: 0.4882] [G loss: 0.8397] [ADA p: 0.800]\n",
      "[Epoch 1110/2000] [Batch 0/11] [D loss: 0.3163] [G loss: 1.1564] [ADA p: 0.800]\n",
      "[Epoch 1111/2000] [Batch 0/11] [D loss: 0.3720] [G loss: 0.8658] [ADA p: 0.800]\n",
      "[Epoch 1112/2000] [Batch 0/11] [D loss: 0.3540] [G loss: 0.8261] [ADA p: 0.800]\n",
      "[Epoch 1113/2000] [Batch 0/11] [D loss: 0.4348] [G loss: 0.2338] [ADA p: 0.800]\n",
      "[Epoch 1114/2000] [Batch 0/11] [D loss: 0.2645] [G loss: 0.8688] [ADA p: 0.800]\n",
      "[Epoch 1115/2000] [Batch 0/11] [D loss: 0.5695] [G loss: 1.3422] [ADA p: 0.800]\n",
      "[Epoch 1116/2000] [Batch 0/11] [D loss: 0.2754] [G loss: 1.6055] [ADA p: 0.800]\n",
      "[Epoch 1117/2000] [Batch 0/11] [D loss: 0.5574] [G loss: 1.3500] [ADA p: 0.800]\n",
      "[Epoch 1118/2000] [Batch 0/11] [D loss: 0.6015] [G loss: 0.9887] [ADA p: 0.800]\n",
      "[Epoch 1119/2000] [Batch 0/11] [D loss: 0.4838] [G loss: 0.8651] [ADA p: 0.800]\n",
      "[Epoch 1120/2000] [Batch 0/11] [D loss: 0.3564] [G loss: 1.7195] [ADA p: 0.800]\n",
      "[Epoch 1121/2000] [Batch 0/11] [D loss: 0.6807] [G loss: 1.6019] [ADA p: 0.800]\n",
      "[Epoch 1122/2000] [Batch 0/11] [D loss: 0.5583] [G loss: 1.3978] [ADA p: 0.800]\n",
      "[Epoch 1123/2000] [Batch 0/11] [D loss: 0.3392] [G loss: 0.4082] [ADA p: 0.800]\n",
      "[Epoch 1124/2000] [Batch 0/11] [D loss: 0.5497] [G loss: 0.9090] [ADA p: 0.800]\n",
      "[Epoch 1125/2000] [Batch 0/11] [D loss: 0.2821] [G loss: 0.5489] [ADA p: 0.800]\n",
      "[Epoch 1126/2000] [Batch 0/11] [D loss: 0.4211] [G loss: 0.7997] [ADA p: 0.800]\n",
      "[Epoch 1127/2000] [Batch 0/11] [D loss: 0.1973] [G loss: 0.2638] [ADA p: 0.800]\n",
      "[Epoch 1128/2000] [Batch 0/11] [D loss: 0.2716] [G loss: 1.0005] [ADA p: 0.800]\n",
      "[Epoch 1129/2000] [Batch 0/11] [D loss: 0.3146] [G loss: 1.4605] [ADA p: 0.800]\n",
      "[Epoch 1130/2000] [Batch 0/11] [D loss: 0.3488] [G loss: 1.0405] [ADA p: 0.800]\n",
      "[Epoch 1131/2000] [Batch 0/11] [D loss: 0.6811] [G loss: 1.4096] [ADA p: 0.800]\n",
      "[Epoch 1132/2000] [Batch 0/11] [D loss: 0.3993] [G loss: 0.8429] [ADA p: 0.800]\n",
      "[Epoch 1133/2000] [Batch 0/11] [D loss: 0.5546] [G loss: 1.3360] [ADA p: 0.800]\n",
      "[Epoch 1134/2000] [Batch 0/11] [D loss: 0.5656] [G loss: 0.8673] [ADA p: 0.800]\n",
      "[Epoch 1135/2000] [Batch 0/11] [D loss: 0.3907] [G loss: 1.1336] [ADA p: 0.800]\n",
      "[Epoch 1136/2000] [Batch 0/11] [D loss: 0.4573] [G loss: 1.4176] [ADA p: 0.800]\n",
      "[Epoch 1137/2000] [Batch 0/11] [D loss: 0.2716] [G loss: 0.4306] [ADA p: 0.800]\n",
      "[Epoch 1138/2000] [Batch 0/11] [D loss: 0.5639] [G loss: 1.5556] [ADA p: 0.800]\n",
      "[Epoch 1139/2000] [Batch 0/11] [D loss: 0.5263] [G loss: 0.9416] [ADA p: 0.800]\n",
      "[Epoch 1140/2000] [Batch 0/11] [D loss: 0.1789] [G loss: 0.7052] [ADA p: 0.800]\n",
      "[Epoch 1141/2000] [Batch 0/11] [D loss: 0.4948] [G loss: 1.3433] [ADA p: 0.800]\n",
      "[Epoch 1142/2000] [Batch 0/11] [D loss: 0.2162] [G loss: 1.3174] [ADA p: 0.800]\n",
      "[Epoch 1143/2000] [Batch 0/11] [D loss: 0.3363] [G loss: 0.9148] [ADA p: 0.800]\n",
      "[Epoch 1144/2000] [Batch 0/11] [D loss: 0.2636] [G loss: 0.9872] [ADA p: 0.800]\n",
      "[Epoch 1145/2000] [Batch 0/11] [D loss: 0.0907] [G loss: 0.6550] [ADA p: 0.800]\n",
      "[Epoch 1146/2000] [Batch 0/11] [D loss: 0.6581] [G loss: 0.7608] [ADA p: 0.800]\n",
      "[Epoch 1147/2000] [Batch 0/11] [D loss: 0.7699] [G loss: 1.4307] [ADA p: 0.800]\n",
      "[Epoch 1148/2000] [Batch 0/11] [D loss: 0.1731] [G loss: 2.6281] [ADA p: 0.800]\n",
      "[Epoch 1149/2000] [Batch 0/11] [D loss: 0.8484] [G loss: 1.7855] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.57it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1149 | FID: 355.64 | ADA p: 0.800\n",
      "[Epoch 1150/2000] [Batch 0/11] [D loss: 0.1689] [G loss: 0.8984] [ADA p: 0.800]\n",
      "[Epoch 1151/2000] [Batch 0/11] [D loss: 0.3687] [G loss: 1.9966] [ADA p: 0.800]\n",
      "[Epoch 1152/2000] [Batch 0/11] [D loss: 0.1599] [G loss: 1.9640] [ADA p: 0.800]\n",
      "[Epoch 1153/2000] [Batch 0/11] [D loss: 0.3114] [G loss: 1.9588] [ADA p: 0.800]\n",
      "[Epoch 1154/2000] [Batch 0/11] [D loss: 0.7685] [G loss: 1.0130] [ADA p: 0.800]\n",
      "[Epoch 1155/2000] [Batch 0/11] [D loss: 0.5299] [G loss: 1.1858] [ADA p: 0.800]\n",
      "[Epoch 1156/2000] [Batch 0/11] [D loss: 0.3525] [G loss: 0.8498] [ADA p: 0.800]\n",
      "[Epoch 1157/2000] [Batch 0/11] [D loss: 0.5524] [G loss: 0.8144] [ADA p: 0.800]\n",
      "[Epoch 1158/2000] [Batch 0/11] [D loss: 0.4072] [G loss: 0.9632] [ADA p: 0.800]\n",
      "[Epoch 1159/2000] [Batch 0/11] [D loss: 0.2212] [G loss: 1.4577] [ADA p: 0.800]\n",
      "[Epoch 1160/2000] [Batch 0/11] [D loss: 0.1781] [G loss: 1.6575] [ADA p: 0.800]\n",
      "[Epoch 1161/2000] [Batch 0/11] [D loss: 0.5155] [G loss: 1.3034] [ADA p: 0.800]\n",
      "[Epoch 1162/2000] [Batch 0/11] [D loss: 0.7219] [G loss: 1.5197] [ADA p: 0.800]\n",
      "[Epoch 1163/2000] [Batch 0/11] [D loss: 0.3002] [G loss: 0.6621] [ADA p: 0.800]\n",
      "[Epoch 1164/2000] [Batch 0/11] [D loss: 0.0593] [G loss: 0.4663] [ADA p: 0.800]\n",
      "[Epoch 1165/2000] [Batch 0/11] [D loss: 0.3443] [G loss: 0.9898] [ADA p: 0.800]\n",
      "[Epoch 1166/2000] [Batch 0/11] [D loss: 0.5216] [G loss: 0.5425] [ADA p: 0.800]\n",
      "[Epoch 1167/2000] [Batch 0/11] [D loss: 0.4682] [G loss: 0.8599] [ADA p: 0.800]\n",
      "[Epoch 1168/2000] [Batch 0/11] [D loss: 0.2930] [G loss: 1.2950] [ADA p: 0.800]\n",
      "[Epoch 1169/2000] [Batch 0/11] [D loss: 0.2815] [G loss: 1.0092] [ADA p: 0.800]\n",
      "[Epoch 1170/2000] [Batch 0/11] [D loss: 0.5862] [G loss: 0.7426] [ADA p: 0.800]\n",
      "[Epoch 1171/2000] [Batch 0/11] [D loss: 0.4439] [G loss: 1.6514] [ADA p: 0.800]\n",
      "[Epoch 1172/2000] [Batch 0/11] [D loss: 0.4194] [G loss: 0.7402] [ADA p: 0.800]\n",
      "[Epoch 1173/2000] [Batch 0/11] [D loss: 0.1254] [G loss: 1.8430] [ADA p: 0.800]\n",
      "[Epoch 1174/2000] [Batch 0/11] [D loss: 0.3187] [G loss: 0.7107] [ADA p: 0.800]\n",
      "[Epoch 1175/2000] [Batch 0/11] [D loss: 0.2096] [G loss: 0.7662] [ADA p: 0.800]\n",
      "[Epoch 1176/2000] [Batch 0/11] [D loss: 0.9070] [G loss: 1.8462] [ADA p: 0.800]\n",
      "[Epoch 1177/2000] [Batch 0/11] [D loss: 0.4455] [G loss: 2.7263] [ADA p: 0.800]\n",
      "[Epoch 1178/2000] [Batch 0/11] [D loss: 0.1251] [G loss: 2.0242] [ADA p: 0.800]\n",
      "[Epoch 1179/2000] [Batch 0/11] [D loss: 0.3427] [G loss: 0.6255] [ADA p: 0.800]\n",
      "[Epoch 1180/2000] [Batch 0/11] [D loss: 0.2702] [G loss: 0.8121] [ADA p: 0.800]\n",
      "[Epoch 1181/2000] [Batch 0/11] [D loss: 0.6649] [G loss: 0.9088] [ADA p: 0.800]\n",
      "[Epoch 1182/2000] [Batch 0/11] [D loss: 0.5549] [G loss: 1.3386] [ADA p: 0.800]\n",
      "[Epoch 1183/2000] [Batch 0/11] [D loss: 0.1867] [G loss: 1.1234] [ADA p: 0.800]\n",
      "[Epoch 1184/2000] [Batch 0/11] [D loss: 1.0825] [G loss: 1.4668] [ADA p: 0.800]\n",
      "[Epoch 1185/2000] [Batch 0/11] [D loss: 0.4005] [G loss: 0.9250] [ADA p: 0.800]\n",
      "[Epoch 1186/2000] [Batch 0/11] [D loss: 0.2811] [G loss: 1.6020] [ADA p: 0.800]\n",
      "[Epoch 1187/2000] [Batch 0/11] [D loss: 0.1982] [G loss: 2.1680] [ADA p: 0.800]\n",
      "[Epoch 1188/2000] [Batch 0/11] [D loss: 0.2388] [G loss: 0.7259] [ADA p: 0.800]\n",
      "[Epoch 1189/2000] [Batch 0/11] [D loss: 0.3547] [G loss: 1.1440] [ADA p: 0.800]\n",
      "[Epoch 1190/2000] [Batch 0/11] [D loss: 0.3267] [G loss: 0.6379] [ADA p: 0.800]\n",
      "[Epoch 1191/2000] [Batch 0/11] [D loss: 0.5865] [G loss: 1.0404] [ADA p: 0.800]\n",
      "[Epoch 1192/2000] [Batch 0/11] [D loss: 0.3236] [G loss: 0.6753] [ADA p: 0.800]\n",
      "[Epoch 1193/2000] [Batch 0/11] [D loss: 0.2392] [G loss: 0.6528] [ADA p: 0.800]\n",
      "[Epoch 1194/2000] [Batch 0/11] [D loss: 0.1544] [G loss: 0.4511] [ADA p: 0.800]\n",
      "[Epoch 1195/2000] [Batch 0/11] [D loss: 0.1746] [G loss: 0.9939] [ADA p: 0.800]\n",
      "[Epoch 1196/2000] [Batch 0/11] [D loss: 0.3770] [G loss: 1.3208] [ADA p: 0.800]\n",
      "[Epoch 1197/2000] [Batch 0/11] [D loss: 0.4845] [G loss: 0.8724] [ADA p: 0.800]\n",
      "[Epoch 1198/2000] [Batch 0/11] [D loss: 0.6986] [G loss: 1.1595] [ADA p: 0.800]\n",
      "[Epoch 1199/2000] [Batch 0/11] [D loss: 0.5259] [G loss: 1.1518] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.34it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1199 | FID: 344.24 | ADA p: 0.800\n",
      "[Epoch 1200/2000] [Batch 0/11] [D loss: 0.6485] [G loss: 0.2934] [ADA p: 0.800]\n",
      "[Epoch 1201/2000] [Batch 0/11] [D loss: 0.2800] [G loss: 0.8732] [ADA p: 0.800]\n",
      "[Epoch 1202/2000] [Batch 0/11] [D loss: 0.4244] [G loss: 0.4596] [ADA p: 0.800]\n",
      "[Epoch 1203/2000] [Batch 0/11] [D loss: 0.2016] [G loss: 1.6226] [ADA p: 0.800]\n",
      "[Epoch 1204/2000] [Batch 0/11] [D loss: 0.2341] [G loss: 1.4233] [ADA p: 0.800]\n",
      "[Epoch 1205/2000] [Batch 0/11] [D loss: 0.2561] [G loss: 1.9309] [ADA p: 0.800]\n",
      "[Epoch 1206/2000] [Batch 0/11] [D loss: 0.5004] [G loss: 0.8432] [ADA p: 0.800]\n",
      "[Epoch 1207/2000] [Batch 0/11] [D loss: 0.2565] [G loss: 1.4340] [ADA p: 0.800]\n",
      "[Epoch 1208/2000] [Batch 0/11] [D loss: 0.3565] [G loss: 0.7629] [ADA p: 0.800]\n",
      "[Epoch 1209/2000] [Batch 0/11] [D loss: 0.4566] [G loss: 0.9811] [ADA p: 0.800]\n",
      "[Epoch 1210/2000] [Batch 0/11] [D loss: 0.2457] [G loss: 1.1061] [ADA p: 0.800]\n",
      "[Epoch 1211/2000] [Batch 0/11] [D loss: 0.2800] [G loss: 1.2980] [ADA p: 0.800]\n",
      "[Epoch 1212/2000] [Batch 0/11] [D loss: 0.3631] [G loss: 0.7830] [ADA p: 0.800]\n",
      "[Epoch 1213/2000] [Batch 0/11] [D loss: 0.4549] [G loss: 1.2719] [ADA p: 0.800]\n",
      "[Epoch 1214/2000] [Batch 0/11] [D loss: 0.6351] [G loss: 1.8086] [ADA p: 0.800]\n",
      "[Epoch 1215/2000] [Batch 0/11] [D loss: 0.3048] [G loss: 1.4298] [ADA p: 0.800]\n",
      "[Epoch 1216/2000] [Batch 0/11] [D loss: 0.2027] [G loss: 1.4470] [ADA p: 0.800]\n",
      "[Epoch 1217/2000] [Batch 0/11] [D loss: 0.2550] [G loss: 1.7030] [ADA p: 0.800]\n",
      "[Epoch 1218/2000] [Batch 0/11] [D loss: 0.6363] [G loss: 0.6771] [ADA p: 0.800]\n",
      "[Epoch 1219/2000] [Batch 0/11] [D loss: 0.2101] [G loss: 0.6924] [ADA p: 0.800]\n",
      "[Epoch 1220/2000] [Batch 0/11] [D loss: 0.7724] [G loss: 0.5989] [ADA p: 0.800]\n",
      "[Epoch 1221/2000] [Batch 0/11] [D loss: 0.3014] [G loss: 0.5208] [ADA p: 0.800]\n",
      "[Epoch 1222/2000] [Batch 0/11] [D loss: 0.3668] [G loss: 0.5467] [ADA p: 0.800]\n",
      "[Epoch 1223/2000] [Batch 0/11] [D loss: 0.0999] [G loss: 1.0238] [ADA p: 0.800]\n",
      "[Epoch 1224/2000] [Batch 0/11] [D loss: 0.3027] [G loss: 0.7762] [ADA p: 0.800]\n",
      "[Epoch 1225/2000] [Batch 0/11] [D loss: 0.2641] [G loss: 0.9703] [ADA p: 0.800]\n",
      "[Epoch 1226/2000] [Batch 0/11] [D loss: 0.4705] [G loss: 0.4284] [ADA p: 0.800]\n",
      "[Epoch 1227/2000] [Batch 0/11] [D loss: 0.5407] [G loss: 1.4198] [ADA p: 0.800]\n",
      "[Epoch 1228/2000] [Batch 0/11] [D loss: 0.4664] [G loss: 1.6299] [ADA p: 0.800]\n",
      "[Epoch 1229/2000] [Batch 0/11] [D loss: 0.1676] [G loss: 1.1168] [ADA p: 0.800]\n",
      "[Epoch 1230/2000] [Batch 0/11] [D loss: 0.2035] [G loss: 1.0188] [ADA p: 0.800]\n",
      "[Epoch 1231/2000] [Batch 0/11] [D loss: 0.2518] [G loss: 1.6494] [ADA p: 0.800]\n",
      "[Epoch 1232/2000] [Batch 0/11] [D loss: 0.4243] [G loss: 1.6332] [ADA p: 0.800]\n",
      "[Epoch 1233/2000] [Batch 0/11] [D loss: 0.2023] [G loss: 0.9182] [ADA p: 0.800]\n",
      "[Epoch 1234/2000] [Batch 0/11] [D loss: 0.8010] [G loss: 1.9722] [ADA p: 0.800]\n",
      "[Epoch 1235/2000] [Batch 0/11] [D loss: 0.5071] [G loss: 0.8883] [ADA p: 0.800]\n",
      "[Epoch 1236/2000] [Batch 0/11] [D loss: 0.5534] [G loss: 0.8780] [ADA p: 0.800]\n",
      "[Epoch 1237/2000] [Batch 0/11] [D loss: 0.2477] [G loss: 0.5539] [ADA p: 0.800]\n",
      "[Epoch 1238/2000] [Batch 0/11] [D loss: 0.7291] [G loss: 1.5602] [ADA p: 0.800]\n",
      "[Epoch 1239/2000] [Batch 0/11] [D loss: 0.4317] [G loss: 1.9683] [ADA p: 0.800]\n",
      "[Epoch 1240/2000] [Batch 0/11] [D loss: 0.3198] [G loss: 0.9478] [ADA p: 0.800]\n",
      "[Epoch 1241/2000] [Batch 0/11] [D loss: 0.1765] [G loss: 0.2901] [ADA p: 0.800]\n",
      "[Epoch 1242/2000] [Batch 0/11] [D loss: 0.3221] [G loss: 1.4744] [ADA p: 0.800]\n",
      "[Epoch 1243/2000] [Batch 0/11] [D loss: 0.2623] [G loss: 2.0845] [ADA p: 0.800]\n",
      "[Epoch 1244/2000] [Batch 0/11] [D loss: 0.3419] [G loss: 2.1116] [ADA p: 0.800]\n",
      "[Epoch 1245/2000] [Batch 0/11] [D loss: 0.2442] [G loss: 1.9363] [ADA p: 0.800]\n",
      "[Epoch 1246/2000] [Batch 0/11] [D loss: 0.2801] [G loss: 1.2475] [ADA p: 0.800]\n",
      "[Epoch 1247/2000] [Batch 0/11] [D loss: 0.2859] [G loss: 1.4647] [ADA p: 0.800]\n",
      "[Epoch 1248/2000] [Batch 0/11] [D loss: 0.8141] [G loss: 1.3129] [ADA p: 0.800]\n",
      "[Epoch 1249/2000] [Batch 0/11] [D loss: 0.2942] [G loss: 1.4673] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.55it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1249 | FID: 351.42 | ADA p: 0.800\n",
      "[Epoch 1250/2000] [Batch 0/11] [D loss: 0.2159] [G loss: 0.8323] [ADA p: 0.800]\n",
      "[Epoch 1251/2000] [Batch 0/11] [D loss: 0.3470] [G loss: 1.3220] [ADA p: 0.800]\n",
      "[Epoch 1252/2000] [Batch 0/11] [D loss: 0.3616] [G loss: 0.3779] [ADA p: 0.800]\n",
      "[Epoch 1253/2000] [Batch 0/11] [D loss: 0.2713] [G loss: 1.9910] [ADA p: 0.800]\n",
      "[Epoch 1254/2000] [Batch 0/11] [D loss: 0.2496] [G loss: 1.3592] [ADA p: 0.800]\n",
      "[Epoch 1255/2000] [Batch 0/11] [D loss: 0.6301] [G loss: 1.1539] [ADA p: 0.800]\n",
      "[Epoch 1256/2000] [Batch 0/11] [D loss: 0.3477] [G loss: 1.5561] [ADA p: 0.800]\n",
      "[Epoch 1257/2000] [Batch 0/11] [D loss: 0.2582] [G loss: 1.4256] [ADA p: 0.800]\n",
      "[Epoch 1258/2000] [Batch 0/11] [D loss: 0.1916] [G loss: 1.4220] [ADA p: 0.800]\n",
      "[Epoch 1259/2000] [Batch 0/11] [D loss: 0.2412] [G loss: 1.4723] [ADA p: 0.800]\n",
      "[Epoch 1260/2000] [Batch 0/11] [D loss: 0.1322] [G loss: 1.1153] [ADA p: 0.800]\n",
      "[Epoch 1261/2000] [Batch 0/11] [D loss: 0.1308] [G loss: 1.2351] [ADA p: 0.800]\n",
      "[Epoch 1262/2000] [Batch 0/11] [D loss: 1.1189] [G loss: 1.0993] [ADA p: 0.800]\n",
      "[Epoch 1263/2000] [Batch 0/11] [D loss: 0.2158] [G loss: 0.6847] [ADA p: 0.800]\n",
      "[Epoch 1264/2000] [Batch 0/11] [D loss: 0.2055] [G loss: 0.8061] [ADA p: 0.800]\n",
      "[Epoch 1265/2000] [Batch 0/11] [D loss: 0.9037] [G loss: 0.9350] [ADA p: 0.800]\n",
      "[Epoch 1266/2000] [Batch 0/11] [D loss: 0.3603] [G loss: 0.9784] [ADA p: 0.800]\n",
      "[Epoch 1267/2000] [Batch 0/11] [D loss: 0.1878] [G loss: 1.5763] [ADA p: 0.800]\n",
      "[Epoch 1268/2000] [Batch 0/11] [D loss: 0.4384] [G loss: 0.1918] [ADA p: 0.800]\n",
      "[Epoch 1269/2000] [Batch 0/11] [D loss: 0.2555] [G loss: 1.1572] [ADA p: 0.800]\n",
      "[Epoch 1270/2000] [Batch 0/11] [D loss: 0.5849] [G loss: 1.5926] [ADA p: 0.800]\n",
      "[Epoch 1271/2000] [Batch 0/11] [D loss: 0.2429] [G loss: 1.3446] [ADA p: 0.800]\n",
      "[Epoch 1272/2000] [Batch 0/11] [D loss: 0.3847] [G loss: 1.7086] [ADA p: 0.800]\n",
      "[Epoch 1273/2000] [Batch 0/11] [D loss: 0.3731] [G loss: 1.7742] [ADA p: 0.800]\n",
      "[Epoch 1274/2000] [Batch 0/11] [D loss: 0.4568] [G loss: 1.7973] [ADA p: 0.800]\n",
      "[Epoch 1275/2000] [Batch 0/11] [D loss: 0.2724] [G loss: 1.5193] [ADA p: 0.800]\n",
      "[Epoch 1276/2000] [Batch 0/11] [D loss: 0.4208] [G loss: 0.2435] [ADA p: 0.800]\n",
      "[Epoch 1277/2000] [Batch 0/11] [D loss: 0.2837] [G loss: 0.6637] [ADA p: 0.800]\n",
      "[Epoch 1278/2000] [Batch 0/11] [D loss: 0.6112] [G loss: 0.4666] [ADA p: 0.800]\n",
      "[Epoch 1279/2000] [Batch 0/11] [D loss: 0.5810] [G loss: 0.9448] [ADA p: 0.800]\n",
      "[Epoch 1280/2000] [Batch 0/11] [D loss: 0.2510] [G loss: 1.5537] [ADA p: 0.800]\n",
      "[Epoch 1281/2000] [Batch 0/11] [D loss: 0.2706] [G loss: 1.1684] [ADA p: 0.800]\n",
      "[Epoch 1282/2000] [Batch 0/11] [D loss: 0.2817] [G loss: 1.5447] [ADA p: 0.800]\n",
      "[Epoch 1283/2000] [Batch 0/11] [D loss: 0.1397] [G loss: 1.9369] [ADA p: 0.800]\n",
      "[Epoch 1284/2000] [Batch 0/11] [D loss: 0.6321] [G loss: 1.0785] [ADA p: 0.800]\n",
      "[Epoch 1285/2000] [Batch 0/11] [D loss: 0.1598] [G loss: 1.2961] [ADA p: 0.800]\n",
      "[Epoch 1286/2000] [Batch 0/11] [D loss: 0.2200] [G loss: 0.3782] [ADA p: 0.800]\n",
      "[Epoch 1287/2000] [Batch 0/11] [D loss: 0.5238] [G loss: 1.5470] [ADA p: 0.800]\n",
      "[Epoch 1288/2000] [Batch 0/11] [D loss: 0.2818] [G loss: 0.8010] [ADA p: 0.800]\n",
      "[Epoch 1289/2000] [Batch 0/11] [D loss: 0.2603] [G loss: 1.0566] [ADA p: 0.800]\n",
      "[Epoch 1290/2000] [Batch 0/11] [D loss: 0.4150] [G loss: 1.5098] [ADA p: 0.800]\n",
      "[Epoch 1291/2000] [Batch 0/11] [D loss: 0.5023] [G loss: 2.9728] [ADA p: 0.800]\n",
      "[Epoch 1292/2000] [Batch 0/11] [D loss: 0.3546] [G loss: 0.8663] [ADA p: 0.800]\n",
      "[Epoch 1293/2000] [Batch 0/11] [D loss: 0.2656] [G loss: 1.1752] [ADA p: 0.800]\n",
      "[Epoch 1294/2000] [Batch 0/11] [D loss: 0.3992] [G loss: 1.0473] [ADA p: 0.800]\n",
      "[Epoch 1295/2000] [Batch 0/11] [D loss: 0.1382] [G loss: 0.3287] [ADA p: 0.800]\n",
      "[Epoch 1296/2000] [Batch 0/11] [D loss: 0.1963] [G loss: 1.2659] [ADA p: 0.800]\n",
      "[Epoch 1297/2000] [Batch 0/11] [D loss: 0.4586] [G loss: 1.5207] [ADA p: 0.800]\n",
      "[Epoch 1298/2000] [Batch 0/11] [D loss: 0.1228] [G loss: 0.9068] [ADA p: 0.800]\n",
      "[Epoch 1299/2000] [Batch 0/11] [D loss: 0.3355] [G loss: 1.9575] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.65it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1299 | FID: 347.92 | ADA p: 0.800\n",
      "[Epoch 1300/2000] [Batch 0/11] [D loss: 0.2386] [G loss: 0.7953] [ADA p: 0.800]\n",
      "[Epoch 1301/2000] [Batch 0/11] [D loss: 0.2987] [G loss: 0.4987] [ADA p: 0.800]\n",
      "[Epoch 1302/2000] [Batch 0/11] [D loss: 0.3390] [G loss: 0.2522] [ADA p: 0.800]\n",
      "[Epoch 1303/2000] [Batch 0/11] [D loss: 0.2661] [G loss: 1.4653] [ADA p: 0.800]\n",
      "[Epoch 1304/2000] [Batch 0/11] [D loss: 0.4363] [G loss: 0.6154] [ADA p: 0.800]\n",
      "[Epoch 1305/2000] [Batch 0/11] [D loss: 0.5220] [G loss: 1.6947] [ADA p: 0.800]\n",
      "[Epoch 1306/2000] [Batch 0/11] [D loss: 0.2637] [G loss: 0.7906] [ADA p: 0.800]\n",
      "[Epoch 1307/2000] [Batch 0/11] [D loss: 0.2286] [G loss: 1.5305] [ADA p: 0.800]\n",
      "[Epoch 1308/2000] [Batch 0/11] [D loss: 0.2784] [G loss: 1.7034] [ADA p: 0.800]\n",
      "[Epoch 1309/2000] [Batch 0/11] [D loss: 0.5971] [G loss: 2.0378] [ADA p: 0.800]\n",
      "[Epoch 1310/2000] [Batch 0/11] [D loss: 0.5701] [G loss: 1.3898] [ADA p: 0.800]\n",
      "[Epoch 1311/2000] [Batch 0/11] [D loss: 0.2942] [G loss: 1.5047] [ADA p: 0.800]\n",
      "[Epoch 1312/2000] [Batch 0/11] [D loss: 0.1509] [G loss: 0.8552] [ADA p: 0.800]\n",
      "[Epoch 1313/2000] [Batch 0/11] [D loss: 0.2680] [G loss: 1.1664] [ADA p: 0.800]\n",
      "[Epoch 1314/2000] [Batch 0/11] [D loss: 0.0482] [G loss: 1.2296] [ADA p: 0.800]\n",
      "[Epoch 1315/2000] [Batch 0/11] [D loss: 0.3769] [G loss: 0.6919] [ADA p: 0.800]\n",
      "[Epoch 1316/2000] [Batch 0/11] [D loss: 0.1564] [G loss: 1.8722] [ADA p: 0.800]\n",
      "[Epoch 1317/2000] [Batch 0/11] [D loss: 0.1899] [G loss: 0.7764] [ADA p: 0.800]\n",
      "[Epoch 1318/2000] [Batch 0/11] [D loss: 0.3490] [G loss: 1.4949] [ADA p: 0.800]\n",
      "[Epoch 1319/2000] [Batch 0/11] [D loss: 0.2585] [G loss: 1.2634] [ADA p: 0.800]\n",
      "[Epoch 1320/2000] [Batch 0/11] [D loss: 0.1160] [G loss: 1.0531] [ADA p: 0.800]\n",
      "[Epoch 1321/2000] [Batch 0/11] [D loss: 0.1154] [G loss: 1.9310] [ADA p: 0.800]\n",
      "[Epoch 1322/2000] [Batch 0/11] [D loss: 0.3716] [G loss: 1.1034] [ADA p: 0.800]\n",
      "[Epoch 1323/2000] [Batch 0/11] [D loss: 0.5469] [G loss: 0.9590] [ADA p: 0.800]\n",
      "[Epoch 1324/2000] [Batch 0/11] [D loss: 0.2207] [G loss: 1.5647] [ADA p: 0.800]\n",
      "[Epoch 1325/2000] [Batch 0/11] [D loss: 0.2559] [G loss: 1.5415] [ADA p: 0.800]\n",
      "[Epoch 1326/2000] [Batch 0/11] [D loss: 1.0603] [G loss: 0.5933] [ADA p: 0.800]\n",
      "[Epoch 1327/2000] [Batch 0/11] [D loss: 0.1119] [G loss: 1.5739] [ADA p: 0.800]\n",
      "[Epoch 1328/2000] [Batch 0/11] [D loss: 0.7581] [G loss: 0.9665] [ADA p: 0.800]\n",
      "[Epoch 1329/2000] [Batch 0/11] [D loss: 0.4744] [G loss: 1.6087] [ADA p: 0.800]\n",
      "[Epoch 1330/2000] [Batch 0/11] [D loss: 0.4410] [G loss: 1.3716] [ADA p: 0.800]\n",
      "[Epoch 1331/2000] [Batch 0/11] [D loss: 0.3073] [G loss: 1.6244] [ADA p: 0.800]\n",
      "[Epoch 1332/2000] [Batch 0/11] [D loss: 0.8131] [G loss: 1.4838] [ADA p: 0.800]\n",
      "[Epoch 1333/2000] [Batch 0/11] [D loss: 0.1496] [G loss: 0.7110] [ADA p: 0.800]\n",
      "[Epoch 1334/2000] [Batch 0/11] [D loss: 0.3820] [G loss: 0.8719] [ADA p: 0.800]\n",
      "[Epoch 1335/2000] [Batch 0/11] [D loss: 0.3496] [G loss: 0.7080] [ADA p: 0.800]\n",
      "[Epoch 1336/2000] [Batch 0/11] [D loss: 0.1402] [G loss: 1.7894] [ADA p: 0.800]\n",
      "[Epoch 1337/2000] [Batch 0/11] [D loss: 0.3330] [G loss: 0.6683] [ADA p: 0.800]\n",
      "[Epoch 1338/2000] [Batch 0/11] [D loss: 0.1415] [G loss: 0.8790] [ADA p: 0.800]\n",
      "[Epoch 1339/2000] [Batch 0/11] [D loss: 0.2245] [G loss: 0.8941] [ADA p: 0.800]\n",
      "[Epoch 1340/2000] [Batch 0/11] [D loss: 0.1395] [G loss: 1.7111] [ADA p: 0.800]\n",
      "[Epoch 1341/2000] [Batch 0/11] [D loss: 0.5184] [G loss: 0.5774] [ADA p: 0.800]\n",
      "[Epoch 1342/2000] [Batch 0/11] [D loss: 0.4649] [G loss: 1.2301] [ADA p: 0.800]\n",
      "[Epoch 1343/2000] [Batch 0/11] [D loss: 0.7268] [G loss: 2.3479] [ADA p: 0.800]\n",
      "[Epoch 1344/2000] [Batch 0/11] [D loss: 0.3562] [G loss: 0.2887] [ADA p: 0.800]\n",
      "[Epoch 1345/2000] [Batch 0/11] [D loss: 0.6006] [G loss: 1.7222] [ADA p: 0.800]\n",
      "[Epoch 1346/2000] [Batch 0/11] [D loss: 0.5234] [G loss: 1.1309] [ADA p: 0.800]\n",
      "[Epoch 1347/2000] [Batch 0/11] [D loss: 0.2465] [G loss: 1.3087] [ADA p: 0.800]\n",
      "[Epoch 1348/2000] [Batch 0/11] [D loss: 0.2134] [G loss: 1.5353] [ADA p: 0.800]\n",
      "[Epoch 1349/2000] [Batch 0/11] [D loss: 0.2513] [G loss: 1.4881] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.41it/s]\n",
      "100%|██████████| 63/63 [00:11<00:00,  5.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1349 | FID: 342.61 | ADA p: 0.800\n",
      "[Epoch 1350/2000] [Batch 0/11] [D loss: 0.2642] [G loss: 1.3152] [ADA p: 0.800]\n",
      "[Epoch 1351/2000] [Batch 0/11] [D loss: 0.3958] [G loss: 1.2071] [ADA p: 0.800]\n",
      "[Epoch 1352/2000] [Batch 0/11] [D loss: 0.1804] [G loss: 1.3551] [ADA p: 0.800]\n",
      "[Epoch 1353/2000] [Batch 0/11] [D loss: 0.7358] [G loss: 0.5403] [ADA p: 0.800]\n",
      "[Epoch 1354/2000] [Batch 0/11] [D loss: 0.3835] [G loss: 0.7907] [ADA p: 0.800]\n",
      "[Epoch 1355/2000] [Batch 0/11] [D loss: 0.2850] [G loss: 1.3684] [ADA p: 0.800]\n",
      "[Epoch 1356/2000] [Batch 0/11] [D loss: 0.1257] [G loss: 1.1914] [ADA p: 0.800]\n",
      "[Epoch 1357/2000] [Batch 0/11] [D loss: 0.2641] [G loss: 1.8343] [ADA p: 0.800]\n",
      "[Epoch 1358/2000] [Batch 0/11] [D loss: 0.3747] [G loss: 1.3769] [ADA p: 0.800]\n",
      "[Epoch 1359/2000] [Batch 0/11] [D loss: 0.3637] [G loss: 2.0359] [ADA p: 0.800]\n",
      "[Epoch 1360/2000] [Batch 0/11] [D loss: 0.5389] [G loss: 0.5108] [ADA p: 0.800]\n",
      "[Epoch 1361/2000] [Batch 0/11] [D loss: 0.1970] [G loss: 1.0452] [ADA p: 0.800]\n",
      "[Epoch 1362/2000] [Batch 0/11] [D loss: 0.2040] [G loss: 1.2006] [ADA p: 0.800]\n",
      "[Epoch 1363/2000] [Batch 0/11] [D loss: 0.7263] [G loss: 2.2057] [ADA p: 0.800]\n",
      "[Epoch 1364/2000] [Batch 0/11] [D loss: 0.4626] [G loss: 0.7743] [ADA p: 0.800]\n",
      "[Epoch 1365/2000] [Batch 0/11] [D loss: 0.4734] [G loss: 0.9057] [ADA p: 0.800]\n",
      "[Epoch 1366/2000] [Batch 0/11] [D loss: 0.4199] [G loss: 0.8867] [ADA p: 0.800]\n",
      "[Epoch 1367/2000] [Batch 0/11] [D loss: 0.3010] [G loss: 0.7943] [ADA p: 0.800]\n",
      "[Epoch 1368/2000] [Batch 0/11] [D loss: 0.4038] [G loss: 1.1382] [ADA p: 0.800]\n",
      "[Epoch 1369/2000] [Batch 0/11] [D loss: 0.4752] [G loss: 1.0495] [ADA p: 0.800]\n",
      "[Epoch 1370/2000] [Batch 0/11] [D loss: 0.5831] [G loss: 1.9560] [ADA p: 0.800]\n",
      "[Epoch 1371/2000] [Batch 0/11] [D loss: 0.2468] [G loss: 0.9028] [ADA p: 0.800]\n",
      "[Epoch 1372/2000] [Batch 0/11] [D loss: 0.2413] [G loss: 1.8049] [ADA p: 0.800]\n",
      "[Epoch 1373/2000] [Batch 0/11] [D loss: 0.2147] [G loss: 0.7708] [ADA p: 0.800]\n",
      "[Epoch 1374/2000] [Batch 0/11] [D loss: 0.2825] [G loss: 1.0088] [ADA p: 0.800]\n",
      "[Epoch 1375/2000] [Batch 0/11] [D loss: 0.2799] [G loss: 0.9917] [ADA p: 0.800]\n",
      "[Epoch 1376/2000] [Batch 0/11] [D loss: 0.7498] [G loss: 0.7428] [ADA p: 0.800]\n",
      "[Epoch 1377/2000] [Batch 0/11] [D loss: 0.5231] [G loss: 0.6513] [ADA p: 0.800]\n",
      "[Epoch 1378/2000] [Batch 0/11] [D loss: 0.3084] [G loss: 1.2615] [ADA p: 0.800]\n",
      "[Epoch 1379/2000] [Batch 0/11] [D loss: 0.5835] [G loss: 1.0694] [ADA p: 0.800]\n",
      "[Epoch 1380/2000] [Batch 0/11] [D loss: 0.3476] [G loss: 0.6140] [ADA p: 0.800]\n",
      "[Epoch 1381/2000] [Batch 0/11] [D loss: 0.6192] [G loss: 0.7238] [ADA p: 0.800]\n",
      "[Epoch 1382/2000] [Batch 0/11] [D loss: 0.1186] [G loss: 0.5815] [ADA p: 0.800]\n",
      "[Epoch 1383/2000] [Batch 0/11] [D loss: 0.4952] [G loss: 0.7708] [ADA p: 0.800]\n",
      "[Epoch 1384/2000] [Batch 0/11] [D loss: 0.3696] [G loss: 1.2201] [ADA p: 0.800]\n",
      "[Epoch 1385/2000] [Batch 0/11] [D loss: 0.4669] [G loss: 1.5085] [ADA p: 0.800]\n",
      "[Epoch 1386/2000] [Batch 0/11] [D loss: 0.0562] [G loss: 1.5902] [ADA p: 0.800]\n",
      "[Epoch 1387/2000] [Batch 0/11] [D loss: 0.2865] [G loss: 0.6683] [ADA p: 0.800]\n",
      "[Epoch 1388/2000] [Batch 0/11] [D loss: 0.5412] [G loss: 2.0166] [ADA p: 0.800]\n",
      "[Epoch 1389/2000] [Batch 0/11] [D loss: 0.8814] [G loss: 0.3060] [ADA p: 0.800]\n",
      "[Epoch 1390/2000] [Batch 0/11] [D loss: 0.2524] [G loss: 0.7356] [ADA p: 0.800]\n",
      "[Epoch 1391/2000] [Batch 0/11] [D loss: 0.4501] [G loss: 1.8606] [ADA p: 0.800]\n",
      "[Epoch 1392/2000] [Batch 0/11] [D loss: 0.3326] [G loss: 1.4107] [ADA p: 0.800]\n",
      "[Epoch 1393/2000] [Batch 0/11] [D loss: 0.2810] [G loss: 1.2378] [ADA p: 0.800]\n",
      "[Epoch 1394/2000] [Batch 0/11] [D loss: 0.5941] [G loss: 0.9018] [ADA p: 0.800]\n",
      "[Epoch 1395/2000] [Batch 0/11] [D loss: 0.2804] [G loss: 1.2017] [ADA p: 0.800]\n",
      "[Epoch 1396/2000] [Batch 0/11] [D loss: 0.2025] [G loss: 0.7657] [ADA p: 0.800]\n",
      "[Epoch 1397/2000] [Batch 0/11] [D loss: 0.1887] [G loss: 1.6486] [ADA p: 0.800]\n",
      "[Epoch 1398/2000] [Batch 0/11] [D loss: 0.4208] [G loss: 1.5926] [ADA p: 0.800]\n",
      "[Epoch 1399/2000] [Batch 0/11] [D loss: 0.2818] [G loss: 0.9927] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:08<00:00,  3.77it/s]\n",
      "100%|██████████| 63/63 [00:10<00:00,  5.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1399 | FID: 345.36 | ADA p: 0.800\n",
      "[Epoch 1400/2000] [Batch 0/11] [D loss: 0.5279] [G loss: 1.1556] [ADA p: 0.800]\n",
      "[Epoch 1401/2000] [Batch 0/11] [D loss: 0.2438] [G loss: 0.8658] [ADA p: 0.800]\n",
      "[Epoch 1402/2000] [Batch 0/11] [D loss: 0.3358] [G loss: 1.2923] [ADA p: 0.800]\n",
      "[Epoch 1403/2000] [Batch 0/11] [D loss: 0.5038] [G loss: 0.7516] [ADA p: 0.800]\n",
      "[Epoch 1404/2000] [Batch 0/11] [D loss: 0.5573] [G loss: 0.9522] [ADA p: 0.800]\n",
      "[Epoch 1405/2000] [Batch 0/11] [D loss: 0.4265] [G loss: 1.2679] [ADA p: 0.800]\n",
      "[Epoch 1406/2000] [Batch 0/11] [D loss: 0.2365] [G loss: 1.1390] [ADA p: 0.800]\n",
      "[Epoch 1407/2000] [Batch 0/11] [D loss: 0.3662] [G loss: 1.9840] [ADA p: 0.800]\n",
      "[Epoch 1408/2000] [Batch 0/11] [D loss: 0.3575] [G loss: 2.2330] [ADA p: 0.800]\n",
      "[Epoch 1409/2000] [Batch 0/11] [D loss: 0.2865] [G loss: 0.7714] [ADA p: 0.800]\n",
      "[Epoch 1410/2000] [Batch 0/11] [D loss: 0.1397] [G loss: 1.4437] [ADA p: 0.800]\n",
      "[Epoch 1411/2000] [Batch 0/11] [D loss: 0.2459] [G loss: 0.8673] [ADA p: 0.800]\n",
      "[Epoch 1412/2000] [Batch 0/11] [D loss: 0.4833] [G loss: 1.2535] [ADA p: 0.800]\n",
      "[Epoch 1413/2000] [Batch 0/11] [D loss: 0.1959] [G loss: 0.4188] [ADA p: 0.800]\n",
      "[Epoch 1414/2000] [Batch 0/11] [D loss: 0.1884] [G loss: 1.8022] [ADA p: 0.800]\n",
      "[Epoch 1415/2000] [Batch 0/11] [D loss: 0.4403] [G loss: 2.2231] [ADA p: 0.800]\n",
      "[Epoch 1416/2000] [Batch 0/11] [D loss: 0.5842] [G loss: 1.1359] [ADA p: 0.800]\n",
      "[Epoch 1417/2000] [Batch 0/11] [D loss: 0.3009] [G loss: 1.0467] [ADA p: 0.800]\n",
      "[Epoch 1418/2000] [Batch 0/11] [D loss: 0.5336] [G loss: 1.6367] [ADA p: 0.800]\n",
      "[Epoch 1419/2000] [Batch 0/11] [D loss: 0.2988] [G loss: 1.9463] [ADA p: 0.800]\n",
      "[Epoch 1420/2000] [Batch 0/11] [D loss: 0.3065] [G loss: 2.0905] [ADA p: 0.800]\n",
      "[Epoch 1421/2000] [Batch 0/11] [D loss: 0.2045] [G loss: 2.0252] [ADA p: 0.800]\n",
      "[Epoch 1422/2000] [Batch 0/11] [D loss: 0.3373] [G loss: 0.9800] [ADA p: 0.800]\n",
      "[Epoch 1423/2000] [Batch 0/11] [D loss: 0.1425] [G loss: 0.5920] [ADA p: 0.800]\n",
      "[Epoch 1424/2000] [Batch 0/11] [D loss: 0.1234] [G loss: 1.1300] [ADA p: 0.800]\n",
      "[Epoch 1425/2000] [Batch 0/11] [D loss: 0.2065] [G loss: 1.0502] [ADA p: 0.800]\n",
      "[Epoch 1426/2000] [Batch 0/11] [D loss: 0.3907] [G loss: 0.2300] [ADA p: 0.800]\n",
      "[Epoch 1427/2000] [Batch 0/11] [D loss: 0.2133] [G loss: 0.7904] [ADA p: 0.800]\n",
      "[Epoch 1428/2000] [Batch 0/11] [D loss: 0.6138] [G loss: 1.9536] [ADA p: 0.800]\n",
      "[Epoch 1429/2000] [Batch 0/11] [D loss: 0.4281] [G loss: 0.3847] [ADA p: 0.800]\n",
      "[Epoch 1430/2000] [Batch 0/11] [D loss: 0.3018] [G loss: 1.3714] [ADA p: 0.800]\n",
      "[Epoch 1431/2000] [Batch 0/11] [D loss: 0.4968] [G loss: 2.2877] [ADA p: 0.800]\n",
      "[Epoch 1432/2000] [Batch 0/11] [D loss: 0.3626] [G loss: 1.5604] [ADA p: 0.800]\n",
      "[Epoch 1433/2000] [Batch 0/11] [D loss: 0.5308] [G loss: 0.4593] [ADA p: 0.800]\n",
      "[Epoch 1434/2000] [Batch 0/11] [D loss: 0.1068] [G loss: 0.6895] [ADA p: 0.800]\n",
      "[Epoch 1435/2000] [Batch 0/11] [D loss: 0.1453] [G loss: 1.6946] [ADA p: 0.800]\n",
      "[Epoch 1436/2000] [Batch 0/11] [D loss: 0.3509] [G loss: 0.9838] [ADA p: 0.800]\n",
      "[Epoch 1437/2000] [Batch 0/11] [D loss: 0.4007] [G loss: 2.8333] [ADA p: 0.800]\n",
      "[Epoch 1438/2000] [Batch 0/11] [D loss: 0.4590] [G loss: 0.8840] [ADA p: 0.800]\n",
      "[Epoch 1439/2000] [Batch 0/11] [D loss: 0.7998] [G loss: 1.6882] [ADA p: 0.800]\n",
      "[Epoch 1440/2000] [Batch 0/11] [D loss: 0.2479] [G loss: 0.9201] [ADA p: 0.800]\n",
      "[Epoch 1441/2000] [Batch 0/11] [D loss: 0.1878] [G loss: 1.6189] [ADA p: 0.800]\n",
      "[Epoch 1442/2000] [Batch 0/11] [D loss: 0.6244] [G loss: 1.6529] [ADA p: 0.800]\n",
      "[Epoch 1443/2000] [Batch 0/11] [D loss: 0.1883] [G loss: 1.6375] [ADA p: 0.800]\n",
      "[Epoch 1444/2000] [Batch 0/11] [D loss: 0.2536] [G loss: 1.6559] [ADA p: 0.800]\n",
      "[Epoch 1445/2000] [Batch 0/11] [D loss: 0.5673] [G loss: 0.3001] [ADA p: 0.800]\n",
      "[Epoch 1446/2000] [Batch 0/11] [D loss: 0.4574] [G loss: 1.1739] [ADA p: 0.800]\n",
      "[Epoch 1447/2000] [Batch 0/11] [D loss: 0.3164] [G loss: 0.9054] [ADA p: 0.800]\n",
      "[Epoch 1448/2000] [Batch 0/11] [D loss: 0.4665] [G loss: 1.6086] [ADA p: 0.800]\n",
      "[Epoch 1449/2000] [Batch 0/11] [D loss: 0.1149] [G loss: 0.9473] [ADA p: 0.800]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:09<00:00,  3.30it/s]\n",
      "100%|██████████| 63/63 [00:13<00:00,  4.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1449 | FID: 340.59 | ADA p: 0.800\n",
      "[Epoch 1450/2000] [Batch 0/11] [D loss: 0.7798] [G loss: 1.6574] [ADA p: 0.800]\n",
      "[Epoch 1451/2000] [Batch 0/11] [D loss: 0.1608] [G loss: 1.1958] [ADA p: 0.800]\n",
      "[Epoch 1452/2000] [Batch 0/11] [D loss: 0.2082] [G loss: 1.2228] [ADA p: 0.800]\n",
      "[Epoch 1453/2000] [Batch 0/11] [D loss: 0.3566] [G loss: 0.8619] [ADA p: 0.800]\n",
      "[Epoch 1454/2000] [Batch 0/11] [D loss: 0.2901] [G loss: 1.8173] [ADA p: 0.800]\n",
      "[Epoch 1455/2000] [Batch 0/11] [D loss: 0.3755] [G loss: 0.4558] [ADA p: 0.800]\n",
      "[Epoch 1456/2000] [Batch 0/11] [D loss: 0.5408] [G loss: 1.0578] [ADA p: 0.800]\n",
      "[Epoch 1457/2000] [Batch 0/11] [D loss: 0.4824] [G loss: 1.4261] [ADA p: 0.800]\n",
      "[Epoch 1458/2000] [Batch 0/11] [D loss: 0.2700] [G loss: 1.6145] [ADA p: 0.800]\n",
      "[Epoch 1459/2000] [Batch 0/11] [D loss: 0.5720] [G loss: 0.3694] [ADA p: 0.800]\n",
      "[Epoch 1460/2000] [Batch 0/11] [D loss: 0.2898] [G loss: 1.4545] [ADA p: 0.800]\n",
      "[Epoch 1461/2000] [Batch 0/11] [D loss: 0.2478] [G loss: 1.1522] [ADA p: 0.800]\n",
      "[Epoch 1462/2000] [Batch 0/11] [D loss: 0.2413] [G loss: 1.3785] [ADA p: 0.800]\n",
      "[Epoch 1463/2000] [Batch 0/11] [D loss: 0.3039] [G loss: 0.7867] [ADA p: 0.800]\n",
      "[Epoch 1464/2000] [Batch 0/11] [D loss: 0.3925] [G loss: 0.9453] [ADA p: 0.800]\n",
      "[Epoch 1465/2000] [Batch 0/11] [D loss: 0.2990] [G loss: 1.4157] [ADA p: 0.800]\n",
      "[Epoch 1466/2000] [Batch 0/11] [D loss: 0.1264] [G loss: 1.4392] [ADA p: 0.800]\n",
      "[Epoch 1467/2000] [Batch 0/11] [D loss: 0.2661] [G loss: 2.6018] [ADA p: 0.800]\n",
      "[Epoch 1468/2000] [Batch 0/11] [D loss: 0.0637] [G loss: 1.3362] [ADA p: 0.800]\n",
      "[Epoch 1469/2000] [Batch 0/11] [D loss: 0.5575] [G loss: 0.6173] [ADA p: 0.800]\n",
      "[Epoch 1470/2000] [Batch 0/11] [D loss: 0.3030] [G loss: 0.7702] [ADA p: 0.800]\n",
      "[Epoch 1471/2000] [Batch 0/11] [D loss: 0.3209] [G loss: 1.2249] [ADA p: 0.800]\n",
      "[Epoch 1472/2000] [Batch 0/11] [D loss: 0.8440] [G loss: 0.8041] [ADA p: 0.800]\n",
      "[Epoch 1473/2000] [Batch 0/11] [D loss: 0.1371] [G loss: 0.6071] [ADA p: 0.800]\n",
      "[Epoch 1474/2000] [Batch 0/11] [D loss: 0.3006] [G loss: 1.0333] [ADA p: 0.800]\n",
      "[Epoch 1475/2000] [Batch 0/11] [D loss: 0.4832] [G loss: 1.6574] [ADA p: 0.800]\n",
      "[Epoch 1476/2000] [Batch 0/11] [D loss: 0.2888] [G loss: 0.9563] [ADA p: 0.800]\n",
      "[Epoch 1477/2000] [Batch 0/11] [D loss: 0.5031] [G loss: 1.9088] [ADA p: 0.800]\n",
      "[Epoch 1478/2000] [Batch 0/11] [D loss: 0.2652] [G loss: 1.1397] [ADA p: 0.800]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "best_fid = 9999999\n",
    "EPOCHS = 2000\n",
    "for epoch in range(EPOCHS*0, 1*EPOCHS):\n",
    "    generator.train()\n",
    "    for i, imgs in enumerate(dataloader):\n",
    "        # Configure input\n",
    "        real_imgs = imgs.to(DEVICE)\n",
    "        \n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        # Sample noise as generator input\n",
    "        z = torch.randn(imgs.shape[0], LATENT_DIM, device=DEVICE)\n",
    "        \n",
    "        # Generate a batch of images\n",
    "        fake_imgs = generator(z)\n",
    "        \n",
    "        # Apply adaptive augmentation\n",
    "        p = ada.augment_p\n",
    "        real_aug = ada.apply_augment(real_imgs, p)\n",
    "        fake_aug = ada.apply_augment(fake_imgs.detach(), p)\n",
    "        \n",
    "        # Real images\n",
    "        real_pred = discriminator(real_aug)\n",
    "        real_loss = adversarial_loss(real_pred, torch.ones(real_pred.size(), device=DEVICE))\n",
    "        \n",
    "        # Fake images\n",
    "        fake_pred = discriminator(fake_aug)\n",
    "        fake_loss = adversarial_loss(fake_pred, torch.zeros(fake_pred.size(), device=DEVICE))\n",
    "        \n",
    "        # Total discriminator loss\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        \n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Update ADA probability\n",
    "        if i % ADA_INTERVAL == 0:\n",
    "            p = ada.update(real_pred)\n",
    "        \n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z)\n",
    "        \n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), torch.ones(gen_imgs.size(0), 1, device=DEVICE))\n",
    "        \n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 50 == 0:\n",
    "            print(\n",
    "                f\"[Epoch {epoch}/{EPOCHS}] [Batch {i}/{len(dataloader)}] \"\n",
    "                f\"[D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}] [ADA p: {p:.3f}]\"\n",
    "            )\n",
    "        # Update ADA with FID feedback (every fid_interval steps)\n",
    "        \n",
    "        # Apply augmentation\n",
    "        real_aug = ada.apply_augment(real_imgs, p)\n",
    "    # Save generated images every epoch\n",
    "    if epoch % 10 == 9:\n",
    "        with torch.no_grad():\n",
    "            sample_z = torch.randn(8, LATENT_DIM, device=DEVICE)\n",
    "            gen_imgs = generator(sample_z)\n",
    "            save_image(gen_imgs.data, f\"images_v4_resblock/{epoch}.png\", nrow=4, normalize=True)\n",
    "        \n",
    "    if epoch % 50 == 49:\n",
    "            generator.eval()\n",
    "            current_fid = calculate_fid(generator, dataloader.dataset, DEVICE)\n",
    "            print(f\"Epoch {epoch} | FID: {current_fid:.2f} | ADA p: {p:.3f}\")\n",
    "            \n",
    "            # Save best model\n",
    "            if current_fid < best_fid:\n",
    "                best_fid = current_fid\n",
    "                torch.save(generator.state_dict(), \"best_generator_res_v4.pth\")\n",
    "                \n",
    "            if current_fid > 2 * best_fid:  # Divergence detected\n",
    "                print(\"Stopping early - FID doubled\")\n",
    "                break\n",
    "\n",
    "# Save models\n",
    "torch.save(generator.state_dict(), \"generator_res.pth\")\n",
    "torch.save(discriminator.state_dict(), \"discriminator_res.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M1Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
